# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:20+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/reference/api/python/relay/frontend.rst:20
msgid "tvm.relay.frontend"
msgstr ""

#: of tvm.relay.frontend:1
msgid "Frontends for constructing Relay programs."
msgstr ""

#: of tvm.relay.frontend:3
msgid "Contains the model importers currently defined for Relay."
msgstr ""

#: of tvm.relay.frontend:1
msgid "**Functions:**"
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_mxnet <tvm.relay.frontend.from_mxnet>`\\ \\(symbol\\[\\, "
"shape\\, dtype\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid "Convert from MXNet\"s model into compatible relay Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`quantize_conv_bias_mkldnn_from_var "
"<tvm.relay.frontend.quantize_conv_bias_mkldnn_from_var>`\\ "
"\\(bias\\_var\\, ...\\)"
msgstr ""

#: of
#: tvm.relay.frontend.mxnet_qnn_op_utils.quantize_conv_bias_mkldnn_from_var:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid "Quantized conv2d bias"
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_keras <tvm.relay.frontend.from_keras>`\\ \\(model\\[\\, "
"shape\\, layout\\]\\)"
msgstr ""

#: of tvm.relay.frontend.keras.from_keras:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid "Convert keras model to relay Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_onnx <tvm.relay.frontend.from_onnx>`\\ \\(model\\[\\, "
"shape\\, dtype\\, opset\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:1 tvm.relay.frontend:1:<autosummary>:1
msgid "Convert a ONNX model into an equivalent Relay Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_tflite <tvm.relay.frontend.from_tflite>`\\ \\(model\\[\\, "
"shape\\_dict\\, dtype\\_dict\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.frontend.tflite.from_tflite:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid "Convert from tflite model into compatible relay Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_coreml <tvm.relay.frontend.from_coreml>`\\ \\(model\\[\\, "
"shape\\]\\)"
msgstr ""

#: of tvm.relay.frontend.coreml.from_coreml:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid "Convert from coreml model into Relay Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_caffe2 <tvm.relay.frontend.from_caffe2>`\\ \\(init\\_net\\,"
" predict\\_net\\[\\, shape\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid ""
"Load caffe2 graph which contains init_net and predict_net into Relay "
"Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_tensorflow <tvm.relay.frontend.from_tensorflow>`\\ "
"\\(graph\\[\\, layout\\, shape\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
"Load tensorflow graph which is a python tensorflow graph object into "
"relay."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_darknet <tvm.relay.frontend.from_darknet>`\\ \\(net\\[\\, "
"shape\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.frontend.darknet.from_darknet:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid "Convert from Darknet's model into compatible relay Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_pytorch <tvm.relay.frontend.from_pytorch>`\\ "
"\\(script\\_module\\, input\\_infos\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
"Load PyTorch model in the form of a scripted PyTorch model and convert "
"into relay."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_caffe <tvm.relay.frontend.from_caffe>`\\ \\(init\\_net\\, "
"predict\\_net\\, ...\\)"
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:1
#: tvm.relay.frontend:1:<autosummary>:1
msgid "Convert from caffe model into compatible relay Function."
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid ""
":py:obj:`from_paddle <tvm.relay.frontend.from_paddle>`\\ "
"\\(program\\_or\\_layer\\[\\, shape\\_dict\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.frontend:1:<autosummary>:1
msgid "Convert a PaddlePaddle model into an equivalent Relay Function."
msgstr ""

#: of tvm.relay.frontend:1
msgid "**Classes:**"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:1:<autosummary>:1
msgid ""
":py:obj:`ChangeDatatype <tvm.relay.frontend.ChangeDatatype>`\\ \\(src\\, "
"dst\\)"
msgstr ""

#: of
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass:1
#: tvm.relay.frontend.mxnet.from_mxnet:1:<autosummary>:1
msgid "Mutator for changing the datatype of Relay programs."
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe tvm.relay.frontend.caffe2.from_caffe2
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass
#: tvm.relay.frontend.coreml.from_coreml
#: tvm.relay.frontend.darknet.from_darknet tvm.relay.frontend.keras.from_keras
#: tvm.relay.frontend.mxnet.from_mxnet tvm.relay.frontend.onnx.from_onnx
#: tvm.relay.frontend.paddlepaddle.from_paddle
#: tvm.relay.frontend.pytorch.from_pytorch
#: tvm.relay.frontend.tensorflow.from_tensorflow
#: tvm.relay.frontend.tflite.from_tflite
msgid "Parameters"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:3
msgid "MXNet symbol."
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:7
#: tvm.relay.frontend.darknet.from_darknet:5
#: tvm.relay.frontend.mxnet.from_mxnet:5 tvm.relay.frontend.onnx.from_onnx:19
msgid "The input shape to the graph"
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:9
#: tvm.relay.frontend.darknet.from_darknet:7
#: tvm.relay.frontend.mxnet.from_mxnet:7 tvm.relay.frontend.onnx.from_onnx:21
msgid "The input types to the graph"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:9
msgid "The argument parameters in mxnet"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:11
msgid "The auxiliary parameters in mxnet"
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe tvm.relay.frontend.caffe2.from_caffe2
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass
#: tvm.relay.frontend.coreml.from_coreml
#: tvm.relay.frontend.darknet.from_darknet tvm.relay.frontend.keras.from_keras
#: tvm.relay.frontend.mxnet.from_mxnet tvm.relay.frontend.onnx.from_onnx
#: tvm.relay.frontend.paddlepaddle.from_paddle
#: tvm.relay.frontend.pytorch.from_pytorch
#: tvm.relay.frontend.tensorflow.from_tensorflow
#: tvm.relay.frontend.tflite.from_tflite
msgid "Returns"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:14
msgid ""
"* **mod** (*tvm.IRModule*) -- The relay module for compilation * "
"**params** (*dict of str to tvm.nd.NDArray*) -- The parameter dict to be "
"used by nnvm"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:14
#: tvm.relay.frontend.onnx.from_onnx:39
#: tvm.relay.frontend.paddlepaddle.from_paddle:12
msgid "**mod** (*tvm.IRModule*) -- The relay module for compilation"
msgstr ""

#: of tvm.relay.frontend.mxnet.from_mxnet:15
msgid ""
"**params** (*dict of str to tvm.nd.NDArray*) -- The parameter dict to be "
"used by nnvm"
msgstr ""

#: of tvm.relay.frontend.keras.from_keras:3
msgid "The keras model to be converted."
msgstr ""

#: of tvm.relay.frontend.keras.from_keras:5
msgid "Input shapes of the model, optional"
msgstr ""

#: of tvm.relay.frontend.keras.from_keras:7
msgid ""
"One of 'NCHW' or 'NHWC', indicates how data should be arranged in the "
"output model. Default layout is 'NCHW' as it in general performs better "
"across TVM."
msgstr ""

#: of tvm.relay.frontend.coreml.from_coreml:7
#: tvm.relay.frontend.keras.from_keras:12
msgid ""
"* **mod** (*tvm.IRModule*) -- The relay module for compilation. * "
"**params** (*dict of str to tvm.nd.NDArray*) -- The parameter dict to be "
"used by Relay."
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:12
#: tvm.relay.frontend.coreml.from_coreml:7
#: tvm.relay.frontend.darknet.from_darknet:10
#: tvm.relay.frontend.keras.from_keras:12
#: tvm.relay.frontend.tflite.from_tflite:9
msgid "**mod** (*tvm.IRModule*) -- The relay module for compilation."
msgstr ""

#: of tvm.relay.frontend.coreml.from_coreml:8
#: tvm.relay.frontend.keras.from_keras:13
msgid ""
"**params** (*dict of str to tvm.nd.NDArray*) -- The parameter dict to be "
"used by Relay."
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:3
msgid ""
"ONNX graphs are represented as Python Protobuf objects. The companion "
"parameters will be handled automatically. However, the input names from "
"onnx graph is vague, mixing inputs and network weights/bias such as "
"\"1\", \"2\"... For convenience, we rename the `real` input names to "
"\"input_0\", \"input_1\"... And renaming parameters to \"param_0\", "
"\"param_1\"..."
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:10
msgid ""
"By default, ONNX defines models in terms of dynamic shapes. The ONNX "
"importer retains that dynamism upon import, and the compiler attempts to "
"convert the model into a static shapes at compile time. If this fails, "
"there may still be dynamic operations in the model. Not all TVM kernels "
"currently support dynamic shapes, please file an issue on "
"discuss.tvm.apache.org if you hit an error with dynamic kernels."
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:17
msgid "ONNX ModelProto after ONNX v1.1.0"
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:23
msgid "Override to autodetected opset. This can be helpful for some testing."
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:26
msgid ""
"If this parameter is true, the importer will take any provided onnx input"
" values (weights, shapes, etc) and embed them into the relay model as "
"Constants instead of variables. This allows more aggressive optimizations"
" at compile time and helps in making models static if certain inputs "
"represent attributes relay would traditionally consider compile-time "
"constants."
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:32
msgid ""
"Default config:     use_nt_batch_matmul : bool = True         True to "
"convert qualified onnx `matmul` to `nn.batch_matmul` strict to NT format"
"         (transpose_a=False, transpose_b=True)."
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:35
#: tvm.relay.frontend.tensorflow.from_tensorflow:19
msgid "Default config:"
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:35
#: tvm.relay.frontend.tensorflow.from_tensorflow:19
msgid "use_nt_batch_matmul"
msgstr ""

#: of
msgid "bool = True"
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:35
msgid ""
"True to convert qualified onnx `matmul` to `nn.batch_matmul` strict to NT"
" format (transpose_a=False, transpose_b=True)."
msgstr ""

#: of tvm.relay.frontend.onnx.from_onnx:39
msgid ""
"* **mod** (*tvm.IRModule*) -- The relay module for compilation * "
"**params** (*dict of str to tvm.nd.NDArray*) -- The parameter dict to be "
"used by relay"
msgstr ""

#: of tvm.relay.frontend.darknet.from_darknet:11
#: tvm.relay.frontend.onnx.from_onnx:40
#: tvm.relay.frontend.tflite.from_tflite:10
msgid ""
"**params** (*dict of str to tvm.nd.NDArray*) -- The parameter dict to be "
"used by relay"
msgstr ""

#: of tvm.relay.frontend.tflite.from_tflite:3
msgid "tflite.Model or tflite.Model.Model (depending on tflite version)"
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:7
#: tvm.relay.frontend.tflite.from_tflite:4
msgid "Input shapes of the model."
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:9
#: tvm.relay.frontend.tflite.from_tflite:6
msgid "Input types of the model."
msgstr ""

#: of tvm.relay.frontend.darknet.from_darknet:10
#: tvm.relay.frontend.tflite.from_tflite:9
msgid ""
"* **mod** (*tvm.IRModule*) -- The relay module for compilation. * "
"**params** (*dict of str to tvm.nd.NDArray*) -- The parameter dict to be "
"used by relay"
msgstr ""

#: of tvm.relay.frontend.coreml.from_coreml:3
msgid "coremltools.models.MLModel of a NeuralNetworkClassifier"
msgstr ""

#: of tvm.relay.frontend.coreml.from_coreml:4
msgid "The input shapes"
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:3
msgid "Caffe2 NetDef containing the weights"
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:5
msgid "Caffe2 NetDef containing the graph"
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:12
#: tvm.relay.frontend.tensorflow.from_tensorflow:23
msgid ""
"* **mod** (*tvm.IRModule*) -- The module that optimizations will be "
"performed on. * **params** (*dict of str to tvm.nd.NDArray*) -- Dict of "
"converted parameters stored in tvm.nd.NDArray format"
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:12
#: tvm.relay.frontend.pytorch.from_pytorch:35
#: tvm.relay.frontend.tensorflow.from_tensorflow:23
msgid ""
"**mod** (*tvm.IRModule*) -- The module that optimizations will be "
"performed on."
msgstr ""

#: of tvm.relay.frontend.caffe2.from_caffe2:13
#: tvm.relay.frontend.tensorflow.from_tensorflow:24
msgid ""
"**params** (*dict of str to tvm.nd.NDArray*) -- Dict of converted "
"parameters stored in tvm.nd.NDArray format"
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:1
msgid ""
"Load tensorflow graph which is a python tensorflow graph object into "
"relay. The companion parameters will be handled automatically."
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:4
msgid "Tensorflow GraphDef"
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:6
msgid "NCHW only supported now to enable NHWC models on GPU."
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:8
msgid "Graph level input shape dictionary."
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:10
msgid "if not specified then the last node is assumed as graph output."
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:12
msgid ""
"Default config:     use_dense : bool = True         Ture to convert "
"`tf.matmul` to `nn.dense`, else to `nn.matmul`.         The `nn.dense` op"
" requires the data tensor to be non-transposed and weight tensor         "
"to be transposed, may insert extra `transpose` to the original graph."
"     use_nt_batch_matmul : bool = True         True to convert "
"`tf.batch_matmul` to `nn.batch_matmul` strict to NT format         "
"(transpose_a=False, transpose_b=True)."
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:16
msgid "use_dense"
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:15
msgid ""
"Ture to convert `tf.matmul` to `nn.dense`, else to `nn.matmul`. The "
"`nn.dense` op requires the data tensor to be non-transposed and weight "
"tensor to be transposed, may insert extra `transpose` to the original "
"graph."
msgstr ""

#: of tvm.relay.frontend.tensorflow.from_tensorflow:19
msgid ""
"True to convert `tf.batch_matmul` to `nn.batch_matmul` strict to NT "
"format (transpose_a=False, transpose_b=True)."
msgstr ""

#: of tvm.relay.frontend.darknet.from_darknet:3
msgid "Darknet net structure."
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:1
msgid ""
"Load PyTorch model in the form of a scripted PyTorch model and convert "
"into relay. The companion parameters will be handled automatically."
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:4
msgid ""
"TorchScripted PyTorch graph Note: We currently only support traces (ie: "
"torch.jit.trace(model, input))"
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:7
msgid ""
"Can be (input name, input shape) or (input name, (input shape, input "
"types)) Graph level input shape and type list The same input names need "
"to be used for deployment, so choose easy to remember names (such as: "
"input0, input1) e.g. [('input0', (1, 2)), ('input1', (3, 4))] or "
"[('input0', ((1, 2), 'int')), ('input1', ((3, 4), 'float'))]"
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:16
msgid "A custom op conversion map in the same format as _convert_map above"
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:18
msgid "The default dtype to use when type information is not provided by PyTorch."
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:20
msgid ""
"When True, replace '.' with `_' in a original parameter name. The Relay "
"text parser treats a variable name followed by a period as a tuple "
"element access, so a variable name like \"dense.weight\" cannot be parsed"
" correctly. Use this option when you want to run the AnnotateSpans pass "
"on the imported module."
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:25
msgid ""
"Return quantized weights and bias, rather than float ones. PyTorch stores"
" quantized weights in a custom format, so we cannot directly access 8 bit"
" weights as Numpy arrays. We use a PyTorch function to unpack quantized "
"weights into float32 arrays and quantization parameters. By default, we "
"return float32 weights and rely on the QNN lowering and the Relay "
"constant folding pass to quantize weights at compile time. In BYOC use "
"cases, however, we cannot apply the constant folding pass on a QNN graph."
" If keep_quantized_weight is True, we quantize weights in the frontend "
"using a function that is equivalent to qnn.op.quantize(...) operating on "
"Numpy arrays."
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:35
msgid ""
"* **mod** (*tvm.IRModule*) -- The module that optimizations will be "
"performed on. * **params** (*dict of str to tvm.runtime.NDArray*) -- Dict"
" of converted parameters stored in tvm.runtime.ndarray format"
msgstr ""

#: of tvm.relay.frontend.pytorch.from_pytorch:36
msgid ""
"**params** (*dict of str to tvm.runtime.NDArray*) -- Dict of converted "
"parameters stored in tvm.runtime.ndarray format"
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:3
msgid "caffemodel"
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:5
msgid "caffe prototxt"
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:12
msgid ""
"* **mod** (*tvm.IRModule*) -- The relay module for compilation. * "
"**params** (*dict of str to tvm.NDArray*) -- The parameter dict to be "
"used by relay"
msgstr ""

#: of tvm.relay.frontend.caffe.from_caffe:13
msgid ""
"**params** (*dict of str to tvm.NDArray*) -- The parameter dict to be "
"used by relay"
msgstr ""

#: of tvm.relay.frontend.paddlepaddle.from_paddle:1
msgid ""
"Convert a PaddlePaddle model into an equivalent Relay Function. "
"PaddlePaddle Program/TranslatedLayer represent the computation graph of "
"PaddlePaddle model, and PaddlePaddle scope stores all the weights of "
"PaddlePaddle model."
msgstr ""

#: of tvm.relay.frontend.paddlepaddle.from_paddle:5
msgid "Loaded model by `paddle.static.load_inference_model` or `paddle.jit.load`"
msgstr ""

#: of tvm.relay.frontend.paddlepaddle.from_paddle:7
msgid "The input shape of model"
msgstr ""

#: of tvm.relay.frontend.paddlepaddle.from_paddle:9
msgid ""
"The scope that saves all the weights of model, use "
"`paddle.static.global_scope` by default"
msgstr ""

#: of tvm.relay.frontend.paddlepaddle.from_paddle:12
msgid ""
"* **mod** (*tvm.IRModule*) -- The relay module for compilation * "
"**params** (*dict of str to tvm.nd.NDArray*)"
msgstr ""

#: of tvm.relay.frontend.paddlepaddle.from_paddle:13
msgid "**params** (*dict of str to tvm.nd.NDArray*)"
msgstr ""

#: of
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass:3
msgid ""
"This pass should be useful for users of the Bring Your Own Datatypes "
"framework. TODO(@gussmith23 @hypercubestart) Add link to documentation "
"when it exists"
msgstr ""

#: of
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass:7
msgid "Example:"
msgstr ""

#: of
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass:21
msgid ""
"The source datatype name, e.g. \"float\" or \"posites2\" (but not "
"\"float32\" or \"custom[posites2]32\")."
msgstr ""

#: of
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass:24
msgid "The destination datatype name, in the same format."
msgstr ""

#: of
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass:27
msgid ""
"**mod** -- Module where all nodes of dtype `src` have been changed to "
"have dtype `dst`."
msgstr ""

#: of
#: tvm.relay.frontend.change_datatype._wrap_class_function_pass.<locals>.PyFunctionPass
msgid "Return type"
msgstr ""


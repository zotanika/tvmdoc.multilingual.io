# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:26+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_deploy_models_deploy_quantized.py>` to download"
" the full example code"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:22
msgid "Deploy a Quantized Model on Cuda"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:23
msgid "**Author**: `Wuwei Lin <https://github.com/vinx13>`_"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:25
msgid ""
"This article is an introductory tutorial of automatic quantization with "
"TVM. Automatic quantization is one of the quantization modes in TVM. More"
" details on the quantization story in TVM can be found `here "
"<https://discuss.tvm.apache.org/t/quantization-story/3920>`_. In this "
"tutorial, we will import a GluonCV pre-trained model on ImageNet to "
"Relay, quantize the Relay model and then perform the inference."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:55
msgid "Prepare the Dataset"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:56
msgid ""
"We will demonstrate how to prepare the calibration dataset for "
"quantization. We first download the validation set of ImageNet and pre-"
"process the dataset."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:97
msgid ""
"The calibration dataset should be an iterable object. We define the "
"calibration dataset as a generator object in Python. In this tutorial, we"
" only use a few samples for calibration."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:123
msgid "Import the model"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:124
msgid ""
"We use the Relay MxNet frontend to import a model from the Gluon model "
"zoo."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:142
msgid "Quantize the Model"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:143
msgid ""
"In quantization, we need to find the scale for each weight and "
"intermediate feature map tensor of each layer."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:146
msgid ""
"For weights, the scales are directly calculated based on the value of the"
" weights. Two modes are supported: `power2` and `max`. Both modes find "
"the maximum value within the weight tensor first. In `power2` mode, the "
"maximum is rounded down to power of two. If the scales of both weights "
"and intermediate feature maps are power of two, we can leverage bit "
"shifting for multiplications. This make it computationally more "
"efficient. In `max` mode, the maximum is used as the scale. Without "
"rounding, `max` mode might have better accuracy in some cases. When the "
"scales are not powers of two, fixed point multiplications will be used."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:156
msgid ""
"For intermediate feature maps, we can find the scales with data-aware "
"quantization. Data-aware quantization takes a calibration dataset as the "
"input argument. Scales are calculated by minimizing the KL divergence "
"between distribution of activation before and after quantization. "
"Alternatively, we can also use pre-defined global scales. This saves the "
"time for calibration. But the accuracy might be impacted."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:183
msgid "Run Inference"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:184
msgid "We create a Relay VM to build and execute the model."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:222
msgid ""
":download:`Download Python source code: deploy_quantized.py "
"<deploy_quantized.py>`"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:228
msgid ""
":download:`Download Jupyter notebook: deploy_quantized.ipynb "
"<deploy_quantized.ipynb>`"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:235
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


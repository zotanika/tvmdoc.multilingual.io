# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:26+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_tune_with_autotvm_tune_conv2d_cuda.py>` to "
"download the full example code"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:22
msgid "Tuning High Performance Convolution on NVIDIA GPUs"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:23
msgid "**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:25
msgid ""
"This is an advanced tutorial for writing high performance tunable "
"template for NVIDIA GPU. By running auto-tuner on this template, we can "
"outperform the vendor provided library CuDNN in many cases."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:29
msgid ""
"Note that this tutorial will not run on Windows or recent versions of "
"macOS. To get it to run, you will need to wrap the body of this tutorial "
"in a :code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:36
msgid "Install dependencies"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:37
msgid ""
"To use autotvm package in tvm, we need to install some extra "
"dependencies. (change \"3\" to \"2\" if you use python2):"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:44
msgid ""
"To make TVM run faster in tuning, it is recommended to use cython as FFI "
"of tvm. In the root directory of tvm, execute"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:52
msgid "Now return to python code. Import packages."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:74
msgid "Step 1:  Define the search space"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:75
msgid ""
"There are plenty of useful schedule primitives in tvm. You can also find "
"some tutorials that describe them in more details, such as (1). :ref"
":`opt-conv-gpu` (2). `Optimizing DepthwiseConv on NVIDIA GPU "
"<https://tvm.apache.org/2017/08/22/Optimize-Deep-Learning-GPU-Operators-"
"with-TVM-A-Depthwise-Convolution-Example>`_"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:80
msgid ""
"However, their implementations are manually tuned for some special input "
"shapes. In this section, we build a large enough space to cover the "
"techniques used in these tutorials. Then we rely on the efficient auto-"
"tuner to search through this space and pick some good configurations."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:85
msgid ""
"If you are familiar with writing cuda schedule, you can find the "
"following template is very general. Actually this template can be easily "
"modified to tune other operators such as depthwise convolution and gemm. "
"In order to fully understand this template, you should be familiar with "
"the schedule primitives and auto tuning API. You can refer to the above "
"tutorials and :ref:`autotvm tutorial <tutorial-autotvm-matmul-x86>`"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:92
msgid ""
"It is worth noting that the search space for a conv2d operator can be "
"very large (at the level of 10^9 for some input shapes)"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:194
msgid "Step 2:  Search through the space"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:195
msgid ""
"We pick the last layer on resnet as test case. Since our space is very "
"large, :code:`XGBoostTuner` is most suitable for our case. Here we only "
"do 20 trials for demonstration. In practice, making 1000 trials usually "
"can find some good kernels for this template"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:237
msgid ""
"Finally we can inspect the best config from log file, check correctness, "
"and measure running time."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:288
msgid ""
":download:`Download Python source code: tune_conv2d_cuda.py "
"<tune_conv2d_cuda.py>`"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:294
msgid ""
":download:`Download Jupyter notebook: tune_conv2d_cuda.ipynb "
"<tune_conv2d_cuda.ipynb>`"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:301
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


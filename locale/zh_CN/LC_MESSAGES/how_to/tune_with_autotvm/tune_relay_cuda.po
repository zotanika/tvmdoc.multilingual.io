# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:20+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_tune_with_autotvm_tune_relay_cuda.py>` to "
"download the full example code"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:22
msgid "Auto-tuning a Convolutional Network for NVIDIA GPU"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:23
msgid ""
"**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_, `Eddie Yan "
"<https://github.com/eqy/>`_"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:25
msgid ""
"Auto-tuning for specific devices and workloads is critical for getting "
"the best performance. This is a tutorial on how to tune a whole "
"convolutional network for NVIDIA GPU."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:29
msgid ""
"The operator implementation for NVIDIA GPU in TVM is written in template "
"form. The template has many tunable knobs (tile factor, unrolling, etc). "
"We will tune all convolution and depthwise convolution operators in the "
"neural network. After tuning, we produce a log file which stores the best"
" knob values for all required operators. When the TVM compiler compiles "
"these operators, it will query this log file to get the best knob values."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:36
msgid ""
"We also released pre-tuned parameters for some NVIDIA GPUs. You can go to"
" `NVIDIA GPU Benchmark <https://github.com/apache/tvm/wiki/Benchmark"
"#nvidia-gpu>`_ to see the results."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:40
msgid ""
"Note that this tutorial will not run on Windows or recent versions of "
"macOS. To get it to run, you will need to wrap the body of this tutorial "
"in a :code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:47
msgid "Install dependencies"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:48
msgid ""
"To use the autotvm package in tvm, we need to install some extra "
"dependencies. (change \"3\" to \"2\" if you use python2):"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:55
msgid ""
"To make TVM run faster during tuning, it is recommended to use cython as "
"FFI of tvm. In the root directory of tvm, execute:"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:63
msgid "Now return to python code. Import packages."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:90
msgid "Define Network"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:91
msgid ""
"First we need to define the network in relay frontend API. We can load "
"some pre-defined network from :code:`tvm.relay.testing`. We can also load"
" models from MXNet, ONNX and TensorFlow."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:152
msgid "Set Tuning Options"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:153
msgid "Before tuning, we apply some configurations."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:188
msgid "How to set tuning options"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:190
msgid "In general, the default value provided here works well."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:192
msgid ""
"If you have large time budget, you can set :code:`n_trial`, "
":code:`early_stopping` larger, which makes the tuning runs longer."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:195
msgid ""
"If you have multiple devices, you can use all of them for measurement to "
"accelerate the tuning process. (see the 'Scale up measurement` section "
"below)."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:202
msgid "Begin Tuning"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:203
msgid ""
"Now we can extract tuning tasks from the network and begin tuning. Here, "
"we provide a simple utility function to tune a list of tasks. This "
"function is just an initial implementation which tunes them in sequential"
" order. We will introduce a more sophisticated tuning scheduler in the "
"future."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:273
msgid "Finally, we launch tuning jobs and evaluate the end-to-end performance."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:325
msgid "Sample Output"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:326
msgid ""
"The tuning needs to compile many programs and extract feature from them. "
"So a high performance CPU is recommended. One sample output is listed "
"below. It takes about 4 hours to get the following output on a 32T AMD "
"Ryzen Threadripper. The tuning target is NVIDIA 1080 Ti. (You can see "
"some errors during compilation. If the tuning is not stuck, it is okay.)"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:356
msgid ""
"As a reference baseline, the time cost of MXNet + TensorRT on resnet-18 "
"is 1.30ms. So we are a little faster."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:360
msgid "**Experiencing Difficulties?**"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:362
msgid ""
"The auto tuning module is error-prone. If you always see \" 0.00/ 0.00 "
"GFLOPS\", then there must be something wrong."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:365
msgid ""
"First, make sure you set the correct configuration of your device. Then, "
"you can print debug information by adding these lines in the beginning of"
" the script. It will print every measurement result, where you can find "
"useful error messages."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:375
msgid ""
"Finally, always feel free to ask our community for help on "
"https://discuss.tvm.apache.org"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:384
msgid "Scale up measurement by using multiple devices"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:385
msgid ""
"If you have multiple devices, you can use all of them for measurement. "
"TVM uses the RPC Tracker to manage distributed devices. The RPC Tracker "
"is a centralized controller node. We can register all devices to the "
"tracker. For example, if we have 10 GPU cards, we can register all of "
"them to the tracker, and run 10 measurements in parallel, accelerating "
"the tuning process."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:391
msgid ""
"To start an RPC tracker, run this command on the host machine. The "
"tracker is required during the whole tuning process, so we need to open a"
" new terminal for this command:"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:399
msgid "The expected output is"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:405
msgid ""
"Then open another new terminal for the RPC server. We need to start one "
"dedicated server for each device. We use a string key to distinguish the "
"types of devices. You can pick a name you like. (Note: For rocm backend, "
"there are some internal errors with the compiler, we need to add `--no-"
"fork` to the argument list.)"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:415
msgid "After registering devices, we can confirm it by querying rpc_tracker"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:421
msgid ""
"For example, if we have four 1080ti, two titanx and one gfx900, the "
"output can be"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:434
msgid ""
"Finally, we need to change the tuning option to use RPCRunner. Use the "
"code below to replace the corresponding part above."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:480
msgid ""
":download:`Download Python source code: tune_relay_cuda.py "
"<tune_relay_cuda.py>`"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:486
msgid ""
":download:`Download Jupyter notebook: tune_relay_cuda.ipynb "
"<tune_relay_cuda.ipynb>`"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_relay_cuda.rst:493
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


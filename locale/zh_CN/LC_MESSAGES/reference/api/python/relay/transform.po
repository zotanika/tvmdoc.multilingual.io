# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:20+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/reference/api/python/relay/transform.rst:19
msgid "tvm.relay.transform"
msgstr ""

#: of tvm.relay.transform:1
msgid "The Relay IR namespace containing transformations."
msgstr ""

#: of tvm.relay.transform:1
msgid "**Functions:**"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`recast <tvm.relay.transform.tvm.relay.transform.recast>`\\ "
"\\(expr\\, dtype\\, out\\_dtype\\[\\, ops\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Convert the types of operations in a graph to a new value."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`AlterOpLayout "
"<tvm.relay.transform.tvm.relay.transform.AlterOpLayout>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Alternate the layouts of operators or replace primitive operators with "
"other expressions."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`AnnotateSpans "
"<tvm.relay.transform.tvm.relay.transform.AnnotateSpans>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateSpans:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Annotate a program with span information by first generating its textual "
"representation and then parsing it back into a Relay AST annotated with "
"span information."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`AnnotateTarget "
"<tvm.relay.transform.tvm.relay.transform.AnnotateTarget>`\\ "
"\\(targets\\[\\, include\\_non\\_call\\_ops\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Annotate ops in an experession with a provied compiler/target and then "
"use it for codegen."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`BackwardFoldScaleAxis "
"<tvm.relay.transform.tvm.relay.transform.BackwardFoldScaleAxis>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.BackwardFoldScaleAxis:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Backward fold axis scaling into weights of conv2d/dense."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`BatchingOps "
"<tvm.relay.transform.tvm.relay.transform.BatchingOps>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.BatchingOps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Batching parallel operators into one for Conv2D, Dense and BatchMatmul."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CanonicalizeCast "
"<tvm.relay.transform.tvm.relay.transform.CanonicalizeCast>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeCast:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Canonicalize cast expressions to make operator fusion more efficient."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CanonicalizeOps "
"<tvm.relay.transform.tvm.relay.transform.CanonicalizeOps>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Canonicalize special operators to basic operators."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CombineParallelBatchMatmul "
"<tvm.relay.transform.tvm.relay.transform.CombineParallelBatchMatmul>`\\ "
"\\(\\[min\\_num\\_branches\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Combine multiple batch matmul operators into one."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CombineParallelConv2D "
"<tvm.relay.transform.tvm.relay.transform.CombineParallelConv2D>`\\ "
"\\(\\[min\\_num\\_branches\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelConv2D:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Combine multiple conv2d operators into one."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CombineParallelDense "
"<tvm.relay.transform.tvm.relay.transform.CombineParallelDense>`\\ "
"\\(\\[min\\_num\\_branches\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Combine multiple dense operators into one."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Conv2dToSparse "
"<tvm.relay.transform.tvm.relay.transform.Conv2dToSparse>`\\ "
"\\(weight\\_name\\, weight\\_shape\\, ...\\)"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Rewrite qualified ```nn.conv2d operation``` to ```nn.sparse_conv2d```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Conv2dToSparse2 "
"<tvm.relay.transform.tvm.relay.transform.Conv2dToSparse2>`\\ \\(layout\\,"
" kernel\\_size\\, ...\\)"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse2:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Rewrite freezed ```nn.conv2d``` operation to ```nn.sparse_conv2d```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ConvertLayout "
"<tvm.relay.transform.tvm.relay.transform.ConvertLayout>`\\ "
"\\(desired\\_layouts\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Given a dest layout, this pass transforms the expr such that most of the "
"ops input data layout is changed to the dest layout."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`DeadCodeElimination "
"<tvm.relay.transform.tvm.relay.transform.DeadCodeElimination>`\\ "
"\\(\\[inline\\_once\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Remove expressions that do not have any users (dead code)."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Defunctionalization "
"<tvm.relay.transform.tvm.relay.transform.Defunctionalization>`\\ "
"\\(func\\, mod\\)"
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Performs defunctionalization on func, transforming func from a higher-"
"order program to a first-order program."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`DefuseOps <tvm.relay.transform.tvm.relay.transform.DefuseOps>`\\"
" \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "The inverse operation of FuseOps."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`DenseToSparse "
"<tvm.relay.transform.tvm.relay.transform.DenseToSparse>`\\ "
"\\(weight\\_name\\, weight\\_shape\\)"
msgstr ""

#: of tvm.relay.transform.transform.DenseToSparse:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Rewrite qualified ```nn.dense operation``` to ```nn.sparse_dense``` This "
"pass is used in ```data_dep_optimization.bsr_dense``` Parameters of this "
"pass is generated by ```analysis.sparse_dense.process_params```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`DynamicToStatic "
"<tvm.relay.transform.tvm.relay.transform.DynamicToStatic>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.DynamicToStatic:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "If possible, convert tvm.relay.dynamic* ops to static versions"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`EliminateCommonSubexpr "
"<tvm.relay.transform.tvm.relay.transform.EliminateCommonSubexpr>`\\ "
"\\(\\[fskip\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.EliminateCommonSubexpr:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Eliminate common subexpressions."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`EtaExpand <tvm.relay.transform.tvm.relay.transform.EtaExpand>`\\"
" \\(\\[expand\\_constructor\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Add abstraction over a constructor or global variable bound to a function"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FakeQuantizationToInteger "
"<tvm.relay.transform.tvm.relay.transform.FakeQuantizationToInteger>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Find regions of the graph of the form"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FastMath <tvm.relay.transform.tvm.relay.transform.FastMath>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.FastMath:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Converts the expensive non linear functions to their fast but approximate"
" counterparts."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FirstOrderGradient "
"<tvm.relay.transform.tvm.relay.transform.FirstOrderGradient>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Transforms all global functions in the module to return the original "
"result, paired with the gradients of the inputs."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FoldConstant "
"<tvm.relay.transform.tvm.relay.transform.FoldConstant>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstant:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Fold the constant expressions in a Relay program."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FoldConstantExpr "
"<tvm.relay.transform.tvm.relay.transform.FoldConstantExpr>`\\ \\(expr\\, "
"mod\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FoldExplicitPadding "
"<tvm.relay.transform.tvm.relay.transform.FoldExplicitPadding>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.FoldExplicitPadding:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"FoldExplicitPadding finds explict padding before an op that can support "
"implicit padding and fuses them."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FoldScaleAxis "
"<tvm.relay.transform.tvm.relay.transform.FoldScaleAxis>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.ForwardFoldScaleAxis:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Fold the scaling of axis into weights of conv2d/dense."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ForwardFoldScaleAxis "
"<tvm.relay.transform.tvm.relay.transform.ForwardFoldScaleAxis>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FuseOps <tvm.relay.transform.tvm.relay.transform.FuseOps>`\\ "
"\\(\\[fuse\\_opt\\_level\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.FuseOps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Fuse operators in an expr to a larger operator according to some rules."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`InferType <tvm.relay.transform.tvm.relay.transform.InferType>`\\"
" \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.InferType:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Infer the type of an expr."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`Inline <tvm.relay.transform.tvm.relay.transform.Inline>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Perform inlining on the given Relay IR module."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`LambdaLift "
"<tvm.relay.transform.tvm.relay.transform.LambdaLift>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.LambdaLift:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Lift the closure to global function."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`LazyGradientInit "
"<tvm.relay.transform.tvm.relay.transform.LazyGradientInit>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.LazyGradientInit:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Reduces memory usage of gradient tensors"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Legalize <tvm.relay.transform.tvm.relay.transform.Legalize>`\\ "
"\\(\\[legalize\\_map\\_attr\\_name\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Legalizes an expression with another expression."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`MergeCompilerRegions "
"<tvm.relay.transform.tvm.relay.transform.MergeCompilerRegions>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.MergeCompilerRegions:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Merge together compiler regions."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`MergeComposite "
"<tvm.relay.transform.tvm.relay.transform.MergeComposite>`\\ "
"\\(pattern\\_table\\)"
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Merge multiple operators into a single composite relay function."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`PartialEvaluate "
"<tvm.relay.transform.tvm.relay.transform.PartialEvaluate>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.PartialEvaluate:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Evaluate the static fragment of the code."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`PartitionGraph "
"<tvm.relay.transform.tvm.relay.transform.PartitionGraph>`\\ "
"\\(\\[mod\\_name\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Partition a Relay program into regions that can be executed on different "
"backends."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`PlanDevices "
"<tvm.relay.transform.tvm.relay.transform.PlanDevices>`\\ "
"\\(default\\_device\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Uses existing \"on_device\" and \"device_copy\" CallNodes to infer the "
"device on which every Relay sub-expression should run (and the result "
"stored)."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`RemoveUnusedFunctions "
"<tvm.relay.transform.tvm.relay.transform.RemoveUnusedFunctions>`\\ "
"\\(\\[entry\\_functions\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.RemoveUnusedFunctions:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Remove unused global relay functions in a relay module."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`SimplifyExpr "
"<tvm.relay.transform.tvm.relay.transform.SimplifyExpr>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.SimplifyExpr:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Simplify the Relay expression, including merging consecutive reshapes."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`SimplifyFCTranspose "
"<tvm.relay.transform.tvm.relay.transform.SimplifyFCTranspose>`\\ "
"\\(target\\_weight\\_name\\)"
msgstr ""

#: of tvm.relay.transform.transform.SimplifyFCTranspose:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Rewrite ```y = nn.dense(x, transpose(w, [1, 0]))``` to ```y = nn.dense(x,"
" wt)``` This pass is used in "
"```data_dep_optimization.simplify_fc_transpose```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`SimplifyInference "
"<tvm.relay.transform.tvm.relay.transform.SimplifyInference>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Simplify the data-flow graph for inference phase."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`SplitArgs <tvm.relay.transform.tvm.relay.transform.SplitArgs>`\\"
" \\(max\\_function\\_args\\)"
msgstr ""

#: of tvm.relay.transform.transform.SplitArgs:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Split function with huge number of arguments to smaller pieces."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToANormalForm "
"<tvm.relay.transform.tvm.relay.transform.ToANormalForm>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Turn Graph Normal Form expression into A Normal Form Expression."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToANormalFormExpr "
"<tvm.relay.transform.tvm.relay.transform.ToANormalFormExpr>`\\ \\(e\\)"
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "ToANormalForm, but on expression level."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToBasicBlockNormalForm "
"<tvm.relay.transform.tvm.relay.transform.ToBasicBlockNormalForm>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Turn an expression to Basic Block Normal Form."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToCPS <tvm.relay.transform.tvm.relay.transform.ToCPS>`\\ "
"\\(expr\\[\\, mod\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.ToCPS:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn expression into continuation passing style(CPS)."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToGraphNormalForm "
"<tvm.relay.transform.tvm.relay.transform.ToGraphNormalForm>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.ToGraphNormalForm:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn a Relay program in A Normal Form into Graph Normal Form"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToMixedPrecision "
"<tvm.relay.transform.tvm.relay.transform.ToMixedPrecision>`\\ "
"\\(\\[mixed\\_precision\\_type\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Automatic mixed precision rewriter."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`build_config "
"<tvm.relay.transform.tvm.relay.transform.build_config>`\\ "
"\\(\\[opt\\_level\\, required\\_pass\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Configure the build behavior by setting config variables."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`function_pass "
"<tvm.relay.transform.tvm.relay.transform.function_pass>`\\ "
"\\(\\[pass\\_func\\, opt\\_level\\, name\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Decorate a function pass."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`gradient <tvm.relay.transform.tvm.relay.transform.gradient>`\\ "
"\\(expr\\[\\, mod\\, mode\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.gradient:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Transform the input function, returning a function that calculate the "
"original result, paired with gradient of the input."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`to_cps <tvm.relay.transform.tvm.relay.transform.to_cps>`\\ "
"\\(func\\[\\, mod\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.to_cps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn expression into CPS expression."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`un_cps <tvm.relay.transform.tvm.relay.transform.un_cps>`\\ "
"\\(func\\)"
msgstr ""

#: of tvm.relay.transform.transform.un_cps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn an cps function into a Function without the continuation argument."
msgstr ""

#: of tvm.relay.transform:1
msgid "**Classes:**"
msgstr ""

#: of tvm.relay.transform.recast.recast:1:<autosummary>:1
msgid ""
":py:obj:`ChangeBatch "
"<tvm.relay.transform.tvm.relay.transform.ChangeBatch>`\\ \\(data\\[\\, "
"batch\\_size\\]\\)"
msgstr ""

#: of tvm.relay.transform.recast.recast:1:<autosummary>:1
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:1
msgid "Change the batch size."
msgstr ""

#: of tvm.relay.transform.recast.recast:1:<autosummary>:1
msgid ""
":py:obj:`FunctionPass "
"<tvm.relay.transform.tvm.relay.transform.FunctionPass>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.recast.recast:1:<autosummary>:1
msgid "A pass that works on each tvm.relay.Function in a module."
msgstr ""

#: of tvm.relay.transform.recast.recast:1:<autosummary>:1
msgid ""
":py:obj:`LayoutConfig "
"<tvm.relay.transform.tvm.relay.transform.LayoutConfig>`\\ "
"\\(\\[skip\\_layers\\]\\)"
msgstr ""

#: of tvm.relay.transform.recast.recast:1:<autosummary>:1
#: tvm.relay.transform.transform.LayoutConfig:1
msgid "A structure for customizing the ConvertLayout pass."
msgstr ""

#: of tvm.relay.transform.recast.recast:1
msgid ""
"Convert the types of operations in a graph to a new value. Note that this"
" is primarily useful for testing performance of individual operations at "
"the new datatype. In a real setting, this pass will almost certainly do a"
" poor job converting from one datatype to another as it just applies hard"
" casting. For example, when recasting from float to integer, many small "
"values will simply be set to 0. Although this will allow autotuning and "
"benchmarking to produce proper timings at the new data type, the output "
"of the model will of course be heavily impacted."
msgstr ""

#: of tvm.relay.transform.recast.recast
#: tvm.relay.transform.transform.AnnotateTarget
#: tvm.relay.transform.transform.CombineParallelBatchMatmul
#: tvm.relay.transform.transform.CombineParallelConv2D
#: tvm.relay.transform.transform.CombineParallelDense
#: tvm.relay.transform.transform.Conv2dToSparse
#: tvm.relay.transform.transform.Conv2dToSparse2
#: tvm.relay.transform.transform.ConvertLayout
#: tvm.relay.transform.transform.DeadCodeElimination
#: tvm.relay.transform.transform.Defunctionalization
#: tvm.relay.transform.transform.DenseToSparse
#: tvm.relay.transform.transform.EliminateCommonSubexpr
#: tvm.relay.transform.transform.EtaExpand
#: tvm.relay.transform.transform.FuseOps tvm.relay.transform.transform.Legalize
#: tvm.relay.transform.transform.MergeComposite
#: tvm.relay.transform.transform.RemoveUnusedFunctions
#: tvm.relay.transform.transform.SimplifyFCTranspose
#: tvm.relay.transform.transform.ToANormalFormExpr
#: tvm.relay.transform.transform.ToMixedPrecision
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass
#: tvm.relay.transform.transform.build_config
#: tvm.relay.transform.transform.function_pass
#: tvm.relay.transform.transform.gradient tvm.relay.transform.transform.to_cps
#: tvm.relay.transform.transform.un_cps
msgid "Parameters"
msgstr ""

#: of tvm.relay.transform.recast.recast:10
msgid "The original function that will have its type changed."
msgstr ""

#: of tvm.relay.transform.recast.recast:12
msgid "The target type to cast to."
msgstr ""

#: of tvm.relay.transform.recast.recast:14
msgid "The output type to cast to."
msgstr ""

#: of tvm.relay.transform.recast.recast:16
msgid ""
"A list of operations that should have their type changed, others will be "
"left as is."
msgstr ""

#: of tvm.relay.transform.recast.recast:19
msgid ""
"A list of integers indicating operations that should not have their type "
"changed, counted starting with the first valid operation encountered. "
"Negative indices are allowed and indicate starting at the last layer."
msgstr ""

#: of tvm.relay.transform.recast.recast
#: tvm.relay.transform.transform.AlterOpLayout
#: tvm.relay.transform.transform.AnnotateSpans
#: tvm.relay.transform.transform.AnnotateTarget
#: tvm.relay.transform.transform.BackwardFoldScaleAxis
#: tvm.relay.transform.transform.BatchingOps
#: tvm.relay.transform.transform.CanonicalizeCast
#: tvm.relay.transform.transform.CanonicalizeOps
#: tvm.relay.transform.transform.CombineParallelBatchMatmul
#: tvm.relay.transform.transform.CombineParallelConv2D
#: tvm.relay.transform.transform.CombineParallelDense
#: tvm.relay.transform.transform.Conv2dToSparse
#: tvm.relay.transform.transform.Conv2dToSparse2
#: tvm.relay.transform.transform.ConvertLayout
#: tvm.relay.transform.transform.DeadCodeElimination
#: tvm.relay.transform.transform.Defunctionalization
#: tvm.relay.transform.transform.DefuseOps
#: tvm.relay.transform.transform.DenseToSparse
#: tvm.relay.transform.transform.DynamicToStatic
#: tvm.relay.transform.transform.EliminateCommonSubexpr
#: tvm.relay.transform.transform.EtaExpand
#: tvm.relay.transform.transform.FakeQuantizationToInteger
#: tvm.relay.transform.transform.FastMath
#: tvm.relay.transform.transform.FirstOrderGradient
#: tvm.relay.transform.transform.FoldConstant
#: tvm.relay.transform.transform.FoldConstantExpr
#: tvm.relay.transform.transform.FoldExplicitPadding
#: tvm.relay.transform.transform.FoldScaleAxis
#: tvm.relay.transform.transform.ForwardFoldScaleAxis
#: tvm.relay.transform.transform.FuseOps
#: tvm.relay.transform.transform.InferType tvm.relay.transform.transform.Inline
#: tvm.relay.transform.transform.LambdaLift
#: tvm.relay.transform.transform.LazyGradientInit
#: tvm.relay.transform.transform.Legalize
#: tvm.relay.transform.transform.MergeCompilerRegions
#: tvm.relay.transform.transform.MergeComposite
#: tvm.relay.transform.transform.PartialEvaluate
#: tvm.relay.transform.transform.PartitionGraph
#: tvm.relay.transform.transform.RemoveUnusedFunctions
#: tvm.relay.transform.transform.SimplifyExpr
#: tvm.relay.transform.transform.SimplifyFCTranspose
#: tvm.relay.transform.transform.SimplifyInference
#: tvm.relay.transform.transform.SplitArgs
#: tvm.relay.transform.transform.ToANormalForm
#: tvm.relay.transform.transform.ToANormalFormExpr
#: tvm.relay.transform.transform.ToBasicBlockNormalForm
#: tvm.relay.transform.transform.ToCPS
#: tvm.relay.transform.transform.ToGraphNormalForm
#: tvm.relay.transform.transform.ToMixedPrecision
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass
#: tvm.relay.transform.transform.build_config
#: tvm.relay.transform.transform.function_pass
#: tvm.relay.transform.transform.gradient tvm.relay.transform.transform.to_cps
#: tvm.relay.transform.transform.un_cps
msgid "Returns"
msgstr ""

#: of tvm.relay.transform.recast.recast:25
msgid "**output_expr** -- The graph after recasting to the specified datatype."
msgstr ""

#: of tvm.relay.transform.recast.recast
#: tvm.relay.transform.transform.AlterOpLayout
#: tvm.relay.transform.transform.AnnotateSpans
#: tvm.relay.transform.transform.AnnotateTarget
#: tvm.relay.transform.transform.BackwardFoldScaleAxis
#: tvm.relay.transform.transform.BatchingOps
#: tvm.relay.transform.transform.CanonicalizeCast
#: tvm.relay.transform.transform.CanonicalizeOps
#: tvm.relay.transform.transform.CombineParallelBatchMatmul
#: tvm.relay.transform.transform.CombineParallelConv2D
#: tvm.relay.transform.transform.CombineParallelDense
#: tvm.relay.transform.transform.Conv2dToSparse
#: tvm.relay.transform.transform.Conv2dToSparse2
#: tvm.relay.transform.transform.ConvertLayout
#: tvm.relay.transform.transform.DeadCodeElimination
#: tvm.relay.transform.transform.Defunctionalization
#: tvm.relay.transform.transform.DefuseOps
#: tvm.relay.transform.transform.DenseToSparse
#: tvm.relay.transform.transform.DynamicToStatic
#: tvm.relay.transform.transform.EliminateCommonSubexpr
#: tvm.relay.transform.transform.EtaExpand
#: tvm.relay.transform.transform.FakeQuantizationToInteger
#: tvm.relay.transform.transform.FastMath
#: tvm.relay.transform.transform.FirstOrderGradient
#: tvm.relay.transform.transform.FoldConstant
#: tvm.relay.transform.transform.FoldConstantExpr
#: tvm.relay.transform.transform.FoldExplicitPadding
#: tvm.relay.transform.transform.FoldScaleAxis
#: tvm.relay.transform.transform.ForwardFoldScaleAxis
#: tvm.relay.transform.transform.FuseOps
#: tvm.relay.transform.transform.InferType tvm.relay.transform.transform.Inline
#: tvm.relay.transform.transform.LambdaLift
#: tvm.relay.transform.transform.LazyGradientInit
#: tvm.relay.transform.transform.Legalize
#: tvm.relay.transform.transform.MergeCompilerRegions
#: tvm.relay.transform.transform.MergeComposite
#: tvm.relay.transform.transform.PartialEvaluate
#: tvm.relay.transform.transform.PartitionGraph
#: tvm.relay.transform.transform.RemoveUnusedFunctions
#: tvm.relay.transform.transform.SimplifyExpr
#: tvm.relay.transform.transform.SimplifyFCTranspose
#: tvm.relay.transform.transform.SimplifyInference
#: tvm.relay.transform.transform.SplitArgs
#: tvm.relay.transform.transform.ToANormalForm
#: tvm.relay.transform.transform.ToANormalFormExpr
#: tvm.relay.transform.transform.ToBasicBlockNormalForm
#: tvm.relay.transform.transform.ToCPS
#: tvm.relay.transform.transform.ToGraphNormalForm
#: tvm.relay.transform.transform.ToMixedPrecision
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass
#: tvm.relay.transform.transform.build_config
#: tvm.relay.transform.transform.function_pass
#: tvm.relay.transform.transform.gradient tvm.relay.transform.transform.to_cps
#: tvm.relay.transform.transform.un_cps
msgid "Return type"
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1
msgid ""
"Alternate the layouts of operators or replace primitive operators with "
"other expressions. This pass can be used for computing convolution in "
"custom layouts or other general weight pre-transformation."
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:6
msgid "**ret** -- The registered pass that alters the layout of operators."
msgstr ""

#: of tvm.relay.transform.transform.AnnotateSpans:5
msgid "**ret** -- The registered AnnotateSpans pass."
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:4
msgid "The list of target compilers used for codegen."
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:6
msgid ""
"If True then non-call ops also will be annotated with targets If False "
"then non-call ops will not be processed"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:10
msgid ""
"**ret** -- The annotated pass that wrapps ops with subgraph_start and "
"subgraph_end."
msgstr ""

#: of tvm.relay.transform.transform.BackwardFoldScaleAxis:3
msgid "**ret** -- The registered pass to backward fold expressions."
msgstr ""

#: of tvm.relay.transform.transform.BackwardFoldScaleAxis:8
msgid ""
"It is recommended to call backward_fold_scale_axis before using "
"forward_fold_scale_axis as backward folding targets the common conv->bn "
"pattern."
msgstr ""

#: of tvm.relay.transform.transform.BatchingOps:3
msgid ""
"**ret** -- The sequential pass which apply batching for different "
"operator types."
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeCast:3
msgid "**ret** -- The registered pass that canonicalizes cast expression."
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeOps:1
msgid ""
"Canonicalize special operators to basic operators. This can simplify "
"followed analysis, e.g. expanding bias_add to expand_dims and "
"broadcast_add."
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeOps:5
msgid "**ret** -- The registered pass performing the canonicalization."
msgstr ""

#: of
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:3
msgid ""
"A dictionary of all the params to change. The keys are all params, and "
"the values are which dimension hold the batch."
msgstr ""

#: of
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:6
msgid "The batch size to change to."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:21
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:9
msgid "**pass** -- The pass."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:1
msgid "Combine multiple batch matmul operators into one. For example:"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:9
#: tvm.relay.transform.transform.CombineParallelDense:9
msgid "Would become:"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:19
#: tvm.relay.transform.transform.CombineParallelConv2D:3
#: tvm.relay.transform.transform.CombineParallelDense:25
msgid ""
"The minimum number of required parallel branches for performing this "
"optimization."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:23
#: tvm.relay.transform.transform.CombineParallelDense:32
msgid "**ret** -- The registered pass that combines parallel dense operators."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelConv2D:7
msgid "**ret** -- The registered pass that combines parallel conv2d operators."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:1
msgid "Combine multiple dense operators into one. For example:"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:17
msgid "or (if to_batch=False)"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:28
msgid ""
"If True, combine parallel dense ops into batch_matmul op. If False, "
"combine parallel dense ops into dense op."
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:3
#: tvm.relay.transform.transform.DenseToSparse:5
msgid "Names of weights which qualified sparse contrains"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:5
#: tvm.relay.transform.transform.DenseToSparse:7
msgid "Weights shape in BSR format."
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:7
#: tvm.relay.transform.transform.Conv2dToSparse2:3
msgid "layout of data"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:10
#: tvm.relay.transform.transform.Conv2dToSparse2:8
#: tvm.relay.transform.transform.DenseToSparse:10
msgid "**ret** -- The registered DenseToSparse pass."
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse2:5
msgid "kernel size of conv2d"
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:1
msgid ""
"Given a dest layout, this pass transforms the expr such that most of the "
"ops input data layout is changed to the dest layout. In ideal situation, "
"there are only 2 layout transforms, one at the start and one at the end."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:5
msgid ""
"This pass is not a part of relay.build and is expected to be called "
"between framework-relay parser and relay.build call. This is very helpful"
" for hardware backends that support/prefer only type of data layout."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:9
msgid "RFC - https://discuss.tvm.apache.org/t/layout-conversion-pass/4009"
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:11
msgid ""
"This pass uses most of the AlterOpLayout and InferCorrectLayout "
"infrastructure. We can define new layouts for conv2d ops for now. Most of"
" the other operators try to adapt to their input layout using the "
"InferCorrectLayout infrastructure."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:15
msgid ""
"Specify a mapping of operator names to a list of layouts to convert to, "
"in the order defined by the operator. An example for nn.conv2d could be: "
"{\"nn.conv2d\", [\"NHWC\", \"OHWI]}, where the first item in the list "
"specifies the data layout and the second specifies the kernel layout."
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:3
msgid "Whether to inline binding that occurs only once."
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:6
msgid ""
"**ret** -- The registered pass that eliminates the dead code in a Relay "
"program."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:4
msgid ""
"At each call site, the function is cloned and type parameters are "
"substituted in. Function arguments are encoded as datatypes and "
"additional apply functions are used for application."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:8
msgid ""
"The input function, which should not be polymorphic or be higher-order. "
"This is because all types must be known and we can't encode function "
"arguments to the program itself."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:12
msgid ""
"The IRModule containing function and type definitions, which is also "
"mutated during this pass."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:16
msgid "**expr** -- The output function."
msgstr ""

#: of tvm.relay.transform.transform.DefuseOps:1
msgid ""
"The inverse operation of FuseOps. It transforms a fused program returned "
"by FuseOps into the program before FuseOps. (i.e., x == "
"DefuseOps(FuseOps(x)))"
msgstr ""

#: of tvm.relay.transform.transform.DefuseOps:4
msgid "**ret** -- The registered pass for operator defusion."
msgstr ""

#: of tvm.relay.transform.transform.DynamicToStatic:3
msgid "**ret** -- The registered pass for dynamic->static conversion."
msgstr ""

#: of tvm.relay.transform.transform.EliminateCommonSubexpr:3
msgid ""
"The callback function that decides whether an expression should be "
"skipped."
msgstr ""

#: of tvm.relay.transform.transform.EliminateCommonSubexpr:7
msgid "**ret** -- The registered pass that eliminates common subexpressions."
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:3
msgid "Whether to expand constructors."
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:5
msgid "Whether to expand global variables."
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:8
msgid "**ret** -- The registered pass that eta expands an expression."
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:15
msgid ""
"where ``q == qnn.quantize`` and ``dq = qnn.dequantize`` and rewrite them "
"into integer versions of ``op1`` and ``op2``"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:18
msgid "Rules for rewriting indivdual ops are in fake_quantization_to_integer.py"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:20
#: tvm.relay.transform.transform.SimplifyExpr:3
msgid "**ret** -- The registered SimplifyExpr pass."
msgstr ""

#: of tvm.relay.transform.transform.FastMath:3
msgid "**ret** -- The registered pass to perform fast math operations."
msgstr ""

#: of tvm.relay.transform.transform.FirstOrderGradient:1
msgid ""
"Transforms all global functions in the module to return the original "
"result, paired with the gradients of the inputs. This pass transforms "
"each global function independently and does not support interprocedural "
"AD. Additionally, this pass does not support any control-flow or "
"references, and should only be used on pure data-flow graphs."
msgstr ""

#: of tvm.relay.transform.transform.FirstOrderGradient:6
msgid "**ret** -- The registered FirstOrderGradient pass."
msgstr ""

#: of tvm.relay.transform.transform.FoldConstant:3
#: tvm.relay.transform.transform.SplitArgs:3
msgid "**ret** -- The registered pass for constant folding."
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:1
msgid ""
"Fold the constant expressions in a Relay program. :param expr: The "
"expression to fold :type expr: Expr :param mod: The module the expr lives"
" in (for global calls) :type mod: IRModule"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:7
msgid "**new_expr** -- The expr after Constant Folding"
msgstr ""

#: of tvm.relay.transform.transform.FoldExplicitPadding:4
msgid "**ret** -- The registered ImplicitPadding pass."
msgstr ""

#: of tvm.relay.transform.transform.FoldScaleAxis:1
msgid ""
"Fold the scaling of axis into weights of conv2d/dense. This pass will "
"invoke both forward and backward scale folding."
msgstr ""

#: of tvm.relay.transform.transform.FoldScaleAxis:4
msgid "**ret** -- The registered pass to fold expressions."
msgstr ""

#: of tvm.relay.transform.transform.FoldScaleAxis:9
msgid ""
"Internally, we will call backward_fold_scale_axis before using "
"forward_fold_scale_axis as backward folding targets the common conv->bn "
"pattern."
msgstr ""

#: of tvm.relay.transform.transform.ForwardFoldScaleAxis:3
msgid "**ret** -- The registered pass to forward fold expressions."
msgstr ""

#: of tvm.relay.transform.transform.ForwardFoldScaleAxis:8
msgid ""
"It is recommended to call backward_fold_scale_axis before using "
"forward_fold_scale_axis, as backward folding targets the common conv->bn "
"pattern."
msgstr ""

#: of tvm.relay.transform.transform.FunctionPass:1
msgid ""
"A pass that works on each tvm.relay.Function in a module. A function pass"
" class should be created through `function_pass`."
msgstr ""

#: of tvm.relay.transform.transform.FuseOps:3
msgid ""
"The level of fuse optimization. -1 indicates that the level will be "
"inferred from pass context."
msgstr ""

#: of tvm.relay.transform.transform.FuseOps:7
msgid "**ret** -- The registered pass for operator fusion."
msgstr ""

#: of tvm.relay.transform.transform.InferType:3
msgid "**ret** -- The registered type inference pass."
msgstr ""

#: of tvm.relay.transform.transform.Inline:1
msgid ""
"Perform inlining on the given Relay IR module. The global functions that "
"are marked as `inline` should be always inlined. A cost model will be "
"needed in the future to decide if it is profitable to inline the "
"function."
msgstr ""

#: of tvm.relay.transform.transform.Inline:5
msgid ""
"**ret** -- The registered pass that performs inlining for a Relay IR "
"module."
msgstr ""

#: of tvm.relay.transform.transform.LambdaLift:3
msgid "**ret** -- The registered pass that lifts the lambda function."
msgstr ""

#: of tvm.relay.transform.transform.LazyGradientInit:4
msgid ""
"**ret** -- A pass which delays and/or reduces memory allocation, by "
"lazily allocating 0 or one filled tensors."
msgstr ""

#: of tvm.relay.transform.transform.Legalize:1
msgid ""
"Legalizes an expression with another expression. This pass can be used to"
" replace an expr with another expr for target dependent optimizations. "
"For example, one expr, though semnatically equivalent to the other, can "
"have better performance on a target. This pass can be used to legalize "
"the expr in a target-dependent manner."
msgstr ""

#: of tvm.relay.transform.transform.Legalize:7
msgid "The Op's attr name which corresponds to the legalize rule function."
msgstr ""

#: of tvm.relay.transform.transform.Legalize:10
msgid "**ret** -- The registered pass that rewrites an expr."
msgstr ""

#: of tvm.relay.transform.transform.MergeCompilerRegions:3
msgid "**ret** -- The registered pass that merges compiler regions."
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:3
msgid ""
"A list of (pattern_name, pattern, check) tuples. The order of the "
"patterns in the list will determine the order of priority in which they "
"are matched. 'check' is a function to check whether an extracted pattern "
"matches. It can be implemented by pattern writer but if not specified it "
"will always return True."
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:11
msgid ""
"**ret** -- The registered pass that merges operators into a single "
"composite relay function."
msgstr ""

#: of tvm.relay.transform.transform.PartialEvaluate:5
msgid ""
"This transformation could be either `Module -> Module` or `Expr -> Expr`."
" It will directly transform the input expression to a new one if the "
"target expression is provided. Otherwise, it will rely on the pass "
"manager to carry out transformation."
msgstr ""

#: of tvm.relay.transform.transform.PartialEvaluate:10
msgid ""
"**ret** -- The registered pass that performs partial evaluation on an "
"expression."
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:4
msgid "**ret** -- The registered pass that partitions the Relay program."
msgstr ""

#: of tvm.relay.transform.transform.PlanDevices:1
msgid ""
"Uses existing \"on_device\" and \"device_copy\" CallNodes to infer the "
"device on which every Relay sub-expression should run (and the result "
"stored). Captures the result of that analysis using new \"on_device\" and"
" \"device_copy\" CallNodes. Note that the device_id of the default_device"
" is ignored."
msgstr ""

#: of tvm.relay.transform.transform.RemoveUnusedFunctions:3
msgid "The set of entry functions to start from."
msgstr ""

#: of tvm.relay.transform.transform.RemoveUnusedFunctions:6
msgid "**ret** -- The registered pass to remove unused functions."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyFCTranspose:4
msgid ""
"Names of weights which qualified ```y = nn.dense(x, transpose(w, [1, "
"0]))``` This parameter is generated by ```analysis.search_fc_transpose```"
" function"
msgstr ""

#: of tvm.relay.transform.transform.SimplifyFCTranspose:8
msgid "**ret** -- The registered SimplifyFCTranspose pass."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyInference:1
msgid ""
"Simplify the data-flow graph for inference phase. An simplified "
"expression which is semantically equal to the input expression will be "
"returned."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyInference:4
msgid ""
"Note that batch norms will only be simplified if their result is indexed "
"at tuple index 0."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyInference:7
msgid "**ret** -- The registered pass to perform operator simplification."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalForm:1
msgid ""
"Turn Graph Normal Form expression into A Normal Form Expression. The "
"scope of the root expression is the global scope. The scope of any non "
"root expression is the least common ancestor of all it's scope. Values "
"are ordered by post-DFS order in each scope."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalForm:6
msgid ""
"**ret** -- The registered pass that transforms an expression into A "
"Normal Form."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:3
msgid "The graph expression."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:6
msgid "**ret** -- The transformed expresion."
msgstr ""

#: of tvm.relay.transform.transform.ToBasicBlockNormalForm:1
msgid ""
"Turn an expression to Basic Block Normal Form. We define a block as a "
"group of expressions implied by the scope structure. Each graph node can "
"only belong to a single block. For any value that is being used in "
"multiple blocks, it has to be referred by a Var which is defined in a "
"block, whose scope is the least common ancestor of blocks this value is "
"used."
msgstr ""

#: of tvm.relay.transform.transform.ToBasicBlockNormalForm:8
msgid ""
"**ret** -- The registered pass that transforms an expression into Basic "
"Block Normal Form."
msgstr ""

#: of tvm.relay.transform.transform.ToCPS:3
#: tvm.relay.transform.transform.to_cps:3
msgid "Every intermediate compute will be passed to a continuation."
msgstr ""

#: of tvm.relay.transform.transform.ToCPS:5
msgid "**result** -- The registered pass that transforms an expression into CPS."
msgstr ""

#: of tvm.relay.transform.transform.ToGraphNormalForm:3
msgid ""
"**ret** -- The registered pass that transforms an expression into Graph "
"Normal Form."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:1
msgid ""
"Automatic mixed precision rewriter. Rewrite an FP32 relay graph into a "
"version where as many operations as possible are in the target "
"mixed_precision_type."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:4
msgid "The target datatype to transform operations in the graph to use."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:6
msgid ""
"Determines how to handle ops not registered with "
"FTVMMixedPrecisionConversionType   0: Does not allow any missing ops. "
"Will throw errors when encountering any.   1: Allow missing ops but emit "
"warnings.   2: Allow missing ops and silently ignore them."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:9
msgid ""
"Determines how to handle ops not registered with "
"FTVMMixedPrecisionConversionType"
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:8
msgid ""
"0: Does not allow any missing ops. Will throw errors when encountering "
"any. 1: Allow missing ops but emit warnings. 2: Allow missing ops and "
"silently ignore them."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:13
msgid "**ret** -- The registered pass."
msgstr ""

#: of tvm.relay.transform.transform.build_config:1
msgid ""
"Configure the build behavior by setting config variables. This function "
"will be deprecated in TVM v0.7. Instead, we should directly use "
"tvm.transform.PassContext."
msgstr ""

#: of tvm.relay.transform.transform.build_config:5
msgid ""
"Optimization level. The optimization pass name and level are as the "
"following:  .. code-block:: python      OPT_PASS_LEVEL = {         "
"\"SimplifyInference\": 0,         \"OpFusion\": 1,         "
"\"FoldConstant\": 2,         \"FoldScaleAxis\": 3,         "
"\"AlterOpLayout\": 3,         \"CanonicalizeOps\": 3,         "
"\"CanonicalizeCast\": 3,         \"EliminateCommonSubexpr\": 3,         "
"\"CombineParallelConv2D\": 4,         \"CombineParallelDense\": 4,"
"         \"CombineParallelBatchMatmul\": 4,         \"FastMath\": 4     }"
msgstr ""

#: of tvm.relay.transform.transform.build_config:5
msgid ""
"Optimization level. The optimization pass name and level are as the "
"following:"
msgstr ""

#: of tvm.relay.transform.transform.build_config:25
msgid "Optimization passes that are required regardless of optimization level."
msgstr ""

#: of tvm.relay.transform.transform.build_config:27
msgid "Optimization passes to be disabled during optimization."
msgstr ""

#: of tvm.relay.transform.transform.build_config:29
msgid "A tracing function for debugging or introspection."
msgstr ""

#: of tvm.relay.transform.transform.build_config:32
msgid "**pass_context** -- The pass context for optimizations."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:3
msgid ""
"This function returns a callback when pass_func is provided. Otherwise, "
"it returns the created function pass using the given optimization "
"function."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:7
msgid "The transformation function or class."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:9
msgid "The optimization level of this module pass."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:11
msgid ""
"The name of the function pass. The name could be empty. In this case, the"
" name of the optimization function will be used as the pass name."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:14
msgid "The list of passes that the module pass is dependent on."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:17
msgid ""
"**create_function_pass** -- A decorator will be returned if pass_func is "
"not provided, otherwise return the decorated result. The returned "
"decorator has two behaviors depending on the input: A new FunctionPass "
"will be returned when we decorate a pass function. A new FunctionPass "
"class will be returned when we decorate a class type."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:25
msgid "Examples"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:26
msgid "The following code block decorates a function pass class."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:49
msgid ""
"The following code creates a function pass by decorating a user defined "
"transform function."
msgstr ""

#: of tvm.relay.transform.transform.gradient:5
msgid "The input expression, which is a Function or a GlobalVar."
msgstr ""

#: of tvm.relay.transform.transform.gradient:9
msgid ""
"The mode of the automatic differentiation algorithm. 'first_order' only "
"works on first order code, but will not produce reference nor closure. "
"'higher_order' works on all code using reference and closure."
msgstr ""

#: of tvm.relay.transform.transform.gradient:15
msgid "**expr** -- The transformed expression."
msgstr ""

#: of tvm.relay.transform.transform.to_cps:5
msgid "The input function."
msgstr ""

#: of tvm.relay.transform.transform.to_cps:7
msgid "The global module."
msgstr ""

#: of tvm.relay.transform.transform.to_cps:10
msgid "**result** -- The output function."
msgstr ""

#: of tvm.relay.transform.transform.un_cps:4
msgid "Note that this will not give the exact same interface as before cps:"
msgstr ""

#: of tvm.relay.transform.transform.un_cps:4
msgid "If the input/output is higher order, they will still be in cps form."
msgstr ""

#: of tvm.relay.transform.transform.un_cps:6
msgid "The input function"
msgstr ""

#: of tvm.relay.transform.transform.un_cps:9
msgid "**result** -- The output function"
msgstr ""


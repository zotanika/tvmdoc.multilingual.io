# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:20+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/reference/api/python/relay/nn.rst:19
msgid "tvm.relay.op.nn"
msgstr ""

#: of tvm.relay.op.nn:1
msgid "Neural network related operators."
msgstr ""

#: of tvm.relay.op.nn:1
msgid "**Classes:**"
msgstr ""

#: of tvm.relay.op.nn:1:<autosummary>:1
msgid ":py:obj:`Constant <tvm.relay.op.nn.Constant>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1 tvm.relay.op.nn:1:<autosummary>:1
msgid "A constant expression in Relay."
msgstr ""

#: of tvm.relay.op.nn:1:<autosummary>:1
msgid ":py:obj:`Expr <tvm.relay.op.nn.Expr>`\\"
msgstr ""

#: of tvm.relay.op.nn:1:<autosummary>:1
msgid "alias of :py:class:`tvm.ir.expr.RelayExpr`"
msgstr ""

#: of tvm.relay.op.nn:1
msgid "**Functions:**"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_avg_pool1d <tvm.relay.op.nn.adaptive_avg_pool1d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "1D adaptive average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_avg_pool2d <tvm.relay.op.nn.adaptive_avg_pool2d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "2D adaptive average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_avg_pool3d <tvm.relay.op.nn.adaptive_avg_pool3d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "3D adaptive avg pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_max_pool1d <tvm.relay.op.nn.adaptive_max_pool1d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "1D adaptive max pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_max_pool2d <tvm.relay.op.nn.adaptive_max_pool2d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "2D adaptive max pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_max_pool3d <tvm.relay.op.nn.adaptive_max_pool3d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "3D adaptive max pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool1d <tvm.relay.op.nn.avg_pool1d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.avg_pool1d:1
msgid "1D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool2d <tvm.relay.op.nn.avg_pool2d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.avg_pool2d:1
msgid "2D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool2d_grad <tvm.relay.op.nn.avg_pool2d_grad>`\\ "
"\\(out\\_grad\\, data\\[\\, pool\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.avg_pool2d_grad:1
msgid "Gradient of 2D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool3d <tvm.relay.op.nn.avg_pool3d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.avg_pool3d:1
msgid "3D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`batch_flatten <tvm.relay.op.nn.batch_flatten>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.batch_flatten:1
msgid "BatchFlatten."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`batch_matmul <tvm.relay.op.nn.batch_matmul>`\\ \\(tensor\\_a\\, "
"tensor\\_b\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.batch_matmul:1
msgid "Compute batch matrix multiplication of `tensor_a` and `tensor_b`."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`batch_norm <tvm.relay.op.nn.batch_norm>`\\ \\(data\\, gamma\\, "
"beta\\, moving\\_mean\\, ...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`batch_to_space_nd <tvm.relay.op.nn.batch_to_space_nd>`\\ "
"\\(data\\, block\\_shape\\, crops\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.batch_to_space_nd:1
msgid "Reshape the batch dimension into spatial dimensions."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bias_add <tvm.relay.op.nn.bias_add>`\\ \\(data\\, bias\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.bias_add:1
msgid "add_bias operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bitpack <tvm.relay.op.nn.bitpack>`\\ \\(data\\[\\, bits\\, "
"pack\\_axis\\, bit\\_axis\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.bitpack:1
msgid "Tensor packing for bitserial operations."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_conv2d <tvm.relay.op.nn.bitserial_conv2d>`\\ "
"\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.bitserial_conv2d:1
msgid "2D convolution using bitserial computation."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_dense <tvm.relay.op.nn.bitserial_dense>`\\ \\(data\\, "
"weight\\[\\, units\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Bitserial Dense operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`const <tvm.relay.op.nn.const>`\\ \\(value\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.expr.const:1
msgid "Create a constant value."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_gemm_weight_transform "
"<tvm.relay.op.nn.contrib_conv2d_gemm_weight_transform>`\\ \\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:1
msgid "Weight Transformation part for 2D convolution with gemm algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_gemm_without_weight_transform "
"<tvm.relay.op.nn.contrib_conv2d_gemm_without_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:1
msgid "2D convolution with gemm algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_nchwc <tvm.relay.op.nn.contrib_conv2d_nchwc>`\\ "
"\\(data\\, kernel\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:1
msgid "Variant of 2D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_winograd_nnpack_weight_transform "
"<tvm.relay.op.nn.contrib_conv2d_winograd_nnpack_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:1
msgid "Weight Transformation part for 2D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_winograd_weight_transform "
"<tvm.relay.op.nn.contrib_conv2d_winograd_weight_transform>`\\ \\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_winograd_without_weight_transform "
"<tvm.relay.op.nn.contrib_conv2d_winograd_without_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:1
msgid "2D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv3d_winograd_weight_transform "
"<tvm.relay.op.nn.contrib_conv3d_winograd_weight_transform>`\\ \\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:1
msgid "Weight Transformation part for 3D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv3d_winograd_without_weight_transform "
"<tvm.relay.op.nn.contrib_conv3d_winograd_without_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:1
msgid "3D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_dense_pack <tvm.relay.op.nn.contrib_dense_pack>`\\ "
"\\(data\\, weight\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Dense operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_depthwise_conv2d_nchwc "
"<tvm.relay.op.nn.contrib_depthwise_conv2d_nchwc>`\\ \\(data\\, kernel\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:1
msgid "Variant of 2D depthwise convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv1d <tvm.relay.op.nn.conv1d>`\\ \\(data\\, weight\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.conv1d:1
msgid "1D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv1d_transpose <tvm.relay.op.nn.conv1d_transpose>`\\ "
"\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv1d_transpose:1
msgid "One dimensional transposed convolution operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv2d <tvm.relay.op.nn.conv2d>`\\ \\(data\\, weight\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.conv2d:1
msgid "2D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_transpose <tvm.relay.op.nn.conv2d_transpose>`\\ "
"\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv2d_transpose:1
msgid "Two dimensional transposed convolution operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv3d <tvm.relay.op.nn.conv3d>`\\ \\(data\\, weight\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.conv3d:1
msgid "3D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_transpose <tvm.relay.op.nn.conv3d_transpose>`\\ "
"\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv3d_transpose:1
msgid "3D transpose convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`correlation <tvm.relay.op.nn.correlation>`\\ \\(data1\\, "
"data2\\, kernel\\_size\\, ...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.correlation:1
msgid "Applies correlation to inputs."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`cross_entropy <tvm.relay.op.nn.cross_entropy>`\\ "
"\\(predictions\\, targets\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.cross_entropy:1
msgid "CrossEntropy without logits."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`cross_entropy_with_logits "
"<tvm.relay.op.nn.cross_entropy_with_logits>`\\ \\(predictions\\, "
"targets\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:1
msgid "CrossEntropy with logits."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`deformable_conv2d <tvm.relay.op.nn.deformable_conv2d>`\\ "
"\\(data\\, offset\\, weight\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.deformable_conv2d:1
msgid "Deformable 2d convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`dense <tvm.relay.op.nn.dense>`\\ \\(data\\, weight\\[\\, "
"units\\, out\\_dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`depth_to_space <tvm.relay.op.nn.depth_to_space>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\, mode\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.depth_to_space:1
msgid "Convert channels into spatial blocks."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`dilate <tvm.relay.op.nn.dilate>`\\ \\(data\\, strides\\[\\, "
"dilation\\_value\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.dilate:1
msgid "Dilate data with given dilation value (0 by default)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`dropout <tvm.relay.op.nn.dropout>`\\ \\(data\\[\\, rate\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.dropout:1
#: tvm.relay.op.nn.nn.dropout_raw:1
msgid "Applies the dropout operation to the input array."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`dropout_raw <tvm.relay.op.nn.dropout_raw>`\\ \\(data\\[\\, "
"rate\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`fast_softmax <tvm.relay.op.nn.fast_softmax>`\\ \\(data\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.softmax:1
msgid "Computes softmax."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`fifo_buffer <tvm.relay.op.nn.fifo_buffer>`\\ \\(data\\, "
"buffer\\, axis\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.fifo_buffer:1
msgid "FIFO buffer to enable computation reuse in CNNs with sliding indow input"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple1d <tvm.relay.op.nn.get_pad_tuple1d>`\\ "
"\\(padding\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.utils.get_pad_tuple1d:1
msgid ""
"Common code to get the 1 dimensional pad option :param padding: Padding "
"size :type padding: Union[int, Tuple[int, ...]]"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple2d <tvm.relay.op.nn.get_pad_tuple2d>`\\ "
"\\(padding\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.utils.get_pad_tuple2d:1
#: tvm.relay.op.nn.utils.get_pad_tuple3d:1
msgid ""
"Common code to get the pad option :param padding: Padding size :type "
"padding: Union[int, Tuple[int, ...]]"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple3d <tvm.relay.op.nn.get_pad_tuple3d>`\\ "
"\\(padding\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_avg_pool1d <tvm.relay.op.nn.global_avg_pool1d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_avg_pool1d:1
msgid "1D global average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_avg_pool2d <tvm.relay.op.nn.global_avg_pool2d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_avg_pool2d:1
msgid "2D global average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_avg_pool3d <tvm.relay.op.nn.global_avg_pool3d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_avg_pool3d:1
msgid "3D global average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_max_pool1d <tvm.relay.op.nn.global_max_pool1d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_max_pool1d:1
msgid "1D global maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_max_pool2d <tvm.relay.op.nn.global_max_pool2d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_max_pool2d:1
msgid "2D global maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_max_pool3d <tvm.relay.op.nn.global_max_pool3d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_max_pool3d:1
msgid "3D global maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`group_norm <tvm.relay.op.nn.group_norm>`\\ \\(data\\, gamma\\, "
"beta\\, num\\_groups\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
"Group normalization normalizes over group of channels for each training "
"examples."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`instance_norm <tvm.relay.op.nn.instance_norm>`\\ \\(data\\, "
"gamma\\, beta\\[\\, axis\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.instance_norm:1
msgid ""
"Instance Normalization (Ulyanov and et al., 2016) Applies instance "
"normalization to the n-dimensional input array."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`l2_normalize <tvm.relay.op.nn.l2_normalize>`\\ \\(data\\, "
"eps\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.l2_normalize:1
msgid "Perform L2 normalization on the input data"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`layer_norm <tvm.relay.op.nn.layer_norm>`\\ \\(data\\, gamma\\, "
"beta\\[\\, axis\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Layer normalization (Lei Ba and et al., 2016)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`leaky_relu <tvm.relay.op.nn.leaky_relu>`\\ \\(data\\[\\, "
"alpha\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.leaky_relu:1
#: tvm.relay.op.nn.nn.prelu:1
msgid ""
"This operator takes data as input and does Leaky version of a Rectified "
"Linear Unit."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`log_softmax <tvm.relay.op.nn.log_softmax>`\\ \\(data\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.log_softmax:1
msgid "Computes log softmax."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`lrn <tvm.relay.op.nn.lrn>`\\ \\(data\\[\\, size\\, axis\\, "
"bias\\, alpha\\, beta\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.lrn:1
msgid "This operator takes data as input and does local response normalization."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`matmul <tvm.relay.op.nn.matmul>`\\ \\(tensor\\_a\\, "
"tensor\\_b\\[\\, units\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Matmul operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool1d <tvm.relay.op.nn.max_pool1d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.max_pool1d:1
msgid "1D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool2d <tvm.relay.op.nn.max_pool2d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.max_pool2d:1
msgid "2D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool2d_grad <tvm.relay.op.nn.max_pool2d_grad>`\\ "
"\\(out\\_grad\\, data\\[\\, pool\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.max_pool2d_grad:1
msgid "Gradient of 2D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool3d <tvm.relay.op.nn.max_pool3d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.max_pool3d:1
msgid "3D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`mirror_pad <tvm.relay.op.nn.mirror_pad>`\\ \\(data\\, "
"pad\\_width\\[\\, mode\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.mirror_pad:1
msgid "MirrorPadding"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`nll_loss <tvm.relay.op.nn.nll_loss>`\\ \\(predictions\\, "
"targets\\, weights\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.nll_loss:1
msgid "Negative log likelihood loss."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`pad <tvm.relay.op.nn.pad>`\\ \\(data\\, pad\\_width\\[\\, "
"pad\\_value\\, pad\\_mode\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.pad:1
msgid "Padding"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`prelu <tvm.relay.op.nn.prelu>`\\ \\(data\\, alpha\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`relu <tvm.relay.op.nn.relu>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.relu:1
msgid "Rectified linear unit."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`softmax <tvm.relay.op.nn.softmax>`\\ \\(data\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`space_to_batch_nd <tvm.relay.op.nn.space_to_batch_nd>`\\ "
"\\(data\\, block\\_shape\\, paddings\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.space_to_batch_nd:1
msgid ""
"Divide spatial dimensions of the data into a grid of blocks and "
"interleave them into batch dim."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`space_to_depth <tvm.relay.op.nn.space_to_depth>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.space_to_depth:1
msgid "Convert spatial blocks into channels."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`sparse_add <tvm.relay.op.nn.sparse_add>`\\ \\(dense\\_mat\\, "
"sparse\\_mat\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.sparse_add:1
msgid ""
"Computes the matrix addition of `dense_mat` and `sparse_mat`, where "
"`dense_mat` is a dense matrix and `sparse_mat` is a sparse (CSR) "
"namedtuple with fields `data`, `indices`, and `indptr`."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`sparse_dense <tvm.relay.op.nn.sparse_dense>`\\ \\(dense\\_mat\\,"
" sparse\\_mat\\[\\, sparse\\_lhs\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.sparse_dense:1
msgid ""
"Computes the matrix multiplication of `dense_mat` and `sparse_mat`, where"
" `dense_mat` is a dense matrix and `sparse_mat` is a sparse (either BSR "
"or CSR) namedtuple with fields `data`, `indices`, and `indptr`."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`sparse_transpose <tvm.relay.op.nn.sparse_transpose>`\\ \\(x\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.sparse_transpose:1
msgid ""
"Computes the fast matrix transpose of x, where x is a sparse tensor in "
"CSR format (represented as a namedtuple with fields `data`, `indices`, "
"and `indptr`)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`upsampling <tvm.relay.op.nn.upsampling>`\\ \\(data\\[\\, "
"scale\\_h\\, scale\\_w\\, layout\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1 tvm.relay.op.nn.nn.upsampling:1
msgid "Upsampling."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`upsampling3d <tvm.relay.op.nn.upsampling3d>`\\ \\(data\\[\\, "
"scale\\_d\\, scale\\_h\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.upsampling3d:1
msgid "3D Upsampling."
msgstr ""

#: of tvm.relay.expr.Constant tvm.relay.expr.const
#: tvm.relay.op.nn.nn.adaptive_avg_pool1d
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d
#: tvm.relay.op.nn.nn.adaptive_max_pool1d
#: tvm.relay.op.nn.nn.adaptive_max_pool2d
#: tvm.relay.op.nn.nn.adaptive_max_pool3d tvm.relay.op.nn.nn.avg_pool1d
#: tvm.relay.op.nn.nn.avg_pool2d tvm.relay.op.nn.nn.avg_pool2d_grad
#: tvm.relay.op.nn.nn.avg_pool3d tvm.relay.op.nn.nn.batch_flatten
#: tvm.relay.op.nn.nn.batch_matmul tvm.relay.op.nn.nn.batch_norm
#: tvm.relay.op.nn.nn.batch_to_space_nd tvm.relay.op.nn.nn.bias_add
#: tvm.relay.op.nn.nn.bitpack tvm.relay.op.nn.nn.bitserial_conv2d
#: tvm.relay.op.nn.nn.bitserial_dense
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_dense_pack
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc tvm.relay.op.nn.nn.conv1d
#: tvm.relay.op.nn.nn.conv1d_transpose tvm.relay.op.nn.nn.conv2d
#: tvm.relay.op.nn.nn.conv2d_transpose tvm.relay.op.nn.nn.conv3d
#: tvm.relay.op.nn.nn.conv3d_transpose tvm.relay.op.nn.nn.correlation
#: tvm.relay.op.nn.nn.cross_entropy
#: tvm.relay.op.nn.nn.cross_entropy_with_logits
#: tvm.relay.op.nn.nn.deformable_conv2d tvm.relay.op.nn.nn.dense
#: tvm.relay.op.nn.nn.depth_to_space tvm.relay.op.nn.nn.dilate
#: tvm.relay.op.nn.nn.dropout tvm.relay.op.nn.nn.dropout_raw
#: tvm.relay.op.nn.nn.fast_softmax tvm.relay.op.nn.nn.fifo_buffer
#: tvm.relay.op.nn.nn.global_avg_pool1d tvm.relay.op.nn.nn.global_avg_pool2d
#: tvm.relay.op.nn.nn.global_avg_pool3d tvm.relay.op.nn.nn.global_max_pool1d
#: tvm.relay.op.nn.nn.global_max_pool2d tvm.relay.op.nn.nn.global_max_pool3d
#: tvm.relay.op.nn.nn.group_norm tvm.relay.op.nn.nn.instance_norm
#: tvm.relay.op.nn.nn.l2_normalize tvm.relay.op.nn.nn.layer_norm
#: tvm.relay.op.nn.nn.leaky_relu tvm.relay.op.nn.nn.log_softmax
#: tvm.relay.op.nn.nn.lrn tvm.relay.op.nn.nn.matmul
#: tvm.relay.op.nn.nn.max_pool1d tvm.relay.op.nn.nn.max_pool2d
#: tvm.relay.op.nn.nn.max_pool2d_grad tvm.relay.op.nn.nn.max_pool3d
#: tvm.relay.op.nn.nn.mirror_pad tvm.relay.op.nn.nn.nll_loss
#: tvm.relay.op.nn.nn.pad tvm.relay.op.nn.nn.prelu tvm.relay.op.nn.nn.relu
#: tvm.relay.op.nn.nn.softmax tvm.relay.op.nn.nn.space_to_batch_nd
#: tvm.relay.op.nn.nn.space_to_depth tvm.relay.op.nn.nn.sparse_add
#: tvm.relay.op.nn.nn.sparse_dense tvm.relay.op.nn.nn.sparse_transpose
#: tvm.relay.op.nn.nn.upsampling tvm.relay.op.nn.nn.upsampling3d
msgid "Parameters"
msgstr ""

#: of tvm.relay.expr.Constant:3
msgid "The data content of the constant expression."
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid ":py:obj:`checked_type <tvm.relay.op.nn.Expr.checked_type>`\\"
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid "Get the checked type of tvm.relay.Expr."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:1
msgid "1D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:3
#: tvm.relay.op.nn.nn.global_avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"across each window represented by W."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:7
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:7
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, in_channels, width)`, to produce an output Tensor "
"with shape (batch_size, in_channels, output_width)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:12
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:12
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:11
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:12
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:12
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:11
msgid ""
"The pooling kernel and stride sizes are automatically chosen for desired "
"output sizes."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:20
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:23
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:22
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:20
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:23
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:22
msgid "For output_size:"
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output width."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:19
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size) for any input (NCW)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:22
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:25
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:24
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:22
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:25
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:24 tvm.relay.op.nn.nn.avg_pool1d:14
#: tvm.relay.op.nn.nn.avg_pool2d:23 tvm.relay.op.nn.nn.avg_pool2d_grad:7
#: tvm.relay.op.nn.nn.avg_pool3d:15 tvm.relay.op.nn.nn.batch_flatten:10
#: tvm.relay.op.nn.nn.bias_add:7 tvm.relay.op.nn.nn.bitserial_conv2d:3
#: tvm.relay.op.nn.nn.bitserial_dense:9
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:6
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:7
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:6
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:6
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:7
#: tvm.relay.op.nn.nn.conv1d:26 tvm.relay.op.nn.nn.conv1d_transpose:3
#: tvm.relay.op.nn.nn.conv2d:26 tvm.relay.op.nn.nn.conv2d_transpose:3
#: tvm.relay.op.nn.nn.conv3d:26 tvm.relay.op.nn.nn.conv3d_transpose:3
#: tvm.relay.op.nn.nn.deformable_conv2d:5 tvm.relay.op.nn.nn.dropout:7
#: tvm.relay.op.nn.nn.dropout_raw:7 tvm.relay.op.nn.nn.fast_softmax:8
#: tvm.relay.op.nn.nn.global_avg_pool1d:16
#: tvm.relay.op.nn.nn.global_avg_pool2d:18
#: tvm.relay.op.nn.nn.global_avg_pool3d:17
#: tvm.relay.op.nn.nn.global_max_pool1d:15
#: tvm.relay.op.nn.nn.global_max_pool2d:18
#: tvm.relay.op.nn.nn.global_max_pool3d:16 tvm.relay.op.nn.nn.l2_normalize:6
#: tvm.relay.op.nn.nn.leaky_relu:8 tvm.relay.op.nn.nn.log_softmax:10
#: tvm.relay.op.nn.nn.lrn:11 tvm.relay.op.nn.nn.max_pool1d:14
#: tvm.relay.op.nn.nn.max_pool2d:22 tvm.relay.op.nn.nn.max_pool2d_grad:7
#: tvm.relay.op.nn.nn.max_pool3d:15 tvm.relay.op.nn.nn.prelu:8
#: tvm.relay.op.nn.nn.softmax:8 tvm.relay.op.nn.nn.upsampling:11
#: tvm.relay.op.nn.nn.upsampling3d:11
msgid "The input data to the operator."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:24
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:27
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:26
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:24
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:27
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:26
msgid "Output height and width."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:26
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:29
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:28
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:26
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:29
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:28 tvm.relay.op.nn.nn.avg_pool1d:24
#: tvm.relay.op.nn.nn.avg_pool2d:33 tvm.relay.op.nn.nn.avg_pool2d_grad:15
#: tvm.relay.op.nn.nn.avg_pool3d:25 tvm.relay.op.nn.nn.bitserial_conv2d:19
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:22
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:23
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:23
#: tvm.relay.op.nn.nn.conv1d:42 tvm.relay.op.nn.nn.conv1d_transpose:19
#: tvm.relay.op.nn.nn.conv2d:42 tvm.relay.op.nn.nn.conv2d_transpose:19
#: tvm.relay.op.nn.nn.conv3d:42 tvm.relay.op.nn.nn.conv3d_transpose:19
#: tvm.relay.op.nn.nn.deformable_conv2d:25
#: tvm.relay.op.nn.nn.global_avg_pool1d:18
#: tvm.relay.op.nn.nn.global_avg_pool2d:20
#: tvm.relay.op.nn.nn.global_avg_pool3d:19
#: tvm.relay.op.nn.nn.global_max_pool1d:17
#: tvm.relay.op.nn.nn.global_max_pool2d:20
#: tvm.relay.op.nn.nn.global_max_pool3d:18 tvm.relay.op.nn.nn.max_pool1d:24
#: tvm.relay.op.nn.nn.max_pool2d:32 tvm.relay.op.nn.nn.max_pool2d_grad:15
#: tvm.relay.op.nn.nn.max_pool3d:25 tvm.relay.op.nn.nn.upsampling:17
#: tvm.relay.op.nn.nn.upsampling3d:19
msgid "Layout of the input."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:28
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:31
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:30
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:28
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:31
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:30
#: tvm.relay.op.nn.nn.global_avg_pool1d:20
#: tvm.relay.op.nn.nn.global_avg_pool3d:21
#: tvm.relay.op.nn.nn.global_max_pool1d:19
#: tvm.relay.op.nn.nn.global_max_pool3d:20
msgid "Layout of the output."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d
#: tvm.relay.op.nn.nn.adaptive_max_pool1d
#: tvm.relay.op.nn.nn.adaptive_max_pool2d
#: tvm.relay.op.nn.nn.adaptive_max_pool3d tvm.relay.op.nn.nn.avg_pool1d
#: tvm.relay.op.nn.nn.avg_pool2d tvm.relay.op.nn.nn.avg_pool2d_grad
#: tvm.relay.op.nn.nn.avg_pool3d tvm.relay.op.nn.nn.batch_flatten
#: tvm.relay.op.nn.nn.batch_matmul tvm.relay.op.nn.nn.batch_norm
#: tvm.relay.op.nn.nn.batch_to_space_nd tvm.relay.op.nn.nn.bias_add
#: tvm.relay.op.nn.nn.bitpack tvm.relay.op.nn.nn.bitserial_conv2d
#: tvm.relay.op.nn.nn.bitserial_dense
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_dense_pack
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc tvm.relay.op.nn.nn.conv1d
#: tvm.relay.op.nn.nn.conv1d_transpose tvm.relay.op.nn.nn.conv2d
#: tvm.relay.op.nn.nn.conv2d_transpose tvm.relay.op.nn.nn.conv3d
#: tvm.relay.op.nn.nn.conv3d_transpose tvm.relay.op.nn.nn.correlation
#: tvm.relay.op.nn.nn.cross_entropy
#: tvm.relay.op.nn.nn.cross_entropy_with_logits
#: tvm.relay.op.nn.nn.deformable_conv2d tvm.relay.op.nn.nn.dense
#: tvm.relay.op.nn.nn.depth_to_space tvm.relay.op.nn.nn.dilate
#: tvm.relay.op.nn.nn.dropout tvm.relay.op.nn.nn.dropout_raw
#: tvm.relay.op.nn.nn.fast_softmax tvm.relay.op.nn.nn.fifo_buffer
#: tvm.relay.op.nn.nn.global_avg_pool1d tvm.relay.op.nn.nn.global_avg_pool2d
#: tvm.relay.op.nn.nn.global_avg_pool3d tvm.relay.op.nn.nn.global_max_pool1d
#: tvm.relay.op.nn.nn.global_max_pool2d tvm.relay.op.nn.nn.global_max_pool3d
#: tvm.relay.op.nn.nn.group_norm tvm.relay.op.nn.nn.instance_norm
#: tvm.relay.op.nn.nn.l2_normalize tvm.relay.op.nn.nn.layer_norm
#: tvm.relay.op.nn.nn.leaky_relu tvm.relay.op.nn.nn.log_softmax
#: tvm.relay.op.nn.nn.lrn tvm.relay.op.nn.nn.matmul
#: tvm.relay.op.nn.nn.max_pool1d tvm.relay.op.nn.nn.max_pool2d
#: tvm.relay.op.nn.nn.max_pool2d_grad tvm.relay.op.nn.nn.max_pool3d
#: tvm.relay.op.nn.nn.mirror_pad tvm.relay.op.nn.nn.nll_loss
#: tvm.relay.op.nn.nn.pad tvm.relay.op.nn.nn.prelu tvm.relay.op.nn.nn.relu
#: tvm.relay.op.nn.nn.softmax tvm.relay.op.nn.nn.space_to_batch_nd
#: tvm.relay.op.nn.nn.space_to_depth tvm.relay.op.nn.nn.sparse_add
#: tvm.relay.op.nn.nn.sparse_dense tvm.relay.op.nn.nn.sparse_transpose
#: tvm.relay.op.nn.nn.upsampling tvm.relay.op.nn.nn.upsampling3d
#: tvm.relay.op.nn.utils.get_pad_tuple1d tvm.relay.op.nn.utils.get_pad_tuple2d
#: tvm.relay.op.nn.utils.get_pad_tuple3d
msgid "Returns"
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d:31
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:34
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:33
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:31
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:34
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:33 tvm.relay.op.nn.nn.avg_pool1d:33
#: tvm.relay.op.nn.nn.avg_pool2d:42 tvm.relay.op.nn.nn.avg_pool2d_grad:24
#: tvm.relay.op.nn.nn.avg_pool3d:34 tvm.relay.op.nn.nn.batch_matmul:21
#: tvm.relay.op.nn.nn.bitserial_conv2d:28 tvm.relay.op.nn.nn.bitserial_dense:26
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:13
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:31
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:32
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:11
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:11
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:11
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_dense_pack:21
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:32
#: tvm.relay.op.nn.nn.conv1d:51 tvm.relay.op.nn.nn.conv1d_transpose:30
#: tvm.relay.op.nn.nn.conv2d:51 tvm.relay.op.nn.nn.conv2d_transpose:30
#: tvm.relay.op.nn.nn.conv3d:51 tvm.relay.op.nn.nn.conv3d_transpose:28
#: tvm.relay.op.nn.nn.cross_entropy:8
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:8
#: tvm.relay.op.nn.nn.deformable_conv2d:34 tvm.relay.op.nn.nn.dense:20
#: tvm.relay.op.nn.nn.fast_softmax:13 tvm.relay.op.nn.nn.global_avg_pool1d:23
#: tvm.relay.op.nn.nn.global_avg_pool2d:25
#: tvm.relay.op.nn.nn.global_avg_pool3d:24
#: tvm.relay.op.nn.nn.global_max_pool1d:22
#: tvm.relay.op.nn.nn.global_max_pool2d:25
#: tvm.relay.op.nn.nn.global_max_pool3d:23 tvm.relay.op.nn.nn.l2_normalize:13
#: tvm.relay.op.nn.nn.leaky_relu:13 tvm.relay.op.nn.nn.log_softmax:15
#: tvm.relay.op.nn.nn.lrn:24 tvm.relay.op.nn.nn.matmul:24
#: tvm.relay.op.nn.nn.max_pool1d:31 tvm.relay.op.nn.nn.max_pool2d:39
#: tvm.relay.op.nn.nn.max_pool2d_grad:22 tvm.relay.op.nn.nn.max_pool3d:32
#: tvm.relay.op.nn.nn.mirror_pad:14 tvm.relay.op.nn.nn.nll_loss:22
#: tvm.relay.op.nn.nn.pad:18 tvm.relay.op.nn.nn.prelu:15
#: tvm.relay.op.nn.nn.relu:9 tvm.relay.op.nn.nn.softmax:13
#: tvm.relay.op.nn.nn.sparse_add:17 tvm.relay.op.nn.nn.sparse_dense:33
#: tvm.relay.op.nn.nn.upsampling:24 tvm.relay.op.nn.nn.upsampling3d:29
msgid "**result** -- The computed result."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool1d
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d
#: tvm.relay.op.nn.nn.adaptive_max_pool1d
#: tvm.relay.op.nn.nn.adaptive_max_pool2d
#: tvm.relay.op.nn.nn.adaptive_max_pool3d tvm.relay.op.nn.nn.avg_pool1d
#: tvm.relay.op.nn.nn.avg_pool2d tvm.relay.op.nn.nn.avg_pool2d_grad
#: tvm.relay.op.nn.nn.avg_pool3d tvm.relay.op.nn.nn.batch_flatten
#: tvm.relay.op.nn.nn.batch_matmul tvm.relay.op.nn.nn.batch_norm
#: tvm.relay.op.nn.nn.batch_to_space_nd tvm.relay.op.nn.nn.bias_add
#: tvm.relay.op.nn.nn.bitpack tvm.relay.op.nn.nn.bitserial_conv2d
#: tvm.relay.op.nn.nn.bitserial_dense
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform
#: tvm.relay.op.nn.nn.contrib_dense_pack
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc tvm.relay.op.nn.nn.conv1d
#: tvm.relay.op.nn.nn.conv1d_transpose tvm.relay.op.nn.nn.conv2d
#: tvm.relay.op.nn.nn.conv2d_transpose tvm.relay.op.nn.nn.conv3d
#: tvm.relay.op.nn.nn.conv3d_transpose tvm.relay.op.nn.nn.correlation
#: tvm.relay.op.nn.nn.cross_entropy
#: tvm.relay.op.nn.nn.cross_entropy_with_logits
#: tvm.relay.op.nn.nn.deformable_conv2d tvm.relay.op.nn.nn.dense
#: tvm.relay.op.nn.nn.depth_to_space tvm.relay.op.nn.nn.dilate
#: tvm.relay.op.nn.nn.dropout tvm.relay.op.nn.nn.dropout_raw
#: tvm.relay.op.nn.nn.fast_softmax tvm.relay.op.nn.nn.fifo_buffer
#: tvm.relay.op.nn.nn.global_avg_pool1d tvm.relay.op.nn.nn.global_avg_pool2d
#: tvm.relay.op.nn.nn.global_avg_pool3d tvm.relay.op.nn.nn.global_max_pool1d
#: tvm.relay.op.nn.nn.global_max_pool2d tvm.relay.op.nn.nn.global_max_pool3d
#: tvm.relay.op.nn.nn.group_norm tvm.relay.op.nn.nn.l2_normalize
#: tvm.relay.op.nn.nn.layer_norm tvm.relay.op.nn.nn.leaky_relu
#: tvm.relay.op.nn.nn.log_softmax tvm.relay.op.nn.nn.lrn
#: tvm.relay.op.nn.nn.matmul tvm.relay.op.nn.nn.max_pool1d
#: tvm.relay.op.nn.nn.max_pool2d tvm.relay.op.nn.nn.max_pool2d_grad
#: tvm.relay.op.nn.nn.max_pool3d tvm.relay.op.nn.nn.mirror_pad
#: tvm.relay.op.nn.nn.nll_loss tvm.relay.op.nn.nn.pad tvm.relay.op.nn.nn.prelu
#: tvm.relay.op.nn.nn.relu tvm.relay.op.nn.nn.softmax
#: tvm.relay.op.nn.nn.space_to_batch_nd tvm.relay.op.nn.nn.space_to_depth
#: tvm.relay.op.nn.nn.sparse_add tvm.relay.op.nn.nn.sparse_dense
#: tvm.relay.op.nn.nn.sparse_transpose tvm.relay.op.nn.nn.upsampling
#: tvm.relay.op.nn.nn.upsampling3d
msgid "Return type"
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool2d:1
msgid "2D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool2d:3
#: tvm.relay.op.nn.nn.global_avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation "
"across each window represented by WxH."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool2d:7
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with shape (batch_size, in_channels, output_height, output_width)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool2d:16
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:16
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output height and width."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool2d:19
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size) for any input (NCHW)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool2d:22
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:22
msgid ""
"If a tuple of integers (height, width) are provided for output_size, the "
"output size is (N x C x height x width) for any input (NCHW)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool3d:1
msgid "3D adaptive avg pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D avg value calculation "
"across each window represented by DxWxH."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool3d:6
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:6
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with shape (batch_size, in_channels, output_depth, "
"output_height, output_width)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool3d:15
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:15
msgid ""
"If this argument is not provided, input depth, height and width will be "
"used as output depth, height and width."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool3d:18
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:18
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size x output_size) for any input (NCDHW)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_avg_pool3d:21
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:21
msgid ""
"If a tuple of integers (depth, height, width) are provided for "
"output_size, the output size is (N x C x depth x height x width) for any "
"input (NCDHW)."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_max_pool1d:1
msgid "1D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_max_pool1d:3
#: tvm.relay.op.nn.nn.global_max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation "
"across each window represented by W."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_max_pool2d:1
msgid "2D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_max_pool2d:3
#: tvm.relay.op.nn.nn.global_max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation "
"across each window represented by WxH."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_max_pool3d:1
msgid "3D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relay.op.nn.nn.adaptive_max_pool3d:3
#: tvm.relay.op.nn.nn.global_max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation "
"across each window represented by DxWxH."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:6 tvm.relay.op.nn.nn.max_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, channels, width)`, to produce an output Tensor."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:10 tvm.relay.op.nn.nn.avg_pool3d:11
#: tvm.relay.op.nn.nn.max_pool1d:10 tvm.relay.op.nn.nn.max_pool3d:11
msgid ""
"The ceil_mode is used to take ceil or floor while computing out shape. "
"count_include_pad indicates including or excluding padded input values in"
" computation. This operator accepts data layout specification."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:16 tvm.relay.op.nn.nn.avg_pool2d:25
#: tvm.relay.op.nn.nn.avg_pool2d_grad:9 tvm.relay.op.nn.nn.avg_pool3d:17
#: tvm.relay.op.nn.nn.max_pool1d:16 tvm.relay.op.nn.nn.max_pool2d:24
#: tvm.relay.op.nn.nn.max_pool2d_grad:9 tvm.relay.op.nn.nn.max_pool3d:17
msgid "The size of window for pooling."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:18 tvm.relay.op.nn.nn.avg_pool2d:27
#: tvm.relay.op.nn.nn.avg_pool2d_grad:11 tvm.relay.op.nn.nn.avg_pool3d:19
#: tvm.relay.op.nn.nn.max_pool1d:18 tvm.relay.op.nn.nn.max_pool2d:26
#: tvm.relay.op.nn.nn.max_pool2d_grad:11 tvm.relay.op.nn.nn.max_pool3d:19
msgid "The strides of pooling."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:20 tvm.relay.op.nn.nn.avg_pool2d:29
#: tvm.relay.op.nn.nn.avg_pool3d:21 tvm.relay.op.nn.nn.max_pool1d:20
#: tvm.relay.op.nn.nn.max_pool2d:28 tvm.relay.op.nn.nn.max_pool3d:21
msgid "The dilation of pooling."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:22 tvm.relay.op.nn.nn.avg_pool2d:31
#: tvm.relay.op.nn.nn.avg_pool2d_grad:13 tvm.relay.op.nn.nn.avg_pool3d:23
#: tvm.relay.op.nn.nn.max_pool1d:22 tvm.relay.op.nn.nn.max_pool2d:30
#: tvm.relay.op.nn.nn.max_pool2d_grad:13 tvm.relay.op.nn.nn.max_pool3d:23
msgid "The padding for pooling."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:26 tvm.relay.op.nn.nn.avg_pool2d:35
#: tvm.relay.op.nn.nn.avg_pool2d_grad:17 tvm.relay.op.nn.nn.avg_pool3d:27
#: tvm.relay.op.nn.nn.global_avg_pool2d:22
#: tvm.relay.op.nn.nn.global_max_pool2d:22 tvm.relay.op.nn.nn.max_pool1d:26
#: tvm.relay.op.nn.nn.max_pool2d:34 tvm.relay.op.nn.nn.max_pool2d_grad:17
#: tvm.relay.op.nn.nn.max_pool3d:27
msgid "Layout of the output"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:28 tvm.relay.op.nn.nn.avg_pool2d:37
#: tvm.relay.op.nn.nn.avg_pool2d_grad:19 tvm.relay.op.nn.nn.avg_pool3d:29
#: tvm.relay.op.nn.nn.max_pool1d:28 tvm.relay.op.nn.nn.max_pool2d:36
#: tvm.relay.op.nn.nn.max_pool2d_grad:19 tvm.relay.op.nn.nn.max_pool3d:29
msgid "To enable or disable ceil while pooling."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool1d:30 tvm.relay.op.nn.nn.avg_pool2d:39
#: tvm.relay.op.nn.nn.avg_pool2d_grad:21 tvm.relay.op.nn.nn.avg_pool3d:31
msgid "To include padding to compute the average."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool2d:7 tvm.relay.op.nn.nn.global_avg_pool2d:7
#: tvm.relay.op.nn.nn.global_max_pool2d:7 tvm.relay.op.nn.nn.max_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with the following rule:"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool2d:11
msgid "with data of shape (b, c, h, w), pool_size (kh, kw)"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool2d:13
msgid ""
"\\mbox{out}(b, c, y, x)  = \\frac{1}{kh * kw} \\sum_{m=0}^{kh-1} "
"\\sum_{n=0}^{kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x "
"+ n)"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool2d:18
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to "
"take ceil or floor while computing out shape. count_include_pad indicates"
" including or excluding padded input values in computation. This operator"
" accepts data layout specification."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool2d_grad:3
msgid ""
"This operator takes out_grad and data as input and calculates gradient of"
" avg_pool2d."
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool2d_grad:5 tvm.relay.op.nn.nn.max_pool2d_grad:5
msgid "The output gradient"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relay.op.nn.nn.avg_pool3d:7 tvm.relay.op.nn.nn.max_pool3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, channels, depth, height, width)`, to produce an "
"output Tensor."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_flatten:3
msgid ""
"This operator flattens all the dimensions except for the batch dimension."
" which results a 2D output."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_flatten:6
msgid ""
"For data with shape ``(d1, d2, ..., dk)`` batch_flatten(data) returns "
"reshaped output of shape ``(d1, d2*...*dk)``."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_flatten:13
msgid "**result** -- The Flattened result."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_matmul:3
msgid ""
"Both `tensor_a` and `tensor_b` can be transposed. For legacy reason, we "
"use NT format (transpose_a=False, transpose_b=True) by default."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_matmul:6
msgid ""
"\\mbox{batch_matmul}(A, B)[i, :, :] = \\mbox{matmul}(A[i, :, :], B[i, :, "
":])"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_matmul:10
msgid "The first input."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_matmul:12
msgid "The second input."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_matmul:14
msgid "Specifies the output data type for mixed precision batch matmul."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_matmul:16
msgid "Whether the first tensor is in transposed format."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_matmul:18
msgid "Whether the second tensor is in transposed format."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:1
msgid ""
"Batch normalization layer (Ioffe and Szegedy, 2014). Normalizes the input"
" at each batch, i.e. applies a transformation that maintains the mean "
"activation close to 0 and the activation standard deviation close to 1."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:6
msgid ""
"data\\_mean[i] = mean(data[:,i,:,...]) \\\\\n"
"data\\_var[i] = var(data[:,i,:,...])"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:11
msgid ""
"Then compute the normalized output, which has the same shape as input, as"
" following:"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:13
msgid ""
"out[:,i,:,...] = \\frac{data[:,i,:,...] - "
"data\\_mean[i]}{\\sqrt{data\\_var[i]+\\epsilon}}\n"
"    * gamma[i] + beta[i]"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:18
msgid "Both *mean* and *var* returns a scalar by treating the input as a vector."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:20 tvm.relay.op.nn.nn.instance_norm:14
msgid ""
"Assume the input has size *k* on axis 1, then both ``gamma`` and ``beta``"
" have shape *(k,)*."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:23
msgid ""
"Besides the inputs and the outputs, this operator accepts two auxiliary "
"states, ``moving_mean`` and ``moving_var``, which are *k*-length vectors."
" They are global statistics for the whole dataset, which are updated by"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:32
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes "
"the 'channel' (separately normalized groups).  The default is 1. "
"Specifying -1 sets the channel axis to be the last item in the input "
"shape."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:38 tvm.relay.op.nn.nn.fast_softmax:6
#: tvm.relay.op.nn.nn.group_norm:24 tvm.relay.op.nn.nn.instance_norm:23
#: tvm.relay.op.nn.nn.layer_norm:17 tvm.relay.op.nn.nn.log_softmax:8
#: tvm.relay.op.nn.nn.softmax:6
msgid "This operator can be optimized away for inference."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:40
msgid "Input to which batch_norm will be applied."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:42 tvm.relay.op.nn.nn.group_norm:28
#: tvm.relay.op.nn.nn.instance_norm:27 tvm.relay.op.nn.nn.layer_norm:21
msgid "The gamma scale factor."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:44 tvm.relay.op.nn.nn.group_norm:30
#: tvm.relay.op.nn.nn.instance_norm:29 tvm.relay.op.nn.nn.layer_norm:23
msgid "The beta offset factor."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:46
msgid "Running mean of input,"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:48
msgid "Running variance of input."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:50 tvm.relay.op.nn.nn.instance_norm:31
msgid "Specify along which shape axis the channel is specified."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:52 tvm.relay.op.nn.nn.group_norm:36
#: tvm.relay.op.nn.nn.instance_norm:33 tvm.relay.op.nn.nn.layer_norm:27
msgid "Small float added to variance to avoid dividing by zero."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:54 tvm.relay.op.nn.nn.group_norm:38
#: tvm.relay.op.nn.nn.instance_norm:35 tvm.relay.op.nn.nn.layer_norm:29
msgid ""
"If True, add offset of beta to normalized tensor, If False, beta is "
"ignored."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:57
msgid ""
"If true, multiply by gamma. If False, gamma is not used. When the next "
"layer is piecewise linear (also e.g. nn.relu), this can be disabled since"
" the scaling will be done by the next layer."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_norm:62
msgid ""
"**result** -- Tuple of normed data (same shape as input), new running "
"mean (k-length vector), and new running variance (k-length vector)"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_to_space_nd:3
#: tvm.relay.op.nn.nn.space_to_batch_nd:4
msgid "N-D with shape [batch, spatial_shape, remaining_shape]"
msgstr ""

#: of tvm.relay.op.nn.nn.batch_to_space_nd:5
#: tvm.relay.op.nn.nn.space_to_batch_nd:6
msgid ""
"1-D of size [M] where M is number of spatial dims, specifies block size "
"for each spatial dimension."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_to_space_nd:8
msgid ""
"2-D of shape [M, 2] where M is number of spatial dims, specifies [begin, "
"end] crop size for each spatial dimension."
msgstr ""

#: of tvm.relay.op.nn.nn.batch_to_space_nd:12
msgid ""
"**result** -- N-D Tensor with shape [batch / prod(block_shape), "
"in_shape[1] * block_shape[0] - crops[0,0] - crops[0,1], ..., in_shape[M] "
"* block_shape[M-1] - crops[M-1, 0] - crops[M-1, 1], remaining_shape]"
msgstr ""

#: of tvm.relay.op.nn.nn.bias_add:3
msgid ""
"Add 1D bias to the axis of data. This function is a special case of add "
"which allows inference of shape of the bias from data."
msgstr ""

#: of tvm.relay.op.nn.nn.bias_add:9
msgid "The bias to be added."
msgstr ""

#: of tvm.relay.op.nn.nn.bias_add:11
msgid "The axis to add the bias."
msgstr ""

#: of tvm.relay.op.nn.nn.bias_add:14
msgid "**result** -- The final result."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:3
msgid ""
"The values along the input tensor's pack_axis are quantized and packed "
"together into the specified pack_type in a new bit axis."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:6
msgid ""
"For example, consider bitpacking with data to be a tensor with shape `[1,"
" 64, 128, 128]`, pack_axis=1, bit_axis=4, pack_type=uint8, and bits=2. "
"The output in this case will be of shape `[1, 8, 128, 128, 2]`. The "
"dimension of axis 1 has been reduced by a factor of 8 since each value is"
" packed into an 8-bit uint8. Axis 4 is now two bitplanes representing the"
" quantized value of the incoming data. The output tensor is now ready to "
"be used in a bitserial operation."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:13
msgid "The incoming tensor to be packed."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:15
msgid "Number of bits that should be packed."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:17
msgid "Axis that should be decomposed and packed."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:19
msgid "New axis containing bitplane."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:21 tvm.relay.op.nn.nn.bitserial_conv2d:23
msgid "Datatype to pack bits into."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:23
msgid "Name of the operation."
msgstr ""

#: of tvm.relay.op.nn.nn.bitpack:26
msgid "**result** -- The packed tensor."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:5
#: tvm.relay.op.nn.nn.bitserial_dense:11
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:6
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:8
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:6
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:6
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:8
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:6
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:8
#: tvm.relay.op.nn.nn.conv1d:28 tvm.relay.op.nn.nn.conv1d_transpose:5
#: tvm.relay.op.nn.nn.conv2d:28 tvm.relay.op.nn.nn.conv2d_transpose:5
#: tvm.relay.op.nn.nn.conv3d:28 tvm.relay.op.nn.nn.conv3d_transpose:5
#: tvm.relay.op.nn.nn.deformable_conv2d:9
msgid "The weight expressions."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:7
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:10
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:11
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:11
#: tvm.relay.op.nn.nn.conv1d:30 tvm.relay.op.nn.nn.conv1d_transpose:7
#: tvm.relay.op.nn.nn.conv2d:30 tvm.relay.op.nn.nn.conv2d_transpose:7
#: tvm.relay.op.nn.nn.conv3d:30 tvm.relay.op.nn.nn.conv3d_transpose:7
#: tvm.relay.op.nn.nn.deformable_conv2d:11
msgid "The strides of convolution."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:9
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:13
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:14
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:14
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:13
#: tvm.relay.op.nn.nn.conv2d:32 tvm.relay.op.nn.nn.conv3d:32
#: tvm.relay.op.nn.nn.conv3d_transpose:9
#: tvm.relay.op.nn.nn.deformable_conv2d:13
msgid "The padding of convolution on both sides of inputs before convolution."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:11
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:19
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:20
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:20
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:19
#: tvm.relay.op.nn.nn.conv1d:38 tvm.relay.op.nn.nn.conv1d_transpose:13
#: tvm.relay.op.nn.nn.conv2d:38 tvm.relay.op.nn.nn.conv2d_transpose:13
#: tvm.relay.op.nn.nn.conv3d:38 tvm.relay.op.nn.nn.conv3d_transpose:15
#: tvm.relay.op.nn.nn.deformable_conv2d:21
msgid "Number of output channels of this convolution."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:13
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:20
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:21
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:22
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:22
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:21
#: tvm.relay.op.nn.nn.conv1d_transpose:15 tvm.relay.op.nn.nn.conv2d:40
#: tvm.relay.op.nn.nn.conv2d_transpose:15 tvm.relay.op.nn.nn.conv3d:40
#: tvm.relay.op.nn.nn.conv3d_transpose:17
#: tvm.relay.op.nn.nn.deformable_conv2d:23
msgid "The spatial of the convolution kernel."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:15
msgid "Number of bits to pack for activations."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:17
msgid "Number of bits to pack for weights."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:21
msgid "Layout of the kernel"
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_conv2d:25
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:28
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:29
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:29
#: tvm.relay.op.nn.nn.conv1d:48 tvm.relay.op.nn.nn.conv1d_transpose:27
#: tvm.relay.op.nn.nn.conv2d:48 tvm.relay.op.nn.nn.conv2d_transpose:27
#: tvm.relay.op.nn.nn.conv3d:48 tvm.relay.op.nn.nn.deformable_conv2d:31
msgid "Specifies the output data type for mixed precision conv2d."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:1
msgid ""
"Bitserial Dense operator. Applies matrix multiplication of two quantized "
"matrices using a fast bitserial algorithm."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:7
msgid "`Y = X * W`"
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:13
#: tvm.relay.op.nn.nn.contrib_dense_pack:16 tvm.relay.op.nn.nn.dense:14
msgid "Number of hidden units of the dense transformation."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:15
msgid "Number of bits incoming tensor should be packed with."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:17
msgid "Number of bits weight tensor should be packed with."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:19
msgid "Datatype to pack individual bits into before computation."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:21
#: tvm.relay.op.nn.nn.contrib_dense_pack:18
msgid "Specifies the output data type for mixed precision dense."
msgstr ""

#: of tvm.relay.op.nn.nn.bitserial_dense:23
msgid "Whether to use unipolar or bipolar quantization for inputs."
msgstr ""

#: of tvm.relay.expr.const:3
msgid "The constant value."
msgstr ""

#: of tvm.relay.expr.const:5
msgid "The data type of the resulting constant."
msgstr ""

#: of tvm.relay.expr.const:10
msgid "When dtype is None, we use the following rule:"
msgstr ""

#: of tvm.relay.expr.const:12
msgid "int maps to \"int32\""
msgstr ""

#: of tvm.relay.expr.const:13
msgid "float maps to \"float32\""
msgstr ""

#: of tvm.relay.expr.const:14
msgid "bool maps to \"bool\""
msgstr ""

#: of tvm.relay.expr.const:15
msgid "other using the same default rule as numpy."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv2d_gemm_without_weight_transform"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:8
msgid "Tile rows of the weight transformation for ConvGemm."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:10
msgid "Tile columns of the weight transformation for ConvGemm."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv2d. It "
"assumes the weight is pre-transformed by "
"nn.contrib_conv2d_gemm_weight_transform"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:14
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:15
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:16
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:16
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:15
#: tvm.relay.op.nn.nn.conv1d:34 tvm.relay.op.nn.nn.conv1d_transpose:11
#: tvm.relay.op.nn.nn.conv2d:34 tvm.relay.op.nn.nn.conv2d_transpose:11
#: tvm.relay.op.nn.nn.conv3d:34 tvm.relay.op.nn.nn.conv3d_transpose:11
#: tvm.relay.op.nn.nn.deformable_conv2d:15
msgid "Specifies the dilation rate to be used for dilated convolution."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:16
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:17
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:17
#: tvm.relay.op.nn.nn.conv1d_transpose:17 tvm.relay.op.nn.nn.conv2d:36
#: tvm.relay.op.nn.nn.conv2d_transpose:17 tvm.relay.op.nn.nn.conv3d:36
#: tvm.relay.op.nn.nn.conv3d_transpose:13
#: tvm.relay.op.nn.nn.deformable_conv2d:19
msgid "Number of groups for grouped convolution."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:25
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:26
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:26
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:25
#: tvm.relay.op.nn.nn.conv1d:44 tvm.relay.op.nn.nn.conv1d_transpose:21
#: tvm.relay.op.nn.nn.conv2d:44 tvm.relay.op.nn.nn.conv2d_transpose:21
#: tvm.relay.op.nn.nn.conv3d:44 tvm.relay.op.nn.nn.conv3d_transpose:21
#: tvm.relay.op.nn.nn.deformable_conv2d:27
msgid "Layout of the weight."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:26
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:27
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:28
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:28
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:27
#: tvm.relay.op.nn.nn.conv1d:46 tvm.relay.op.nn.nn.conv1d_transpose:23
#: tvm.relay.op.nn.nn.conv2d:46 tvm.relay.op.nn.nn.conv2d_transpose:23
#: tvm.relay.op.nn.nn.conv3d:46 tvm.relay.op.nn.nn.conv3d_transpose:23
#: tvm.relay.op.nn.nn.deformable_conv2d:29
msgid "Layout of the output, by default, out_layout is the same as data_layout"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it"
" with data to produce an output, following a specialized NCHWc data "
"layout."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:9
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:9
msgid "The kernel expressions."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:3
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv2d_winograd_without_weight_transform"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:8
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:8
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:10
msgid "The Tile size of winograd. E.g. 2 for F(2x2, 3x3) and 4 for F(4x4, 3x3)"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv2d. It "
"assumes the weight is pre-transformed by "
"nn.contrib_conv2d_winograd_weight_transform"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv3d_winograd_without_weight_transform"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:8
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:10
msgid ""
"The Tile size of winograd. E.g. 2 for F(2x2x2, 3x3x3) and 4 for F(4x4x4, "
"3x3x3)"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv3d. It "
"assumes the weight is pre-transformed by "
"nn.contrib_conv3d_winograd_weight_transform"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_dense_pack:1
msgid "Dense operator. Applies a linear transformation with packed weight"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_dense_pack:6 tvm.relay.op.nn.nn.dense:6
msgid "`Y = X * W^T`"
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_dense_pack:8
msgid "The input data to the operator, of shape `(batch, units_in)`."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_dense_pack:11
msgid ""
"The transformed weight expressions, 3-D matrix, of shape `(units // "
"pack_weight_tile, units_in, pack_weight_tile)`."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_dense_pack:14
msgid "The layout of weight, such as \"NC\" or \"NC8n\"."
msgstr ""

#: of tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:3
msgid ""
"This operator takes the weight as the depthwise convolution kernel and "
"depthwise convolves it with data to produce an output, following a "
"specialized NCHWc data layout."
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d:3 tvm.relay.op.nn.nn.conv2d:3
#: tvm.relay.op.nn.nn.conv3d:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it"
" with data to produce an output."
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d:7
msgid ""
"In the default case, where the data_layout is `NCW` and kernel_layout is "
"`OIW`, conv1d takes in a data Tensor with shape `(batch_size, "
"in_channels, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_size)` to produce an output Tensor with the following"
" rule:"
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d:13
msgid ""
"\\mbox{out}[b, c, w] = \\sum_{dw, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * w + dw] *\n"
"   \\mbox{weight}[c, k, dw]"
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCW` for data and `OIW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d:32
msgid "The padding of convolution on both sides of the input before convolution."
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d:36
msgid "Currently unused for 1D convolution."
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d:40
msgid "The spatial dimension of the convolution kernel."
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d_transpose:9
#: tvm.relay.op.nn.nn.conv2d_transpose:9
msgid "The padding of convolution on both sides of inputs."
msgstr ""

#: of tvm.relay.op.nn.nn.conv1d_transpose:25
#: tvm.relay.op.nn.nn.conv2d_transpose:25
msgid "Used to disambiguate the output shape."
msgstr ""

#: of tvm.relay.op.nn.nn.conv2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` and kernel_layout is"
" `OIHW`, conv2d takes in a data Tensor with shape `(batch_size, "
"in_channels, height, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_size[0], kernel_size[1])` to produce an output Tensor"
" with the following rule:"
msgstr ""

#: of tvm.relay.op.nn.nn.conv2d:13
msgid ""
"\\mbox{out}[b, c, y, x] = \\sum_{dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * y  + dy, \\mbox{strides}[1] * "
"x + dx] *\n"
"   \\mbox{weight}[c, k, dy, dx]"
msgstr ""

#: of tvm.relay.op.nn.nn.conv2d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCHW` for data and `OIHW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: of tvm.relay.op.nn.nn.conv3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` and kernel_layout "
"is `OIDHW`, conv3d takes in a data Tensor with shape `(batch_size, "
"in_channels, depth, height, width)`, and a weight Tensor with shape "
"`(channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])`"
" to produce an output Tensor with the following rule:"
msgstr ""

#: of tvm.relay.op.nn.nn.conv3d:13
msgid ""
"\\mbox{out}[b, c, z, y, x] = \\sum_{dz, dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * z  + dz, \\mbox{strides}[1] * "
"y  + dy,\n"
"   \\mbox{strides}[2] * x + dx] * \\mbox{weight}[c, k, dz, dy, dx]"
msgstr ""

#: of tvm.relay.op.nn.nn.conv3d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCDHW` for data and `OIDHW` for weight), perform the "
"computation, then convert to the out_layout."
msgstr ""

#: of tvm.relay.op.nn.nn.conv3d_transpose:25
msgid "Specifies the output data type for mixed precision conv3d."
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:3
msgid ""
"The correlation layer performs multiplicative patch comparisons between "
"two feature maps. Given two multi-channel feature maps :math:`f_{1}, "
"f_{2}`, with :math:`w`, :math:`h`, and :math:`c` being their width, "
"height, and number of channels, the correlation layer lets the network "
"compare each patch from :math:`f_{1}` with each patch from :math:`f_{2}`."
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:8
msgid ""
"For now we consider only a single comparison of two patches. The "
"'correlation' of two patches centered at :math:`x_{1}` in the first map "
"and :math:`x_{2}` in the second map is then defined as:"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:12
msgid ""
"c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k] \\times [-k,k]} <f_{1}(x_{1} + o),"
" f_{2}(x_{2} + o)>"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:16
msgid "for a square patch of size :math:`K:=2k+1`."
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:18
msgid ""
"Note that the equation above is identical to one step of a convolution in"
" neural networks, but instead of convolving data with a filter, it "
"convolves data with other    data. For this reason, it has no training "
"weights."
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:22
msgid ""
"Computing :math:`c(x_{1}, x_{2})` involves :math:`c * K^{2}` "
"multiplications. Comparing all patch combinations involves "
":math:`w^{2}*h^{2}` such computations."
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:25
msgid ""
"Given a maximum displacement :math:`d`, for each location :math:`x_{1}` "
"it computes correlations :math:`c(x_{1}, x_{2})` only in a neighborhood "
"of size :math:`D:=2d+1`, by limiting the range of :math:`x_{2}`. We use "
"strides :math:`s_{1}, s_{2}`, to quantize :math:`x_{1}` globally and to "
"quantize :math:`x_{2}` within the neighborhood centered around "
":math:`x_{1}`."
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:31
msgid "The final output is defined by the following expression:"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:33
msgid "out[n, q, i, j] = c(x_{i, j}, x_{q})"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:37
msgid ""
"where :math:`i` and :math:`j` enumerate spatial locations in "
":math:`f_{1}`, and :math:`q` denotes the :math:`q^{th}` neighborhood of "
":math:`x_{i,j}`."
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:40 tvm.relay.op.nn.nn.correlation:42
msgid "4-D with shape [batch, channel, height, width]"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:44
msgid "Kernel size for correlation, must be an odd number"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:46
msgid "Max displacement of Correlation"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:48
msgid "Stride for data1"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:50
msgid "Stride for data2 within the neightborhood centered around data1"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:52
msgid ""
"Padding size, or [pad_height, pad_width] for 2 ints, or [pad_top, "
"pad_left, pad_bottom, pad_right] for 4 ints"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:56
msgid "operation type is either multiplication or substraction"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:58
msgid "layout of data1, data2 and the output"
msgstr ""

#: of tvm.relay.op.nn.nn.correlation:61
msgid "**Output** -- 4-D with shape [batch, out_channel, out_height, out_width]"
msgstr ""

#: of tvm.relay.op.nn.nn.cross_entropy:3
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:3
#: tvm.relay.op.nn.nn.nll_loss:10
msgid "The predictions."
msgstr ""

#: of tvm.relay.op.nn.nn.cross_entropy:5
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:5
msgid "The targets."
msgstr ""

#: of tvm.relay.op.nn.nn.deformable_conv2d:3
msgid ""
"The deformable convolution operation is described in "
"https://arxiv.org/abs/1703.06211"
msgstr ""

#: of tvm.relay.op.nn.nn.deformable_conv2d:7
msgid "The offset expressions."
msgstr ""

#: of tvm.relay.op.nn.nn.deformable_conv2d:17
msgid "Number of deformable groups."
msgstr ""

#: of tvm.relay.op.nn.nn.dense:1
msgid "Dense operator. Applies a linear transformation"
msgstr ""

#: of tvm.relay.op.nn.nn.dense:8
msgid "The input data to the operator, of shape `(d_1, d_2, ..., d_n, units_in)`."
msgstr ""

#: of tvm.relay.op.nn.nn.dense:11
msgid "The weight expressions, 2-D matrix, of shape `(units, units_in)`."
msgstr ""

#: of tvm.relay.op.nn.nn.dense:16
msgid ""
"Specifies the output data type for mixed precision dense, of shape `(d_1,"
" d_2, ..., d_n, units)`."
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:3
msgid "Input data with channels divisible by block_size**2"
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:5
msgid "Size of blocks to convert channels into."
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:7 tvm.relay.op.nn.nn.space_to_depth:7
msgid "One of NCHW or NHWC, indicates channel axis."
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:9
msgid "One of DCR or CDR, indicates which order channels are accessed in."
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:13
msgid ""
"**result** --  Tensor with shape [in_batch, in_channel / block_size * "
"block_size,                    in_height * block_size, in_width * "
"block_size]"
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:13 tvm.relay.op.nn.nn.space_to_depth:10
msgid "**result** --"
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:15
msgid "Tensor with shape [in_batch, in_channel / block_size * block_size,"
msgstr ""

#: of tvm.relay.op.nn.nn.depth_to_space:16
msgid "in_height * block_size, in_width * block_size]"
msgstr ""

#: of tvm.relay.op.nn.nn.dilate:3
msgid "n-D, can be any layout."
msgstr ""

#: of tvm.relay.op.nn.nn.dilate:5
msgid "Dilation stride on each dimension, 1 means no dilation."
msgstr ""

#: of tvm.relay.op.nn.nn.dilate:7
msgid "Value used to dilate the input."
msgstr ""

#: of tvm.relay.op.nn.nn.dilate:10
msgid "**Output** -- The computed result"
msgstr ""

#: of tvm.relay.op.nn.nn.dropout:3 tvm.relay.op.nn.nn.dropout_raw:3
msgid ""
"During training, each element of the input is set to zero with "
"probability ``p``. The whole array is rescaled by ``1/(1-p)`` to keep the"
" expected sum of the input unchanged."
msgstr ""

#: of tvm.relay.op.nn.nn.dropout:9 tvm.relay.op.nn.nn.dropout_raw:9
msgid "The probability for an element to be reset to 0."
msgstr ""

#: of tvm.relay.op.nn.nn.dropout:12 tvm.relay.op.nn.nn.dropout_raw:12
msgid "**result** -- The result of dropout"
msgstr ""

#: of tvm.relay.op.nn.nn.fast_softmax:1
msgid "Computes softmax. Use approximation to compute exponent for faster speed."
msgstr ""

#: of tvm.relay.op.nn.nn.fast_softmax:4 tvm.relay.op.nn.nn.softmax:3
msgid ""
"\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n"
"\n"
msgstr ""

#: of tvm.relay.op.nn.nn.fast_softmax:10 tvm.relay.op.nn.nn.softmax:10
msgid "The axis to sum over when computing softmax"
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:3
msgid "Compute equivalent of"
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:12
msgid "Useful for"
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:14
msgid ""
"Encoding explicit re-use of computation in convolution ops operated on a "
"sliding window input"
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:15
msgid ""
"Implementing a FIFO queue to cache intermediate results, e.g. as in Fast "
"WaveNet."
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:17 tvm.relay.op.nn.nn.relu:6
msgid "The input data"
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:19
msgid "Previous value of the FIFO buffer"
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:21
msgid "Specify which axis should be used for buffering"
msgstr ""

#: of tvm.relay.op.nn.nn.fifo_buffer:24
msgid "**result** -- Updated value for the buffer"
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple1d:5
msgid ""
"* **pad_left** (*int*) -- Padding size on left * **pad_right** (*int*) --"
" Padding size on right."
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple1d:5
#: tvm.relay.op.nn.utils.get_pad_tuple2d:6
#: tvm.relay.op.nn.utils.get_pad_tuple3d:7
msgid "**pad_left** (*int*) -- Padding size on left"
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple1d:6
#: tvm.relay.op.nn.utils.get_pad_tuple2d:8
#: tvm.relay.op.nn.utils.get_pad_tuple3d:10
msgid "**pad_right** (*int*) -- Padding size on right."
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple2d:5
msgid ""
"* **pad_top** (*int*) -- Padding size on top * **pad_left** (*int*) -- "
"Padding size on left * **pad_down** (*int*) -- Padding size on down. * "
"**pad_right** (*int*) -- Padding size on right."
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple2d:5
#: tvm.relay.op.nn.utils.get_pad_tuple3d:6
msgid "**pad_top** (*int*) -- Padding size on top"
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple2d:7
#: tvm.relay.op.nn.utils.get_pad_tuple3d:9
msgid "**pad_down** (*int*) -- Padding size on down."
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple3d:5
msgid ""
"* **pad_front** (*int*) -- Padding size on front * **pad_top** (*int*) --"
" Padding size on top * **pad_left** (*int*) -- Padding size on left * "
"**pad_back** (*int*) -- Padding size on back * **pad_down** (*int*) -- "
"Padding size on down. * **pad_right** (*int*) -- Padding size on right."
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple3d:5
msgid "**pad_front** (*int*) -- Padding size on front"
msgstr ""

#: of tvm.relay.op.nn.utils.get_pad_tuple3d:8
msgid "**pad_back** (*int*) -- Padding size on back"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool1d:6
#: tvm.relay.op.nn.nn.global_max_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, in_channels, width)`, to produce an output Tensor "
"with the following rule:"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool1d:10
msgid "with data of shape (b, c, w)"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool1d:12
msgid ""
"\\mbox{out}(b, c, 1)  = \\frac{1}{w} \\sum_{n=0}^{w-1} \\mbox{data}(b, c,"
" n)"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool2d:11
#: tvm.relay.op.nn.nn.global_max_pool2d:11
msgid "with data of shape (b, c, h, w)"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool2d:13
msgid ""
"\\mbox{out}(b, c, 1, 1)  = \\frac{1}{h * w} \\sum_{m=0}^{h-1} "
"\\sum_{n=0}^{w-1}\n"
"     \\mbox{data}(b, c, m, n)"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"across each window represented by DxWxH."
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool3d:6
#: tvm.relay.op.nn.nn.global_max_pool3d:6
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with the following rule:"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool3d:10
msgid "with data of shape (b, c, d, h, w)"
msgstr ""

#: of tvm.relay.op.nn.nn.global_avg_pool3d:12
msgid ""
"\\mbox{out}(b, c, 1, 1, 1)  = \\frac{1}{d * h * w} \\sum_{l=0}^{d-1}  "
"\\sum_{m=0}^{h-1}\n"
"     \\sum_{n=0}^{w-1} \\mbox{data}(b, c, l, m, n)"
msgstr ""

#: of tvm.relay.op.nn.nn.global_max_pool1d:10
msgid "with data of shape (b, c, w) .. math::"
msgstr ""

#: of tvm.relay.op.nn.nn.global_max_pool2d:13
msgid ""
"\\mbox{out}(b, c, 1, 1)  = \\max_{m=0, \\ldots, h} \\max_{n=0, \\ldots, "
"w}\n"
"     \\mbox{data}(b, c, m, n)"
msgstr ""

#: of tvm.relay.op.nn.nn.global_max_pool3d:10
msgid "with data of shape (b, c, d, h, w) .. math::"
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:1
msgid ""
"Group normalization normalizes over group of channels for each training "
"examples. We can say that, Group Norm is in between Instance Norm and "
"Layer Norm. When we put all the channels into a single group, group "
"normalization becomes Layer normalization. And, when we put each channel "
"into different groups it becomes Instance normalization"
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:6
msgid "https://arxiv.org/pdf/1803.08494.pdf"
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:8
msgid ""
"Applies group normalization to the n-dimensional input array by "
"seperating the input channels into 'num_groups' groups, each containing "
"'num_channels / num_groups' channels. The mean and standard-deviation are"
" calculated separately over the each group. gamma and beta are learnable "
"per-channel affine transform parameter vectors of size num_channels."
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:13 tvm.relay.op.nn.nn.layer_norm:6
msgid ""
"out = \\frac{data - mean(data, axis)}{\\sqrt{var(data, axis)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:18
msgid ""
"Unlike batch normalization, the mean and var are computed along a group "
"of channels."
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:20
msgid ""
"If the input has size k on axis 1, then both gamma and beta have shape "
"(k,)."
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:26
msgid "Input to which group_norm will be applied."
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:32
msgid "The number of groups to separate the channels into."
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:34
msgid "The axis of the channels."
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:41 tvm.relay.op.nn.nn.instance_norm:38
#: tvm.relay.op.nn.nn.layer_norm:32
msgid "If True, multiply by gamma. If False, gamma is not used."
msgstr ""

#: of tvm.relay.op.nn.nn.group_norm:44 tvm.relay.op.nn.nn.layer_norm:35
msgid "**result** -- The normalized data."
msgstr ""

#: of tvm.relay.op.nn.nn.instance_norm:4
msgid ""
"out = \\frac{data - mean(data)}{\\sqrt{var(data)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: of tvm.relay.op.nn.nn.instance_norm:9
msgid ""
"The instance normalization is similar to batch normalization, but unlike "
"batch normalization, the mean and var are calculated per-dimension "
"separately for each object(instance) in a mini-batch, not over a batch. "
"And the same normalization is applied both at test and train time."
msgstr ""

#: of tvm.relay.op.nn.nn.instance_norm:17
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes "
"the 'channel'.  The default is 1. Specifying -1 sets the channel axis to "
"be the last item in the input shape."
msgstr ""

#: of tvm.relay.op.nn.nn.instance_norm:25
msgid "Input to which instance_norm will be applied."
msgstr ""

#: of tvm.relay.op.nn.nn.instance_norm:41
msgid ""
"* **result** (*tvm.relay.Expr*) -- The normalized data. * **.. _`Instance"
" Normalization** (The Missing Ingredient for Fast Stylization`:) -- "
"https://arxiv.org/abs/1607.08022"
msgstr ""

#: of tvm.relay.op.nn.nn.instance_norm:41
msgid "**result** (*tvm.relay.Expr*) -- The normalized data."
msgstr ""

#: of tvm.relay.op.nn.nn.instance_norm:42
msgid ""
"**.. _`Instance Normalization** (The Missing Ingredient for Fast "
"Stylization`:) -- https://arxiv.org/abs/1607.08022"
msgstr ""

#: of tvm.relay.op.nn.nn.l2_normalize:3
msgid ""
"y(i, j) = x(i, j) / sqrt(max(sum(x^2), eps))\n"
"\n"
msgstr ""

#: of tvm.relay.op.nn.nn.l2_normalize:8
msgid "epsilon value"
msgstr ""

#: of tvm.relay.op.nn.nn.l2_normalize:10
msgid "axis over the normalization applied"
msgstr ""

#: of tvm.relay.op.nn.nn.layer_norm:1
msgid ""
"Layer normalization (Lei Ba and et al., 2016). Applies layer "
"normalization to the n-dimensional input array. This operator takes an "
"n-dimensional input array and normalizes the input using the given axis:"
msgstr ""

#: of tvm.relay.op.nn.nn.layer_norm:11
msgid ""
"Unlike batch normalization, the mean and var are computed along the "
"channel dimension."
msgstr ""

#: of tvm.relay.op.nn.nn.layer_norm:13
msgid ""
"Assume the input has size k on axis 1, then both gamma and beta have "
"shape (k,)."
msgstr ""

#: of tvm.relay.op.nn.nn.layer_norm:19
msgid "Input to which layer_norm will be applied."
msgstr ""

#: of tvm.relay.op.nn.nn.layer_norm:25
msgid "The axis that should be normalized, typically the axis of the channels."
msgstr ""

#: of tvm.relay.op.nn.nn.leaky_relu:4
msgid "`y = x > 0 ? x : alpha * x`"
msgstr ""

#: of tvm.relay.op.nn.nn.leaky_relu:10 tvm.relay.op.nn.nn.prelu:10
msgid "Slope coefficient for the negative half axis."
msgstr ""

#: of tvm.relay.op.nn.nn.log_softmax:3
msgid "\\text{log_softmax}(x)_i = \\log \\frac{exp(x_i)}{\\sum_j exp(x_j)}"
msgstr ""

#: of tvm.relay.op.nn.nn.log_softmax:12
msgid "The axis to sum over when computing log softmax"
msgstr ""

#: of tvm.relay.op.nn.nn.lrn:3
msgid ""
"Normalize the input in a local region across or within feature maps. Each"
" input value is divided by (data / (bias + (alpha * sum_data ^2 "
"/size))^beta) where n is the size of each local region, and the sum is "
"taken over the region centered at that value (zero padding is added where"
" necessary)."
msgstr ""

#: of tvm.relay.op.nn.nn.lrn:8
msgid ""
"(data / (bias + (alpha * sum_data ^2 /size))^beta)\n"
"\n"
msgstr ""

#: of tvm.relay.op.nn.nn.lrn:13
msgid "The size of the local region to be considered for normalization."
msgstr ""

#: of tvm.relay.op.nn.nn.lrn:15
msgid "Input data layout channel axis. Default value is 1 for NCHW format"
msgstr ""

#: of tvm.relay.op.nn.nn.lrn:17
msgid "The offset parameter to avoid dividing by 0."
msgstr ""

#: of tvm.relay.op.nn.nn.lrn:19
msgid "The scaling parameter."
msgstr ""

#: of tvm.relay.op.nn.nn.lrn:21
msgid "The exponent parameter."
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:1
msgid ""
"Matmul operator. Applies a linear transformation. The A & B can be "
"transposed."
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:4
msgid "`C = A * B`"
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:8
msgid ""
"The first input of the operator, of shape `(d_1, d_2, ..., d_n, "
"units_in)` or `(d_1, d_2, ..., units_in, d_n)`."
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:11
msgid ""
"The second input expressions, 2-D matrix, of shape `(units_in, units)` or"
" `(units, units_in)`."
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:14
msgid "Number of hidden units of the matmul transformation."
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:16
msgid ""
"Specifies the output data type for mixed precision matmul, of shape "
"`(d_1, d_2, ..., d_n, units)`."
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:19
msgid "Whether the data tensor is in transposed format."
msgstr ""

#: of tvm.relay.op.nn.nn.matmul:21
msgid "Whether the weight tensor is in transposed format."
msgstr ""

#: of tvm.relay.op.nn.nn.max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relay.op.nn.nn.max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation with "
"in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relay.op.nn.nn.max_pool2d:11
msgid "with data of shape (b, c, h, w) and pool_size (kh, kw)"
msgstr ""

#: of tvm.relay.op.nn.nn.max_pool2d:13
msgid ""
"\\mbox{out}(b, c, y, x)  = \\max_{m=0, \\ldots, kh-1} \\max_{n=0, "
"\\ldots, kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x "
"+ n)"
msgstr ""

#: of tvm.relay.op.nn.nn.max_pool2d:18
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to "
"take ceil or floor while computing out shape. This operator accepts data "
"layout specification."
msgstr ""

#: of tvm.relay.op.nn.nn.max_pool2d_grad:3
msgid ""
"This operator takes out_grad and data as input and calculates gradient of"
" max_pool2d."
msgstr ""

#: of tvm.relay.op.nn.nn.max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relay.op.nn.nn.mirror_pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified "
"widths using mirroring of the border pixels."
msgstr ""

#: of tvm.relay.op.nn.nn.mirror_pad:6 tvm.relay.op.nn.nn.pad:6
msgid "The input data to the operator"
msgstr ""

#: of tvm.relay.op.nn.nn.mirror_pad:8 tvm.relay.op.nn.nn.pad:8
msgid ""
"Number of values padded to the edges of each axis, in the format of "
"((before_1, after_1), ..., (before_N, after_N))"
msgstr ""

#: of tvm.relay.op.nn.nn.mirror_pad:11
msgid "What type of mirroring to use, must be SYMMETRIC or REFLECT."
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:6
msgid "output{n, i_1, i_2, ..., i_k} = -p * w"
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:6
msgid "where t = target{n, i_1, i_2, ..., i_k}"
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:5
msgid ""
"p = predictions{n, t, i_1, i_2, i_k} w = weights{n, i_1, i_2, ..., i_k} "
"if t != ignore_index else 0"
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:8
msgid "result = reduction(output)"
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:12
msgid "The target value of each prediction."
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:14
msgid "The weight of each target value."
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:16
msgid ""
"The reduction method to apply to the output. Possible values are "
"\"mean\", \"sum\" and \"none\"."
msgstr ""

#: of tvm.relay.op.nn.nn.nll_loss:19
msgid "The target value to ignore."
msgstr ""

#: of tvm.relay.op.nn.nn.pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified "
"widths using the specified value."
msgstr ""

#: of tvm.relay.op.nn.nn.pad:11
msgid "The value used for padding"
msgstr ""

#: of tvm.relay.op.nn.nn.pad:13
msgid ""
"'constant' pads with constant_value pad_value 'edge' pads using the edge "
"values of the input array 'reflect' pads by reflecting values with "
"respect to the edge"
msgstr ""

#: of tvm.relay.op.nn.nn.prelu:4
msgid "y = x > 0 ? x : alpha * x"
msgstr ""

#: of tvm.relay.op.nn.nn.prelu:12
msgid "Specify which shape axis the channel is specified."
msgstr ""

#: of tvm.relay.op.nn.nn.relu:3
msgid ""
"out = max(x, 0)\n"
"\n"
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_batch_nd:9
msgid ""
"2-D of shape [M, 2] where M is number of spatial dims, specifies [before,"
" after] paddings for each spatial dimension."
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_batch_nd:12
msgid "The value used for padding."
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_batch_nd:15
msgid ""
"**result** -- N-D Tensor with shape [in_batch * prod(block_shape), "
"padded_data[1] / block_shape[0], ..., padded_data[M] / block_shape[M-1], "
"remaining_shape]"
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_depth:3
msgid "Input data with spatial dimensions divisible by block_size"
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_depth:5
msgid "Size of blocks to decompose into channels."
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_depth:10
msgid ""
"**result** --  Tensor with shape [in_batch, in_channel * block_size * "
"block_size,                    in_height / block_size, in_width / "
"block_size]"
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_depth:12
msgid "Tensor with shape [in_batch, in_channel * block_size * block_size,"
msgstr ""

#: of tvm.relay.op.nn.nn.space_to_depth:13
msgid "in_height / block_size, in_width / block_size]"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_add:5
msgid ""
"\\mbox{sparse_add}(dense_mat, sparse_mat)[m, n] = "
"\\mbox{add}(\\mbox{as_dense}(S), (D))[m, n]"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_add:9
msgid ""
"where `as_dense` returns dense equivalent of the given S(sparse matrix) "
"while performing addition with given D(dense matrix)."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_add:12
msgid "The input dense matrix for the matrix addition"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_add:14
msgid "The input sparse matrix(CSR) for the matrix addition."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_add:21
msgid "Examples"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:9
msgid "\\if sparse_lhs=False:"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:6
msgid ""
"\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
"= \\mbox{matmul}(D, \\mbox{as_dense}(S)^T)[m, n]"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:15
msgid "\\if sparse_lhs=True:"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:12
msgid ""
"\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
"= \\mbox{matmul}(\\mbox{as_dense}(S), (D)^T)[m, n]"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:17
msgid ""
"where `as_dense` returns dense equivalent of the given S(sparse matrix) "
"while performing matmul with given D(dense matrix)."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:20
msgid ""
"See "
"https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
" and "
"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.bsr_matrix.html"
" for more detail on the sparse matrix representation."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:26
msgid "The input dense matrix for the matrix multiplication"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:28
msgid "The input sparse matrix for the matrix multiplication."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_dense:30
msgid "Indicates whether lhs or rhs matrix is sparse. Default value is False."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_transpose:5
msgid "** Currently only support Square Matrices **"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_transpose:7
msgid "\\mbox{sparse_transpose}(x)[n, n] = (x^T)[n, n]"
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_transpose:11
msgid ""
"Please refer to "
"https://github.com/scipy/scipy/blob/v1.3.0/scipy/sparse/csr.py for the "
"algorithm implemented in this operator."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_transpose:14
msgid "The sparse weight matrix for the fast matrix transpose."
msgstr ""

#: of tvm.relay.op.nn.nn.sparse_transpose:17
msgid ""
"**result** -- Tuple of output sparse tensor (same shape and format as "
"input), i.e. if CSR then output is in ([data, indices, indptr]) form"
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling:3
msgid ""
"This operator takes data as input and does 2D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCHW` with data of"
" shape (n, c, h, w) out will have a shape (n, c, h*scale_h, w*scale_w)"
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling:8
msgid ""
"method indicates the algorithm to be used while calculating the out value"
" and method can be one of (\"bilinear\", \"nearest_neighbor\", "
"\"bicubic\")"
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling:13 tvm.relay.op.nn.nn.upsampling3d:15
msgid "The scale factor for height upsampling."
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling:15 tvm.relay.op.nn.nn.upsampling3d:17
msgid "The scale factor for width upsampling."
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling:19
msgid "Scale method to used [nearest_neighbor, bilinear, bicubic]."
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling:21
msgid "Whether to keep corners in proper place."
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling3d:3
msgid ""
"This operator takes data as input and does 3D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCDHW` with data "
"of shape (n, c, d, h, w) out will have a shape (n, c, d*scale_d, "
"h*scale_h, w*scale_w)"
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling3d:8
msgid ""
"method indicates the algorithm to be used while calculating the out value"
" and method can be one of (\"trilinear\", \"nearest_neighbor\")"
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling3d:13
msgid "The scale factor for depth upsampling."
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling3d:21
msgid "Scale method to used [nearest_neighbor, trilinear]."
msgstr ""

#: of tvm.relay.op.nn.nn.upsampling3d:23
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Refer to the ONNX Resize operator "
"specification for details. Available options are \"half_pixel\", "
"\"align_corners\" and \"asymmetric\"."
msgstr ""


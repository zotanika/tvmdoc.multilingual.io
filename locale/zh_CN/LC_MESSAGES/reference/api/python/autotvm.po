# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:20+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/reference/api/python/autotvm.rst:19
msgid "tvm.autotvm"
msgstr ""

#: of tvm.autotvm:1
msgid "The auto-tuning module of tvm"
msgstr ""

#: of tvm.autotvm:3
msgid "This module includes:"
msgstr ""

#: of tvm.autotvm:5
msgid "Tuning space definition API"
msgstr ""

#: of tvm.autotvm:7
msgid "Efficient auto-tuners"
msgstr ""

#: of tvm.autotvm:9
msgid "Tuning result and database support"
msgstr ""

#: of tvm.autotvm:11
msgid "Distributed measurement to scale up tuning"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest:1
msgid "Apply the history best config"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput
#: tvm.autotvm.measure.measure.MeasureResult
#: tvm.autotvm.measure.measure.create_measure_batch
#: tvm.autotvm.measure.measure.measure_option
#: tvm.autotvm.measure.measure_methods.LocalBuilder
#: tvm.autotvm.measure.measure_methods.LocalRunner
#: tvm.autotvm.measure.measure_methods.RPCRunner tvm.autotvm.record.decode
#: tvm.autotvm.record.encode tvm.autotvm.record.load_from_file
#: tvm.autotvm.record.measure_str_key tvm.autotvm.record.pick_best
#: tvm.autotvm.record.split_workload tvm.autotvm.task.dispatcher.ApplyConfig
#: tvm.autotvm.task.dispatcher.ApplyGraphBest.update
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.load
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update
#: tvm.autotvm.task.dispatcher.DispatchContext.query
#: tvm.autotvm.task.dispatcher.DispatchContext.update
#: tvm.autotvm.task.dispatcher.FallbackContext.clear_cache
#: tvm.autotvm.task.dispatcher.FallbackContext.update
#: tvm.autotvm.task.dispatcher.clear_fallback_cache
#: tvm.autotvm.task.space.AnnotateEntity
#: tvm.autotvm.task.space.AnnotateEntity.apply
#: tvm.autotvm.task.space.ConfigEntity
#: tvm.autotvm.task.space.ConfigEntity.from_json_dict
#: tvm.autotvm.task.space.ConfigSpace.add_flop
#: tvm.autotvm.task.space.ConfigSpace.axis
#: tvm.autotvm.task.space.ConfigSpace.define_annotate
#: tvm.autotvm.task.space.ConfigSpace.define_knob
#: tvm.autotvm.task.space.ConfigSpace.define_reorder
#: tvm.autotvm.task.space.ConfigSpace.define_split
#: tvm.autotvm.task.space.ConfigSpace.get
#: tvm.autotvm.task.space.ConfigSpace.raise_error
#: tvm.autotvm.task.space.FallbackConfigEntity.fallback_split
#: tvm.autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log
#: tvm.autotvm.task.space.ReorderEntity
#: tvm.autotvm.task.space.ReorderEntity.apply
#: tvm.autotvm.task.space.SplitEntity tvm.autotvm.task.space.SplitEntity.apply
#: tvm.autotvm.task.space.VirtualAxis tvm.autotvm.task.space.get_factors
#: tvm.autotvm.task.space.get_pow2s tvm.autotvm.task.task.Task
#: tvm.autotvm.task.task.Task.instantiate
#: tvm.autotvm.task.task.args_to_workload tvm.autotvm.task.task.compute_flop
#: tvm.autotvm.task.task.create tvm.autotvm.task.task.deserialize_args
#: tvm.autotvm.task.task.serialize_args tvm.autotvm.task.task.template
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.get
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.reset
#: tvm.autotvm.task.topi_integration.register_topi_compute
#: tvm.autotvm.task.topi_integration.register_topi_schedule
#: tvm.autotvm.tuner.callback.log_to_database
#: tvm.autotvm.tuner.callback.log_to_file
#: tvm.autotvm.tuner.callback.progress_bar tvm.autotvm.tuner.ga_tuner.GATuner
#: tvm.autotvm.tuner.ga_tuner.GATuner.load_history
#: tvm.autotvm.tuner.ga_tuner.GATuner.next_batch
#: tvm.autotvm.tuner.ga_tuner.GATuner.update
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update
#: tvm.autotvm.tuner.tuner.Tuner tvm.autotvm.tuner.tuner.Tuner.load_history
#: tvm.autotvm.tuner.tuner.Tuner.next_batch tvm.autotvm.tuner.tuner.Tuner.tune
#: tvm.autotvm.tuner.tuner.Tuner.update
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune
msgid "Parameters"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest:3
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.load:3
msgid ""
"Collection of tuning records. If is str, then it should be the filename "
"of a records log file. Each row of this file is an encoded record pair. "
"Otherwise, it is an iterator."
msgstr ""

#: ../../_staging/reference/api/python/autotvm.rst:24
msgid "tvm.autotvm.measure"
msgstr ""

#: of tvm.autotvm.measure.measure:1
msgid "User facing API for specifying how to measure the generated code"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:1
msgid "Stores all the necessary inputs for a measurement."
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:3
msgid "The target device"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:5
msgid "Task function"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:7
msgid "Specific configuration."
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:1
msgid "Stores all the results of a measurement"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:3
msgid ""
"If no error occurs during measurement, it is an array of measured running"
" times. If an error occurs during measurement, it is an array of the "
"exception objections."
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:6
msgid "Denote error type, defined by MeasureErrorNo"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:8
msgid "All cost of this measure, including rpc, compilation, test runs"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:10
msgid "The absolute time stamp when we finish measurement."
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:1
msgid ""
"Set options for measure. To measure a config, we will build it and run "
"it. So we have to set options for these two steps. They have their own "
"options on timeout, parallel, etc."
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:5
msgid "Specify how to build programs"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:7
msgid "Specify how to run programs"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:11
#: tvm.autotvm.task.space.ConfigSpace.define_split:27
#: tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:9
#: tvm.autotvm.task.task.template:14
#: tvm.autotvm.task.topi_integration.register_topi_compute:18
#: tvm.autotvm.task.topi_integration.register_topi_schedule:21
msgid "Examples"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:12
msgid ""
"# example setting for using local devices >>> measure_option = "
"autotvm.measure_option( >>>     builder=autotvm.LocalBuilder(),      # "
"use all local cpu cores for compilation >>>     "
"runner=autotvm.LocalRunner(          # measure them sequentially >>>"
"         number=10, >>>         timeout=5) >>> )"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:20
msgid ""
"# example setting for using remote devices >>> measure_option = "
"autotvm.measure_option( >>>    builder=autotvm.LocalBuilder(),  # use all"
" local cpu cores for compilation >>>    runner=autotvm.RPCRunner( >>>"
"        'rasp3b', 'locahost', 9190, # device key, host and port of the "
"rpc tracker >>>        number=4, >>>        timeout=4) # timeout of a run"
" on the device. RPC request waiting time is excluded. >>>)"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:31
msgid ""
"To make measurement results accurate, you should pick the correct value "
"for the argument `number` and `repeat` in Runner(). Some devices need a "
"certain minimum running time to \"warm up,\" such as GPUs that need time "
"to reach a performance power state. Using `min_repeat_ms` can dynamically"
" adjusts `number`, so it is recommended. The typical value for NVIDIA GPU"
" is 150 ms."
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:1
msgid "Get a standard measure_batch function."
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:3
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner:3
msgid "The tuning task"
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:5
msgid ""
"The option for measuring generated code. You should use the return value "
"of function :any:`measure_option` for this argument."
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch
#: tvm.autotvm.record.decode tvm.autotvm.record.encode
#: tvm.autotvm.record.measure_str_key
#: tvm.autotvm.task.dispatcher.DispatchContext.query
#: tvm.autotvm.task.space.AnnotateEntity.apply
#: tvm.autotvm.task.space.AnnotateSpace.get_num_output
#: tvm.autotvm.task.space.ConfigEntity.from_json_dict
#: tvm.autotvm.task.space.ConfigEntity.get_flatten_feature
#: tvm.autotvm.task.space.ConfigEntity.get_other_option
#: tvm.autotvm.task.space.ConfigEntity.to_json_dict
#: tvm.autotvm.task.space.ConfigSpace.valid
#: tvm.autotvm.task.space.OtherOptionSpace.get_num_output
#: tvm.autotvm.task.space.ReorderEntity.apply
#: tvm.autotvm.task.space.ReorderSpace.get_num_output
#: tvm.autotvm.task.space.SplitEntity.apply
#: tvm.autotvm.task.space.SplitSpace.get_num_output
#: tvm.autotvm.task.space.TransformSpace.get_num_output
#: tvm.autotvm.task.space.VirtualAxis.get_num_output
#: tvm.autotvm.task.space.get_factors tvm.autotvm.task.space.get_pow2s
#: tvm.autotvm.task.task.Task.instantiate
#: tvm.autotvm.task.task.args_to_workload tvm.autotvm.task.task.compute_flop
#: tvm.autotvm.task.task.create tvm.autotvm.task.task.get_config
#: tvm.autotvm.task.task.template
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.get
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.get_tasks
#: tvm.autotvm.task.topi_integration.register_topi_compute
#: tvm.autotvm.task.topi_integration.register_topi_schedule
#: tvm.autotvm.tuner.callback.log_to_file
#: tvm.autotvm.tuner.ga_tuner.GATuner.has_next
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.has_next
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.has_next
#: tvm.autotvm.tuner.tuner.Tuner.has_next
msgid "Returns"
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:9
msgid "**measure_batch** -- a callback function to measure a batch of configs"
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch
#: tvm.autotvm.record.decode tvm.autotvm.record.encode
#: tvm.autotvm.record.measure_str_key
#: tvm.autotvm.task.dispatcher.DispatchContext.query
#: tvm.autotvm.task.space.AnnotateEntity.apply
#: tvm.autotvm.task.space.AnnotateSpace.get_num_output
#: tvm.autotvm.task.space.ConfigEntity.from_json_dict
#: tvm.autotvm.task.space.ConfigEntity.get_flatten_feature
#: tvm.autotvm.task.space.ConfigEntity.get_other_option
#: tvm.autotvm.task.space.ConfigEntity.to_json_dict
#: tvm.autotvm.task.space.ConfigSpace.valid
#: tvm.autotvm.task.space.OtherOptionSpace.get_num_output
#: tvm.autotvm.task.space.ReorderEntity.apply
#: tvm.autotvm.task.space.ReorderSpace.get_num_output
#: tvm.autotvm.task.space.SplitEntity.apply
#: tvm.autotvm.task.space.SplitSpace.get_num_output
#: tvm.autotvm.task.space.TransformSpace.get_num_output
#: tvm.autotvm.task.space.VirtualAxis.get_num_output
#: tvm.autotvm.task.space.get_factors tvm.autotvm.task.space.get_pow2s
#: tvm.autotvm.task.task.args_to_workload tvm.autotvm.task.task.compute_flop
#: tvm.autotvm.task.task.create tvm.autotvm.task.task.get_config
#: tvm.autotvm.task.task.template
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.get
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.get_tasks
#: tvm.autotvm.task.topi_integration.register_topi_compute
#: tvm.autotvm.task.topi_integration.register_topi_schedule
#: tvm.autotvm.tuner.callback.log_to_file
#: tvm.autotvm.tuner.ga_tuner.GATuner.has_next
#: tvm.autotvm.tuner.ga_tuner.GATuner.next_batch
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.has_next
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.has_next
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch
#: tvm.autotvm.tuner.tuner.Tuner.has_next
#: tvm.autotvm.tuner.tuner.Tuner.next_batch
msgid "Return type"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:1
msgid "Run compilation on local machine"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:3
#: tvm.autotvm.measure.measure_methods.LocalRunner:3
msgid "The timeout of a compilation"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:5
#: tvm.autotvm.measure.measure_methods.RPCRunner:6
msgid "The number of tasks run in parallel. \"None\" will use all cpu cores"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:7
msgid ""
"If supplied, additional kwargs passed to build_func. Overrides any "
"build_kwargs supplied by the Runner."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:10
msgid ""
"If is 'default', use default build function If is 'ndk', use function for"
" android ndk If id 'stackvm', use function for stackvm If is callable, "
"use it as custom build function, expect lib_format field."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:15
msgid "If False, do not fork when building. Requires n_parallel=1."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:1
msgid ""
"Run generated code on remove devices. This function will ask a RPC "
"Tracker to get device for measurement."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:4
msgid "The timeout of a RPCRunner measurement task"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:8
msgid "The key of the device registered in the tracker"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:10
msgid "The host address of RPC Tracker"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:12
msgid "The port of RPC Tracker"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:5
#: tvm.autotvm.measure.measure_methods.RPCRunner:14
msgid ""
"The number of times to run the generated code for taking average. We call"
" these runs as one `repeat` of measurement."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:17
msgid ""
"The number of times to repeat the measurement. In total, the generated "
"code will be run (1 + number x repeat) times, where the first \"1\" is "
"warm up and will be discarded. The returned result contains `repeat` "
"costs, each of which is an average of `number` costs."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:14
#: tvm.autotvm.measure.measure_methods.RPCRunner:23
msgid ""
"The minimum duration of one `repeat` in milliseconds. By default, one "
"`repeat` contains `number` runs. If this parameter is set, the parameters"
" `number` will be dynamically adjusted to meet the minimum duration "
"requirement of one `repeat`. i.e., When the run time of one `repeat` "
"falls below this time, the `number` parameter will be automatically "
"increased."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:21
#: tvm.autotvm.measure.measure_methods.RPCRunner:30
msgid "The cool down interval between two measurements."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:23
#: tvm.autotvm.measure.measure_methods.RPCRunner:32
msgid ""
"Whether to flush cache on CPU between repeated measurements. Flushing "
"cache can make the measured latency of one operator closer to its actual "
"latency during end-to-end inference. To make this option effective, the "
"argument `number` should also be set to 1. This is only has effect on CPU"
" task."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:38
msgid ""
"If given, a context manager that loads the module to be timed into the "
"remote runtime. If not given, default_module_loader is used."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:1
msgid "Run generated code on local devices."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:8
msgid ""
"The number of times to repeat the measurement. In total, the generated "
"code will be run (1 + number x repeat) times, where the first one is warm"
" up and will be discarded. The returned result contains `repeat` costs, "
"each of which is an average of `number` costs."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:32
msgid ""
"This is a \"fake\" local mode. We start a silent rpc tracker and rpc "
"server for the user. In this way we reuse timeout/isolation mechanism in "
"RPC infrastructure."
msgstr ""

#: ../../_staging/reference/api/python/autotvm.rst:44
msgid "tvm.autotvm.tuner"
msgstr ""

#: of tvm.autotvm.tuner:1
msgid ""
"A tuner takes a task as input. It proposes some promising "
":any:`ConfigEntity` in the :any:`ConfigSpace` and measure them on the "
"real hardware. Then it proposed the next batch of :any:`ConfigEntity` "
"according to the measure results. This tuning loop is repeated."
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner:1
msgid "Base class for tuners"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:3
#: tvm.autotvm.tuner.tuner.Tuner:3
msgid "Tuning Task"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.has_next:1
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.has_next:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.has_next:1
#: tvm.autotvm.tuner.tuner.Tuner.has_next:1
msgid "Whether has next untried config in the space"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.has_next:3
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.has_next:3
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.has_next:3
#: tvm.autotvm.tuner.tuner.Tuner.has_next:3
msgid "**has_next**"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.next_batch:1
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch:1
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch:1
#: tvm.autotvm.tuner.tuner.Tuner.next_batch:1
msgid "get the next batch of configs to be measure on real hardware"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.next_batch:3
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch:3
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch:3
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch:3
#: tvm.autotvm.tuner.tuner.Tuner.next_batch:3
msgid "The size of the batch"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:1
#: tvm.autotvm.tuner.tuner.Tuner.update:1
msgid "Update parameters of the tuner according to measurement results"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:3
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:3
#: tvm.autotvm.tuner.tuner.Tuner.update:3
msgid "The input for measurement"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:5
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:5
#: tvm.autotvm.tuner.tuner.Tuner.update:5
msgid "result for measurement"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:1
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:1
msgid "Begin tuning"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:3
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:3
msgid "Maximum number of configs to try (measure on real hardware)"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:5
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:5
msgid ""
"The options for how to measure generated code. You should use the return "
"value ot autotvm.measure_option for this argument."
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:8
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:8
msgid ""
"Early stop the tuning when not finding better configs in this number of "
"trials"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:10
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:10
msgid ""
"A list of callback functions. The signature of callback function is "
"(Tuner, List of MeasureInput, List of MeasureResult) with no return "
"value. These callback functions will be called on every measurement pair."
" See autotvm/tuner/callback.py for some examples."
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:15
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:15
msgid ""
"One of tvm.autotvm.utils.SI_PREFIXES. The SI prefix to use when reporting"
" FLOPS."
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.reset:1
msgid "reset the status of tuner"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:1
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:1
#: tvm.autotvm.tuner.tuner.Tuner.load_history:1
msgid "load history data for transfer learning"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:3
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:3
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:3
#: tvm.autotvm.tuner.tuner.Tuner.load_history:3
msgid "Previous tuning records"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:5
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:5
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:5
#: tvm.autotvm.tuner.tuner.Tuner.load_history:5
msgid ""
"Defaults to 500. Indicates the minimum number of records to train the "
"tuner with. If there are less than `min_seed_records` number of records "
"in `data_set`, no training of the tuner will be done."
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:1
msgid "Enumerate the search space in a random order"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:5
msgid "A tuple of index range to random"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.GridSearchTuner:1
msgid "Enumerate the search space in a grid search order"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:1
msgid ""
"Tuner with genetic algorithm. This tuner does not have a cost model so it"
" always run measurement on real machines. This tuner expands the "
":code:`ConfigEntity` as gene."
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:5
msgid "number of genes in one generation"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:7
msgid "number of elite to keep"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:9
msgid "probability of mutation of a knob in a gene"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:1
msgid "Tuner that uses xgboost as cost model"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:5
msgid ""
"The size of a plan. After `plan_size` trials, the tuner will refit a new "
"cost model and do planing for the next `plan_size` trials."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:8
msgid ""
"If is 'itervar', use features extracted from IterVar (loop variable). If "
"is 'knob', use flatten ConfigEntity directly. If is 'curve', use sampled "
"curve feature (relation feature).  Note on choosing feature type: For "
"single task tuning, 'itervar' and 'knob' are good. 'itervar' is more "
"accurate but 'knob' is much faster. There are some constraints on "
"'itervar', if you meet problems with feature extraction when using "
"'itervar', you can switch to 'knob'.  For cross-shape tuning (e.g. many "
"convolutions with different shapes), 'itervar' and 'curve' has better "
"transferability, 'knob' is faster.  For cross-device or cross-operator "
"tuning, you can use 'curve' only."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:8
msgid ""
"If is 'itervar', use features extracted from IterVar (loop variable). If "
"is 'knob', use flatten ConfigEntity directly. If is 'curve', use sampled "
"curve feature (relation feature)."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:12
msgid ""
"Note on choosing feature type: For single task tuning, 'itervar' and "
"'knob' are good. 'itervar' is more accurate but 'knob' is much faster. "
"There are some constraints on 'itervar', if you meet problems with "
"feature extraction when using 'itervar', you can switch to 'knob'."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:19
msgid ""
"For cross-shape tuning (e.g. many convolutions with different shapes), "
"'itervar' and 'curve' has better transferability, 'knob' is faster."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:23
msgid "For cross-device or cross-operator tuning, you can use 'curve' only."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:25
msgid ""
"If is 'reg', use regression loss to train cost model. The cost model "
"predicts the normalized flops. If is 'rank', use pairwise rank loss to "
"train cost model. The cost model predicts relative rank score."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:30
msgid "The number of threads."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:32
msgid ""
"If is 'sa', use a default simulated annealing optimizer. Otherwise it "
"should be a ModelOptimizer object."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:35
msgid ""
"If is not None, the tuner will first select top-(plan_size * "
"diversity_filter_ratio) candidates according to the cost model and then "
"pick batch_size of them according to the diversity metric."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:39
msgid ""
"The verbose level. If is 0, output nothing. Otherwise, output debug "
"information every `verbose` iterations."
msgstr ""

#: of tvm.autotvm.tuner.callback:1
msgid "Namespace of callback utilities of AutoTVM"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:1
msgid ""
"Log the tuning records into file. The rows of the log are stored in the "
"format of autotvm.record.encode."
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:4
msgid "The file to log to."
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:6
msgid "The log protocol. Can be 'json' or 'pickle'"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:9
msgid "**callback** -- Callback function to do the logging."
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_database:1
msgid "Save the tuning records to a database object."
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_database:3
msgid "The database"
msgstr ""

#: of tvm.autotvm.tuner.callback.Monitor:1
msgid "A monitor to collect statistic during tuning"
msgstr ""

#: of tvm.autotvm.tuner.callback.Monitor.trial_scores:1
msgid "get scores (currently is flops) of all trials"
msgstr ""

#: of tvm.autotvm.tuner.callback.Monitor.trial_timestamps:1
msgid "get wall clock time stamp of all trials"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:1
msgid "Display progress bar for tuning"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:3
msgid "The total number of trials"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:5
msgid "The prefix of output message"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:7
msgid "SI prefix for flops"
msgstr ""

#: ../../_staging/reference/api/python/autotvm.rst:71
msgid "tvm.autotvm.task"
msgstr ""

#: of tvm.autotvm.task:1
msgid "Task is a tunable composition of template functions."
msgstr ""

#: of tvm.autotvm.task:3
msgid ""
"Tuner takes a tunable task and optimizes the joint configuration space of"
" all the template functions in the task. This module defines the task "
"data structure, as well as a collection(zoo) of typical tasks of "
"interest."
msgstr ""

#: of tvm.autotvm.task.task:1
msgid "Definition of task function."
msgstr ""

#: of tvm.autotvm.task.task:3
msgid ""
"Task can be constructed from tuple of func, args, and kwargs. func is a "
"state-less function, or a string that registers the standard task."
msgstr ""

#: of tvm.autotvm.task.task.serialize_args:1
msgid "serialize arguments of a topi function to a hashable tuple."
msgstr ""

#: of tvm.autotvm.task.task.deserialize_args:1
msgid "The inverse function of :code:`serialize_args`."
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:1
msgid ""
"Convert argument list to hashable workload tuple. This function will "
"convert list to tuple, tvm node to python value and flatten "
"te.tensor.Tensor to a tuple"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:5 tvm.autotvm.task.task.create:3
#: tvm.autotvm.task.topi_integration.register_topi_compute:8
#: tvm.autotvm.task.topi_integration.register_topi_schedule:11
msgid "The AutoTVM task name"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:7
msgid "The arguments to the function"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:10
msgid "**ret** -- The hashable value"
msgstr ""

#: of tvm.autotvm.task.task.Task:1
msgid "A Tunable Task"
msgstr ""

#: of tvm.autotvm.task.task.Task:3
msgid "The name of the task."
msgstr ""

#: of tvm.autotvm.task.task.Task:5
msgid "Positional argument of func"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:1
msgid ""
"Instantiate this task function (template) with a config. Returns "
"corresponding schedule."
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:4
msgid "parameter config for this template"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:7
msgid ""
"* **sch** (*tvm.te.schedule.Schedule*) -- The tvm schedule * **arg_bufs**"
" (*Array of te.tensor.Tensor*) -- The input/output buffers"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:7
msgid "**sch** (*tvm.te.schedule.Schedule*) -- The tvm schedule"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:8
msgid "**arg_bufs** (*Array of te.tensor.Tensor*) -- The input/output buffers"
msgstr ""

#: of tvm.autotvm.task.task.TaskTemplate:1
msgid "Task template is used to creates a tunable AutoTVM task."
msgstr ""

#: of tvm.autotvm.task.task.TaskTemplate:3
msgid ""
"It can be defined by a pair of compute and schedule function using "
"`_register_task_compute` and `_register_task_schedule`, or by a "
"customized task creation function that is more flexible using "
"`_register_customized_task`."
msgstr ""

#: of tvm.autotvm.task.task.TaskTemplate:8
msgid ""
"Note that when customized func is registered, compute and schedule "
"function will be ignored"
msgstr ""

#: of tvm.autotvm.task.task.MissingTask:1
msgid ""
"Dummy task template for a task lookup which cannot be resolved. This can "
"occur if the task being requested from _lookup_task() has not been "
"imported in this run."
msgstr ""

#: of tvm.autotvm.task.task.template:1
msgid "Decorate a function as a tunable schedule template."
msgstr ""

#: of tvm.autotvm.task.task.template:3
msgid "The task name"
msgstr ""

#: of tvm.autotvm.task.task.template:5
msgid ""
"A callable template function. If it is None, return a decorator. If is "
"callable, decorate this function."
msgstr ""

#: of tvm.autotvm.task.task.template:10
msgid "**func** -- The decorated function"
msgstr ""

#: of tvm.autotvm.task.task.template:15
msgid ""
"The following code is a tunable template for a blocked matrix "
"multiplication"
msgstr ""

#: of tvm.autotvm.task.task.create:1
msgid "Create a tuning task and initialize its search space"
msgstr ""

#: of tvm.autotvm.task.task.create:5
msgid "Positional arguments"
msgstr ""

#: of tvm.autotvm.task.task.create:7
msgid "The compilation target"
msgstr ""

#: of tvm.autotvm.task.task.create:9
msgid "The compilation target for host side"
msgstr ""

#: of tvm.autotvm.task.task.create:12
msgid "**tsk** -- a task object"
msgstr ""

#: of tvm.autotvm.task.task.get_config:1
msgid "Get current config object"
msgstr ""

#: of tvm.autotvm.task.task.get_config:3
msgid "**cfg** -- The current config"
msgstr ""

#: of tvm.autotvm.task.task.FlopCalculationError:1
msgid "Error happens when estimating FLOP for a compute op"
msgstr ""

#: of tvm.autotvm.task.task.compute_flop:1
msgid ""
"Calculate number of FLOP (floating number operations) of the compute ops "
"in a schedule"
msgstr ""

#: of tvm.autotvm.task.task.compute_flop:3
msgid "schedule"
msgstr ""

#: of tvm.autotvm.task.task.compute_flop:6
msgid "**flop** -- number of FLOP in this schedule"
msgstr ""

#: of tvm.autotvm.task.space:1
msgid "Template configuration space."
msgstr ""

#: of tvm.autotvm.task.space:3
msgid ""
"Each template function can be parameterized by a ConfigSpace. The space "
"is declared when we invoke the template function with ConfigSpace. During"
" evaluation, we pass in a ConfigEntity, which contains a specific entity "
"in the space. This entity contains deterministic parameters."
msgstr ""

#: ../../_staging/docstring of tvm.autotvm.task.space.Axis.index:1
msgid "Alias for field number 1"
msgstr ""

#: ../../_staging/docstring of tvm.autotvm.task.space.Axis.space:1
msgid "Alias for field number 0"
msgstr ""

#: of tvm.autotvm.task.space.InstantiationError:1
msgid ""
"Actively detected error in instantiating a template with a config, raised"
" by cfg.raise_error e.g. too many unrolling, too many threads in a block"
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:1
msgid ""
"Base class for transform space TransformSpace is the node in the "
"computation graph of axes"
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:6
msgid ""
"We can regard our schedule code as a transformation graph of axes. "
"Starting from raw axes in the definition of te.compute, we can transform "
"these axes by some operators. The operator includes 'split', 'reorder' "
"and 'annotate'. Each operator has some tunable parameters (e.g. the split"
" factor). Then the tuning process is just to find good parameters of "
"these op."
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:12
msgid ""
"So all the combinations of the parameters of these op form our search "
"space."
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:14
msgid ""
"Naming convention: We call the set of all possible values as XXXSpace. "
"(XXX can be Split, Reorder, Config ...) We call a specific entity in a "
"space as XXXEntity."
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace.get_num_output:1
#: tvm.autotvm.task.space.OtherOptionSpace.get_num_output:1
#: tvm.autotvm.task.space.ReorderSpace.get_num_output:1
#: tvm.autotvm.task.space.SplitSpace.get_num_output:1
#: tvm.autotvm.task.space.TransformSpace.get_num_output:1
#: tvm.autotvm.task.space.VirtualAxis.get_num_output:1
msgid "get number of output axes after this transform"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace.get_num_output:3
#: tvm.autotvm.task.space.OtherOptionSpace.get_num_output:3
#: tvm.autotvm.task.space.ReorderSpace.get_num_output:3
#: tvm.autotvm.task.space.SplitSpace.get_num_output:3
#: tvm.autotvm.task.space.TransformSpace.get_num_output:3
#: tvm.autotvm.task.space.VirtualAxis.get_num_output:3
msgid "**n** -- number of output axes"
msgstr ""

#: of tvm.autotvm.task.space.VirtualAxis:1
msgid "Axis placeholder in template"
msgstr ""

#: of tvm.autotvm.task.space.VirtualAxis:3
msgid ""
"If is int, return a virtual axis whose length is the provided argument. "
"If is IterVar, return a virtual axis whose length is extracted from the "
"IterVar's extent domain."
msgstr ""

#: of tvm.autotvm.task.space.get_factors:1
msgid "return all factors of an integer"
msgstr ""

#: of tvm.autotvm.task.space.get_factors:3
msgid "integer to factorize"
msgstr ""

#: of tvm.autotvm.task.space.get_factors:6
msgid "**factors** -- List of all factors"
msgstr ""

#: of tvm.autotvm.task.space.get_pow2s:1
msgid "return all power-of-two numbers that are less or equal than the integer"
msgstr ""

#: of tvm.autotvm.task.space.get_pow2s:3
msgid "integer for reference"
msgstr ""

#: of tvm.autotvm.task.space.get_pow2s:6
msgid "**factors** -- List of all power-of-two numbers"
msgstr ""

#: of tvm.autotvm.task.space.SplitSpace:1
msgid "Split an axis for several times"
msgstr ""

#: of tvm.autotvm.task.space.SplitEntity:1
msgid "A split operation with detailed parameters that can apply to an axis"
msgstr ""

#: of tvm.autotvm.task.space.SplitEntity:4
msgid ""
"the size of every axis after split. e.g. an axis of extent 128, we split "
"it into 3 axes, a possible size is [4, 4, 8] (4x4x8 = 128)."
msgstr ""

#: of tvm.autotvm.task.space.SplitEntity.apply:1
msgid "Apply split to an axis"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:3
#: tvm.autotvm.task.space.ReorderEntity.apply:3
#: tvm.autotvm.task.space.SplitEntity.apply:3
msgid "The tvm schedule"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:5
#: tvm.autotvm.task.space.ReorderEntity.apply:5
#: tvm.autotvm.task.space.SplitEntity.apply:5
msgid "The stage to be applied"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:7
#: tvm.autotvm.task.space.ConfigSpace.define_split:5
#: tvm.autotvm.task.space.ReorderEntity.apply:7
#: tvm.autotvm.task.space.SplitEntity.apply:7
msgid "axis to split"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity.apply:10
#: tvm.autotvm.task.space.SplitEntity.apply:10
msgid "**axes** -- The transformed axes."
msgstr ""

#: of tvm.autotvm.task.space.ReorderSpace:1
msgid "The parameter space for ordering an array of axes"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity:1
msgid "A reorder operation with detailed parameters that can apply to axes"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity:3
msgid "define the permutation"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity.apply:1
msgid "Apply reorder to an array of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace:1
msgid "The parameter space for annotating an array of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity:1
msgid "An annotation operation with detailed parameters that can apply to axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity:3
msgid "The annotations of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:1
msgid "Apply annotation to an array of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:9
msgid "the length of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:11
msgid "maximum unroll step"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:13
msgid "valid vector lanes for vectorization"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:15
msgid "cfg for recording error"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:17
msgid "source tensor for attaching cache"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:20
msgid "**axes** -- The transformed axes"
msgstr ""

#: of tvm.autotvm.task.space.OtherOptionSpace:1
msgid "The parameter space for general option"
msgstr ""

#: of tvm.autotvm.task.space.OtherOptionEntity:1
msgid "The parameter entity for general option, with a detailed value"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace:1
msgid ""
"The configuration space of a schedule. Pass it as config in template to "
"collect transformation space and build transform graph of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.axis:1
msgid "get a virtual axis (axis placeholder)"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.axis:3
msgid ""
"If is int, return an axis whose length is the provided argument. If is "
"IterVar, return an axis whose length is extracted from the IterVar's "
"extent domain."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:1
msgid "Define a new tunable knob which splits an axis into a list of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:3
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:3
#: tvm.autotvm.task.space.ConfigSpace.define_split:3
msgid "name to index the entity of this space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:7
msgid ""
"name of policy. If is 'factors', the tuner will try all divisible "
"factors. If is 'power2', the tuner will try power-of-two factors less or "
"equal to the length. If is 'verbose', the tuner will try all candidates "
"in above two policies. If is 'candidate', try given candidates."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:13
msgid ""
"extra arguments for policy  ``max_factor``:     the maximum split factor "
"(`int`). ``filter``:     see examples below for how to use filter "
"(`Callable[[int], bool]`). ``num_outputs``:     the total number of axis "
"after split (`int`). ``no_tail``:     should we only include divisible "
"numbers as split factors (`bool`). `candidate``:     (policy=candidate) "
"manual candidate list (`List`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:14
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:14
#: tvm.autotvm.task.space.ConfigSpace.define_split:13
msgid "extra arguments for policy"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:15
msgid "``max_factor``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:16
msgid "the maximum split factor (`int`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:17
msgid "``filter``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:18
msgid "see examples below for how to use filter (`Callable[[int], bool]`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:19
msgid "``num_outputs``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:20
msgid "the total number of axis after split (`int`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:21
msgid "``no_tail``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:22
msgid "should we only include divisible numbers as split factors (`bool`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:24
msgid "`candidate``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:24
msgid "(policy=candidate) manual candidate list (`List`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_reorder:1
msgid "Define a new tunable knob which reorders a list of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_reorder:5
msgid "axes to reorder"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_reorder:7
msgid ""
"name of policy If is 'identity', do an identity permutation. If is 'all',"
" try all permutations. If is 'interval_all', try all permutations of an "
"interval of axes. If is 'candidate', try listed candidate. If is "
"'interleave', interleave chains of spatial axes and chains of reduction "
"axes."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:1
msgid "Define a new tunable knob which annotates a list of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:5
msgid "axes to annotate"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:7
msgid ""
"name of policy If is 'unroll', unroll the axes. If is 'try_unroll', try "
"to unroll the axes. If is 'try_unroll_vec', try to unroll or vectorize "
"the axes. If is 'bind_gpu', bind the first few axes to gpu threads. If is"
" 'locate_cache', choose n axes to attach shared/local cache."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_knob:1
msgid "Define a tunable knob with a list of candidates"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_knob:3
msgid "name key of that option"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_knob:5
msgid "list of candidates"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.add_flop:1
msgid "Add float operation statistics for this tuning task"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.add_flop:3
msgid "number of float operations"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.raise_error:1
msgid ""
"register error in config Using this to actively detect error when "
"scheduling. Otherwise these error will occur during runtime, which will "
"cost more time."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.valid:1
msgid "Check whether the config meets all the constraints"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.valid:5
msgid ""
"This check should be called after instantiation of task, because the "
"ConfigEntity/ConfigSpace collects errors during instantiation"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.valid:8
msgid "**valid** -- whether the config meets all the constraints"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get:1
msgid "Get a config entity with detailed parameters from this space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get:3
msgid "index in the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:1
msgid "A configuration with detailed parameters"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:3
msgid "index of this config in space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:5
msgid "hash of schedule code"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:7
msgid "map name to transform entity"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:9
msgid "List of constraints"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_flatten_feature:1
msgid "flatten entities to a numerical one-dimensional feature vector"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_flatten_feature:3
msgid "**fea** -- one dimensional float32 array"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_other_option:1
msgid ""
"**other_option** -- other tunable parameters (tunable parameters defined "
"by `cfg.define_knob`)"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.to_json_dict:1
msgid "convert to a json serializable dictionary"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.to_json_dict:3
msgid "**json_dict** -- a json serializable dictionary"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:1
msgid "Build a ConfigEntity from json serializable dictionary"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:3
msgid ""
"Json serializable dictionary. This should be the return value of "
":any:`to_json_dict`."
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:7
msgid "**config** -- The corresponding config object"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity:1
msgid "The config entity created to support fallback"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:1
msgid "Fallback a split knob"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:3
msgid "name of the knob"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:5
msgid "The maximum tile size for every dimension. Value `-1` means no constraint."
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:10
msgid ""
"If you use cfg.define_split('tile_0', 128, num_outputs=3), Then "
"cfg.fallback_split('tile_0', [-1, 8, 4]) will give you cfg['tile_0'].size"
" = [4, 8, 4]"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:13
msgid ""
"If you use cfg.define_split('tile_0', 49, num_outputs=3), Then "
"cfg.fallback_split('tile_0', [-1, 8, 4]) will give you cfg['tile_0'].size"
" = [7, 7, 1]"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log:1
msgid ""
"A data driven fallback mechanism. We use tuned parameters from TopHub as "
"reference data. For an unseen shape, we find the most similar tuned one "
"from TopHub and mimic its parameters. Note that we are not matching by "
"workload (e.g., input size, kernel size), but instead matching by "
"configuration space. The idea is that if two workloads have similar "
"configuration space, their optimal configurations are also likely to be "
"similar."
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log:9
msgid "The reference log"
msgstr ""

#: of tvm.autotvm.task.dispatcher:1
msgid "Template dispatcher module."
msgstr ""

#: of tvm.autotvm.task.dispatcher:3
msgid ""
"A dispatcher is a function that can contains multiple behaviors. Its "
"specific behavior is can be controlled by DispatchContext."
msgstr ""

#: of tvm.autotvm.task.dispatcher:6
msgid ""
"DispatchContext is used in two ways, usually via different implementation"
" of the DispatchContext base class."
msgstr ""

#: of tvm.autotvm.task.dispatcher:9
msgid "During search, we can use it to pass the current proposal from tuner."
msgstr ""

#: of tvm.autotvm.task.dispatcher:10
msgid "During evaluation, we can use it to set pick the best policy."
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext:1
msgid "Base class of dispatch context."
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext:3
msgid ""
"DispatchContext enables the target and workload specific dispatch "
"mechanism for templates."
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext.query:1
msgid ""
"Query the context to get the specific config for a template. If cannot "
"find the result inside this context, this function will query it from the"
" upper contexts."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:3
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:3
#: tvm.autotvm.task.dispatcher.DispatchContext.query:5
#: tvm.autotvm.task.dispatcher.DispatchContext.update:3
#: tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:4
#: tvm.autotvm.task.dispatcher.FallbackContext.update:3
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:4
msgid "The current target"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:5
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:5
#: tvm.autotvm.task.dispatcher.DispatchContext.query:7
#: tvm.autotvm.task.dispatcher.DispatchContext.update:5
#: tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:6
#: tvm.autotvm.task.dispatcher.FallbackContext.update:5
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:6
msgid "The current workload."
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext.query:10
msgid "**cfg** -- The specific configuration."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:1
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:1
#: tvm.autotvm.task.dispatcher.DispatchContext.update:1
#: tvm.autotvm.task.dispatcher.FallbackContext.update:1
msgid "Update context with a specific config."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:7
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:7
#: tvm.autotvm.task.dispatcher.DispatchContext.update:7
#: tvm.autotvm.task.dispatcher.FallbackContext.update:7
msgid "The specific configuration."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:12
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:12
#: tvm.autotvm.task.dispatcher.DispatchContext.update:12
#: tvm.autotvm.task.dispatcher.FallbackContext.update:12
msgid ""
"This interface is for cases when TVM decides to replace an operator in "
"the graph. For example, `AlterOpLayout` pass (enables when `opt_level = "
"3`) replaces `NCHW` convolution with `NCHW[x]c` implementation on x86 "
"CPUs. Thus in TOPI, we first query schedule using original `NCHW` "
"workload, then update the dispatcher with the new `NCHW[x]c` workload. So"
" that later on, `NCHW[x]c` convolution can get schedule from the "
"dispatcher using its own workload directly."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:38
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:38
#: tvm.autotvm.task.dispatcher.DispatchContext.update:38
#: tvm.autotvm.task.dispatcher.FallbackContext.update:38
msgid ""
"We directly store `config` back because `conv2d_NCHW` and `conv2d_NCHWc` "
"share the same schedule parameters. One can construct a new "
"`ConfigEntity` if this is not the case."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyConfig:1
msgid "Apply a deterministic config entity for all queries."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyConfig:3
msgid "The specific configuration we care about."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyConfig.update:1
msgid "Override update"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest.load:1
msgid "Load records to this dispatch context"
msgstr ""

#: of tvm.autotvm.task.dispatcher.FallbackContext:1
msgid "A fallback dispatch context."
msgstr ""

#: of tvm.autotvm.task.dispatcher.FallbackContext:3
msgid ""
"Any tunable template can be called under this context. This is the root "
"context."
msgstr ""

#: of tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:1
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:1
msgid ""
"Clear fallback cache. Pass the same argument as _query_inside to this "
"function to clean the cache."
msgstr ""

#: of tvm.autotvm.task.dispatcher.clear_fallback_cache:9
msgid ""
"This is used in alter_op_layout to clear the bad cache created before "
"call topi compute function"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest:1
msgid "Load the graph level tuning optimal schedules."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest:3
msgid ""
"The input records should be in the ascending order of node index for "
"target operator. Usually this can be obtained with graph tuner."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest:7
msgid ""
"This context maintains an internal counter to indicate the current node "
"index."
msgstr ""

#: of tvm.autotvm.task.topi_integration:1
msgid "Decorators for registering tunable templates to TOPI."
msgstr ""

#: of tvm.autotvm.task.topi_integration:3
msgid ""
"These decorators can make your simple implementation be able to use "
"different configurations for different workloads. Here we directly use "
"all arguments to the TOPI call as \"workload\", so make sure all the "
"arguments (except tvm.te.Tensor) in you calls are hashable. For "
"tvm.te.Tensor, we will serialize it to a hashable tuple."
msgstr ""

#: of tvm.autotvm.task.topi_integration:9
#: tvm.autotvm.task.topi_integration.register_topi_compute:19
#: tvm.autotvm.task.topi_integration.register_topi_schedule:22
msgid "See tvm/topi/python/topi/arm_cpu/depthwise_conv2d.py for example usage."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv:1
msgid "Global environment for extracting tuning tasks from graph"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.reset:1
msgid "Reset task collections"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.reset:3
msgid "The relay ops to be extracted"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:1
msgid "Add AutoTVM task"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:3
msgid "AutoTVM task name."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:5
msgid "Arguments to the TOPI function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get_tasks:1
msgid "Get collected tasks"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get_tasks:3
msgid "**tasks** -- A list of tasks extracted from the graph"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:1
msgid "Get the single instance of TaskExtractEnv"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:3
msgid ""
"Whether to fetch all workloads in the network, even though some of them "
"are the same. This is useful for graph tuning."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:8
msgid "**env** -- The single instance of TaskExtractEnv"
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_compute:1
msgid "Register a tunable template for a topi compute function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_compute:3
msgid ""
"The registration will wrap this topi compute to take `cfg` as the first "
"argument, followed by the original argument list. It uses all its "
"argument as workload and stores this \"workload\" to its final ComputeOp,"
" which can be used to reconstruct \"workload\" in the following "
"topi_schedule call."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_compute:10
#: tvm.autotvm.task.topi_integration.register_topi_schedule:13
msgid "If it is None, return a decorator. If is callable, decorate this function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_compute:14
#: tvm.autotvm.task.topi_integration.register_topi_schedule:17
msgid "**decorator** -- A decorator"
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:1
msgid "Register a tunable template for a topi schedule function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:3
msgid ""
"The registration will wrap this topi schedule to take `cfg` as the first "
"argument, followed by the original argument list."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:6
msgid ""
"Note that this function will try to find \"workload\" from all the "
"ComputeOp in the input. You can attach \"workload\" to your compute op by"
" using :any:`register_topi_compute`."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:9
msgid ""
"The task name has to be the same as that of the corresponding topi "
"compute function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.get_workload:1
msgid "Retrieve the workload from outputs"
msgstr ""

#: ../../_staging/reference/api/python/autotvm.rst:88
msgid "tvm.autotvm.record"
msgstr ""

#: of tvm.autotvm.record:1
msgid "Tuning record and serialization format"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:1
msgid "get unique str key for MeasureInput"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:3
msgid "input for the measure"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:5
msgid "whether includes config in the str key"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:8
msgid "**key** -- The str representation of key"
msgstr ""

#: of tvm.autotvm.record.encode:1
msgid "encode (MeasureInput, MeasureResult) pair to a string"
msgstr ""

#: of tvm.autotvm.record.encode:5
msgid "pair of input/result"
msgstr ""

#: of tvm.autotvm.record.decode:5 tvm.autotvm.record.encode:7
msgid "log protocol, json or pickle"
msgstr ""

#: of tvm.autotvm.record.encode:10
msgid "**row** -- a row in the logger file"
msgstr ""

#: of tvm.autotvm.record.decode:1
msgid "Decode encoded record string to python object"
msgstr ""

#: of tvm.autotvm.record.decode:3
msgid "a row in the logger file"
msgstr ""

#: of tvm.autotvm.record.decode:8
msgid ""
"**ret** -- The tuple of input and result, or None if input uses old "
"version log format."
msgstr ""

#: of tvm.autotvm.record.load_from_file:1
msgid ""
"Generator: load records from file. This is a generator that yields the "
"records."
msgstr ""

#: of tvm.autotvm.record.load_from_file
msgid "Yields"
msgstr ""

#: of tvm.autotvm.record.load_from_file:7
msgid "**input** (*autotvm.measure.MeasureInput*)"
msgstr ""

#: of tvm.autotvm.record.load_from_file:8
msgid "**result** (*autotvm.measure.MeasureResult*)"
msgstr ""

#: of tvm.autotvm.record.split_workload:1
msgid ""
"Split a log file into separate files, each of which contains only a "
"single workload This function can also delete duplicated records in log "
"file"
msgstr ""

#: of tvm.autotvm.record.split_workload:4
msgid "input filename"
msgstr ""

#: of tvm.autotvm.record.split_workload:6
msgid "whether delete duplicated items"
msgstr ""

#: of tvm.autotvm.record.pick_best:1
msgid ""
"Pick the best entries from a file and store them to another file. This "
"function distills the useful log entries from a large log file. If "
"out_file already exists, the best entries from both in_file and out_file "
"will be saved."
msgstr ""

#: of tvm.autotvm.record.pick_best:6
msgid "The filename of input"
msgstr ""

#: of tvm.autotvm.record.pick_best:8
msgid "The filename of output"
msgstr ""


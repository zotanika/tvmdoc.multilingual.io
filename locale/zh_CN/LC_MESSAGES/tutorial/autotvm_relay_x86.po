# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:20+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/tutorial/autotvm_relay_x86.rst:13
msgid ""
"Click :ref:`here <sphx_glr_download_tutorial_autotvm_relay_x86.py>` to "
"download the full example code"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:22
msgid "Compiling and Optimizing a Model with the Python Interface (AutoTVM)"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:23
msgid "**Author**: `Chris Hoge <https://github.com/hogepodge>`_"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:26
msgid ""
"In the `TVMC Tutorial <tvmc_command_line_driver>`_, we covered how to "
"compile, run, and tune a pre-trained vision model, ResNet-50-v2 using the"
" command line interface for TVM, TVMC. TVM is more that just a command-"
"line tool though, it is an optimizing framework with APIs available for a"
" number of different languages that gives you tremendous flexibility in "
"working with machine learning models."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:32
msgid ""
"In this tutorial we will cover the same ground we did with TVMC, but show"
" how it is done with the Python API. Upon completion of this section, we "
"will have used the Python API for TVM to accomplish the following tasks:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:36
msgid "Compile a pre-trained ResNet 50 v2 model for the TVM runtime."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:37
msgid ""
"Run a real image through the compiled model, and interpret the output and"
" model performance."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:39
msgid "Tune the model that model on a CPU using TVM."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:40
msgid "Re-compile an optimized model using the tuning data collected by TVM."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:41
msgid ""
"Run the image through the optimized model, and compare the output and "
"model performance."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:44
msgid ""
"The goal of this section is to give you an overview of TVM's capabilites "
"and how to use them through the Python API."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:49
msgid ""
"TVM is a deep learning compiler framework, with a number of different "
"modules available for working with deep learning models and operators. In"
" this tutorial we will work through how to load, compile, and optimize a "
"model using the Python API."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:54
msgid ""
"We begin by importing a number of dependencies, including ``onnx`` for "
"loading and converting the model, helper utilities for downloading test "
"data, the Python Image Library for working with the image data, ``numpy``"
" for pre and post-processing of the image data, the TVM Relay framework, "
"and the TVM Graph Executor."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:83
msgid "Downloading and Loading the ONNX Model"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:85
msgid ""
"For this tutorial, we will be working with ResNet-50 v2. ResNet-50 is a "
"convolutional neural network that is 50-layers deep and designed to "
"classify images. The model we will be using has been pre-trained on more "
"than a million images with 1000 different classifications. The network "
"has an input image size of 224x224. If you are interested exploring more "
"of how the ResNet-50 model is structured, we recommend downloading "
"`Netron <https://netron.app>`_, a freely available ML model viewer."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:93
msgid ""
"TVM provides a helper library to download pre-trained models. By "
"providing a model URL, file name, and model type through the module, TVM "
"will download the model and save it to disk. For the instance of an ONNX "
"model, you can then load it into memory using the ONNX runtime."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:98
msgid "Working with Other Model Formats"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:100
msgid ""
"TVM supports many popular model formats. A list can be found in the "
":ref:`Compile Deep Learning Models <tutorial-frontend>` section of the "
"TVM Documentation."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:130
msgid "Downloading, Preprocessing, and Loading the Test Image"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:132
msgid ""
"Each model is particular when it comes to expected tensor shapes, formats"
" and data types. For this reason, most models require some pre and post-"
"processing, to ensure the input is valid and to interpret the output. "
"TVMC has adopted NumPy's ``.npz`` format for both input and output data."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:137
msgid ""
"As input for this tutorial, we will use the image of a cat, but you can "
"feel free to substitute this image for any of your choosing."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:145
msgid ""
"Download the image data, then convert it to a numpy array to use as an "
"input to the model."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:180
msgid "Compile the Model With Relay"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:182
msgid ""
"The next step is to compile the ResNet model. We begin by importing the "
"model to relay using the `from_onnx` importer. We then build the model, "
"with standard optimizations, into a TVM library.  Finally, we create a "
"TVM graph runtime module from the library."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:203
msgid "Defining the Correct Target"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:205
msgid ""
"Specifying the correct target can have a huge impact on the performance "
"of the compiled module, as it can take advantage of hardware features "
"available on the target. For more information, please refer to :ref"
":`Auto-tuning a convolutional network for x86 CPU <tune_relay_x86>`. We "
"recommend identifying which CPU you are running, along with optional "
"features, and set the target appropriately. For example, for some "
"processors ``target = \"llvm -mcpu=skylake\"``, or ``target = \"llvm "
"-mcpu=skylake-avx512\"`` for processors with the AVX-512 vector "
"instruction set."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:244
msgid "Execute on the TVM Runtime"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:245
msgid ""
"Now that we've compiled the model, we can use the TVM runtime to make "
"predictions with it. To use TVM to run the model and make predictions, we"
" need two things:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:249
msgid "The compiled model, which we just produced."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:250
msgid "Valid input to the model to make predictions on."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:273
msgid "Collect Basic Performance Data"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:274
msgid ""
"We want to collect some basic performance data associated with this "
"unoptimized model and compare it to a tuned model later. To help account "
"for CPU noise, we run the computation in multiple batches in multiple "
"repetitions, then gather some basis statistics on the mean, median, and "
"standard deviation."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:307
#: ../../_staging/tutorial/autotvm_relay_x86.rst:355
#: ../../_staging/tutorial/autotvm_relay_x86.rst:545
#: ../../_staging/tutorial/autotvm_relay_x86.rst:741
#: ../../_staging/tutorial/autotvm_relay_x86.rst:801
msgid "Out:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:319
msgid "Postprocess the output"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:321
msgid ""
"As previously mentioned, each model will have its own particular way of "
"providing output tensors."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:324
msgid ""
"In our case, we need to run some post-processing to render the outputs "
"from ResNet-50-V2 into a more human-readable form, using the lookup-table"
" provided for the model."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:370
msgid "This should produce the following output:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:383
msgid "Tune the model"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:384
msgid ""
"The previous model was compiled to work on the TVM runtime, but did not "
"include any platform specific optimization. In this section, we will show"
" you how to build an optimized model using TVM to target your working "
"platform."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:388
msgid ""
"In some cases, we might not get the expected performance when running "
"inferences using our compiled module. In cases like this, we can make use"
" of the auto-tuner, to find a better configuration for our model and get "
"a boost in performance. Tuning in TVM refers to the process by which a "
"model is optimized to run faster on a given target. This differs from "
"training or fine-tuning in that it does not affect the accuracy of the "
"model, but only the runtime performance. As part of the tuning process, "
"TVM will try running many different operator implementation variants to "
"see which perform best. The results of these runs are stored in a tuning "
"records file."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:398
msgid "In the simplest form, tuning requires you to provide three things:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:400
msgid "the target specification of the device you intend to run this model on"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:401
msgid "the path to an output file in which the tuning records will be stored"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:402
msgid "a path to the model to be tuned."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:423
msgid ""
"Set up some basic parameters for the runner. The runner takes compiled "
"code that is generated with a specific set of parameters and measures the"
" performance of it. ``number`` specifies the number of different "
"configurations that we will test, while ``repeat`` specifies how many "
"measurements we will take of each configuration. ``min_repeat_ms`` is a "
"value that specifies how long need to run configuration test. If the "
"number of repeats falls under this time, it will be increased. This "
"option is necessary for accurate tuning on GPUs, and is not required for "
"CPU tuning. Setting this value to 0 disables it. The ``timeout`` places "
"an upper limit on how long to run training code for each tested "
"configuration."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:462
msgid ""
"Create a simple structure for holding tuning options. We use an XGBoost "
"algorithim for guiding the search. For a production job, you will want to"
" set the number of trials to be larger than the value of 10 used here. "
"For CPU we recommend 1500, for GPU 3000-4000. The number of trials "
"required can depend on the particular model and processor, so it's worth "
"spending some time evaluating performance across a range of values to "
"find the best balance between tuning time and model optimization. Because"
" running tuning is time intensive we set number of trials to 10, but do "
"not recommend a value this small. The ``early_stopping`` parameter is the"
" minimum number of trails to run before a condition that stops the search"
" early can be applied. The measure option indicates where trial code will"
" be built, and where it will be run. In this case, we're using the "
"``LocalRunner`` we just created and a ``LocalBuilder``. The "
"``tuning_records`` option specifies a file to write the tuning data to."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:501
msgid "Defining the Tuning Search Algorithm"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:503
msgid ""
"By default this search is guided using an `XGBoost Grid` algorithm. "
"Depending on your model complexity and amount of time available, you "
"might want to choose a different algorithm."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:509
msgid "Setting Tuning Parameters"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:511
msgid ""
"In this example, in the interest of time, we set the number of trials and"
" early stopping to 10. You will likely see more performance improvements "
"if you set these values to be higher but this comes at the expense of "
"time spent tuning. The number of trials required for convergence will "
"vary depending on the specifics of the model and the target platform."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:653
msgid "The output from this tuning process will look something like this:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:686
msgid "Compiling an Optimized Model with Tuning Data"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:688
msgid ""
"As an output of the tuning process above, we obtained the tuning records "
"stored in ``resnet-50-v2-autotuning.json``. The compiler will use the "
"results to generate high performance code for the model on your specified"
" target."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:692
msgid ""
"Now that tuning data for the model has been collected, we can re-compile "
"the model using optimized operators to speed up our computations."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:716
msgid "Verify that the optimized model runs and produces the same results:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:756
msgid "Verifying that the predictions are the same:"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:769
msgid "Comparing the Tuned and Untuned Models"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:770
msgid ""
"We want to collect some basic performance data associated with this "
"optimized model to compare it to the unoptimized model. Depending on your"
" underlying hardware, number of iterations, and other factors, you should"
" see a performance improvement in comparing the optimized model to the "
"unoptimized model."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:814
msgid "Final Remarks"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:816
msgid ""
"In this tutorial, we gave a short example of how to use the TVM Python "
"API to compile, run, and tune a model. We also discussed the need for pre"
" and post-processing of inputs and outputs. After the tuning process, we "
"demonstrated how to compare the performance of the unoptimized and "
"optimize models."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:822
msgid ""
"Here we presented a simple example using ResNet 50 V2 locally. However, "
"TVM supports many more features including cross-compilation, remote "
"execution and profiling/benchmarking."
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:829
msgid "**Total running time of the script:** ( 6 minutes  41.462 seconds)"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:844
msgid ""
":download:`Download Python source code: autotvm_relay_x86.py "
"<autotvm_relay_x86.py>`"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:850
msgid ""
":download:`Download Jupyter notebook: autotvm_relay_x86.ipynb "
"<autotvm_relay_x86.ipynb>`"
msgstr ""

#: ../../_staging/tutorial/autotvm_relay_x86.rst:857
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


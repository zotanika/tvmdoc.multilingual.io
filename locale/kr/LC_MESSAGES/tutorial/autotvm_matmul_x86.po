# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-06 10:20+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:13
msgid ""
"Click :ref:`here <sphx_glr_download_tutorial_autotvm_matmul_x86.py>` to "
"download the full example code"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:24
msgid "Optimizing Operators with Schedule Templates and AutoTVM"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:25
msgid ""
"**Authors**: `Lianmin Zheng <https://github.com/merrymercy>`_, `Chris "
"Hoge <https://github.com/hogepodge>`_"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:29
msgid ""
"In this tutorial, we show how the TVM Tensor Expression (TE) language can"
" be used to write schedule templates that can be searched by AutoTVM to "
"find the optimal schedule. This process is called Auto-Tuning, which "
"helps automate the process of optimizing tensor computation."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:34
msgid ""
"This tutorial builds on the previous `tutorial on how to write a matrix "
"multiplication using TE <tensor_expr_get_started>`."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:37
msgid "There are two steps in auto-tuning."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:39
msgid "The first step is defining a search space."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:40
msgid ""
"The second step is running a search algorithm to explore through this "
"space."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:42
msgid ""
"In this tutorial, you can learn how to perform these two steps in TVM. "
"The whole workflow is illustrated by a matrix multiplication example."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:46
msgid ""
"Note that this tutorial will not run on Windows or recent versions of "
"macOS. To get it to run, you will need to wrap the body of this tutorial "
"in a :code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:53
msgid "Install dependencies"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:54
msgid "To use autotvm package in TVM, we need to install some extra dependencies."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:60
msgid ""
"To make TVM run faster in tuning, it is recommended to use cython as FFI "
"of TVM. In the root directory of TVM, execute:"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:68
msgid "Now return to python code. Begin by importing the required packages."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:96
msgid "Basic Matrix Multiplication with TE"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:97
msgid ""
"Recall the basic implementation of matrix multiplication using TE. We "
"write it down here with a few changes. We will wrap the multiplication in"
" a python function definition. For simplicity, we will focus our "
"attention on a split optimization, using a fixed value that defines the "
"block size of the reordering."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:140
msgid "Matrix Multiplication with AutoTVM"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:141
msgid ""
"In the previous schedule code, we use a constant \"8\" as the tiling "
"factor. However, it might not be the best one because the best tiling "
"factor depends on real hardware environment and input shape."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:145
msgid ""
"If you want the schedule code to be portable across a wider range of "
"input shapes and target hardware, it is better to define a set of "
"candidate values and pick the best one according to the measurement "
"results on target hardware."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:150
msgid ""
"In autotvm, we can define a tunable parameter, or a \"knob\" for such "
"kind of value."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:156
msgid "A Basic Matrix Multiplication Template"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:157
msgid ""
"We begin with an example of how to create a tunable parameter set for the"
" block size of the `split` scheduling operation."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:204
msgid ""
"Here we make four modifications to the previous schedule code and get a "
"tunable \"template\". We can explain the modifications one by one."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:207
msgid "Use a decorator to mark this function as a simple template."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:208
msgid ""
"Get a config object: You can regard this :code:`cfg` as an argument of "
"this function but we obtain it in a different way. With this argument, "
"this function is no longer a deterministic schedule. Instead, we can pass"
" different configurations to this function and get different schedules. A"
" function that uses a configuration object like this is called a "
"\"template\"."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:214
msgid ""
"To make the template function more compact, we can do two things to "
"define the parameter search space within a single function."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:217
msgid ""
"Define a search space across a set values. This is done by making "
":code:`cfg` a :any:`ConfigSpace` object. It will collect all of the "
"tunable knobs in this function and build a search space from it."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:220
msgid ""
"Schedule according to an entity in this space. This is done by making "
":code:`cfg` a :any:`ConfigEntity` object. When it is a "
":any:`ConfigEntity`, it will ignore all space definition API (namely, "
":code:`cfg.define_XXXXX(...)`). Instead, it will store deterministic "
"values for all tunable knobs, and we schedule according to these values."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:226
msgid ""
"During auto-tuning, we will first call this template with a "
":any:`ConfigSpace` object to build the search space. Then we call this "
"template with different :any:`ConfigEntity` in the built space to get "
"different schedules. Finally we will measure the code generated by "
"different schedules and pick the best one."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:232
msgid ""
"Define two tunable knobs. The first one is :code:`tile_y` with 5 possible"
" values. The second one is :code:`tile_x` with a same list of possible "
"values. These two knobs are independent, so they span a search space with"
" size 25 = 5x5."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:236
msgid ""
"The configuration knobs are passed to the :code:`split` schedule "
"operation, allowing us to schedule according to the 5x5 deterministic "
"values we previously defined in :code:`cfg`."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:243
msgid "A Matrix Multiplication Template with the Advanced Parameter API"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:244
msgid ""
"In the previous template, we manually listed all of the possible values "
"for a knob. This is the lowest level API to define the space, and gives "
"an explicit enumeration of the parameter space to search. However, we "
"also provide another set of APIs that can make the definition of the "
"search space easier and smarter. Where possible, we receomment you use "
"this higher-level API"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:250
msgid ""
"In the following example, we use :any:`ConfigSpace.define_split` to "
"define a split knob. It will enumerate all the possible ways to split an "
"axis and construct the space."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:254
msgid ""
"We also have :any:`ConfigSpace.define_reorder` for reorder knob and "
":any:`ConfigSpace.define_annotate` for annotation like unroll, "
"vectorization, thread binding. When the high level API cannot meet your "
"requirements, you can always fall back to using the low level API."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:302
msgid "More Explanation on :code:`cfg.define_split`"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:304
msgid ""
"In this template, :code:`cfg.define_split(\"tile_y\", y, num_outputs=2)` "
"will enumerate all possible combinations that can split axis y into two "
"axes with factors of the length of y. For example, if the length of y is "
"32 and we want to split it into two axes using factors of 32, then there "
"are 6 possible values for (length of outer axis, length of inner axis) "
"pair, namely (32, 1), (16, 2), (8, 4), (4, 8), (2, 16) or (1, 32). These "
"are all 6 possible values of `tile_y`."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:312
msgid ""
"During scheduling, :code:`cfg[\"tile_y\"]` is a :code:`SplitEntity` "
"object. We stores the lengths of outer axes and inner axes in "
":code:`cfg['tile_y'].size` (a tuple with two elements).  In this "
"template, we apply it by using :code:`yo, yi = cfg['tile_y'].apply(s, C, "
"y)`. Actually, this is equivalent to :code:`yo, yi = s[C].split(y, "
"cfg[\"tile_y\"].size[1])` or  :code:`yo, yi = s[C].split(y, "
"nparts=cfg['tile_y\"].size[0])`"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:320
msgid ""
"The advantage of using cfg.apply API is that it makes multi-level splits "
"(that is, when num_outputs >= 3) easier."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:326
msgid "Step 2: Use AutoTVM to Optimize the Matrix Multiplication"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:327
msgid ""
"In Step 1, we wrote a matrix multiplication template that allowed us to "
"paramaterize the block size used in the `split` schedule. We can now "
"conduct a search over this parameter space. The next step is to pick a "
"tuner to guide the exploration of this space."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:333
msgid "Auto-tuners in TVM"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:334
msgid "The job for a tuner can be described by following pseudo code"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:344
msgid ""
"When proposing the next batch of configs, the tuner can take different "
"strategies. Some of the tuner strategies provided by TVM include:"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:347
msgid ""
":any:`tvm.autotvm.tuner.RandomTuner`: Enumerate the space in a random "
"order"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:348
msgid ""
":any:`tvm.autotvm.tuner.GridSearchTuner`: Enumerate the space in a grid "
"search order"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:349
msgid ""
":any:`tvm.autotvm.tuner.GATuner`: Using genetic algorithm to search "
"through the space"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:350
msgid ""
":any:`tvm.autotvm.tuner.XGBTuner`: Uses a model based method. Train a "
"XGBoost model to predict the speed of lowered IR and pick the next batch "
"according to the prediction."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:354
msgid ""
"You can choose the tuner according to the size of your space, your time "
"budget and other factors.  For example, if your space is very small (less"
" than 1000), a gridsearch tuner or a random tuner is good enough. If your"
" space is at the level of 10^9 (this is the space size of a conv2d "
"operator on CUDA GPU), XGBoostTuner can explore more efficiently and find"
" better configs."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:363
msgid "Begin tuning"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:364
msgid ""
"Here we continue our matrix multiplication example. First we create a "
"tuning task. We can also inspect the initialized search space. In this "
"case, for a 512x512 square matrix multiplication, the space size is "
"10x10=100 Note that the task and search space are independent of the "
"tuner picked."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:384
#: ../../_staging/tutorial/autotvm_matmul_x86.rst:449
#: ../../_staging/tutorial/autotvm_matmul_x86.rst:505
msgid "Out:"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:398
msgid ""
"Then we need to define how to measure the generated code and pick a "
"tuner. Since our space is small, a random tuner is just okay."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:401
msgid ""
"We only make 10 trials in this tutorial for demonstration. In practice, "
"you can do more trials according to your time budget. We will log the "
"tuning results into a log file. This file can be used to choose the best "
"configuration discovered by the tuner later."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:424
msgid ""
"There are two steps for measuring a config: build and run. By default, we"
" use all CPU cores to compile program. We then measure them sequentially."
" To help reduce variance, we take 5 measurements and average them."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:470
msgid ""
"With tuning completed, we can choose the configuration from the log file "
"that has the best measured performance and compile the schedule with the "
"corresponding parameters. We also do a quick verfication that the "
"schedule is producing correct answers.  We can call the function "
":code:`matmul` directly under the :any:`autotvm.apply_history_best` "
"context. When we call this function, it will query the dispatch context "
"with its argument and get the best config with the same argument."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:517
msgid "Final Notes and Summary"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:518
msgid ""
"In this tutorial, we have shown how to build operator templates that "
"allow TVM to search a parameter space and choose optimized schedule "
"configurations. To gain a deeper understanding of how this works, we "
"recommend expanding on this example by adding new search parameters to "
"the schedule based on schedule operations demonstated in the `Getting "
"Started With Tensor Expressions <tensor_expr_get_started>_` tutorial. In "
"the upcoming sections, we will demonstate the AutoScheduler, a method for"
" TVM to optimize common operators without the need for the user to "
"provide a user-defined template."
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:540
msgid ""
":download:`Download Python source code: autotvm_matmul_x86.py "
"<autotvm_matmul_x86.py>`"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:546
msgid ""
":download:`Download Jupyter notebook: autotvm_matmul_x86.ipynb "
"<autotvm_matmul_x86.ipynb>`"
msgstr ""

#: ../../_staging/tutorial/autotvm_matmul_x86.rst:553
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


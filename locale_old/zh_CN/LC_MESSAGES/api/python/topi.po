# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-04 20:34+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../api/python/topi.rst:19
msgid "tvm.topi"
msgstr ""

#: of tvm.topi:1
msgid "TVM Operator Inventory."
msgstr ""

#: of tvm.topi:3
msgid ""
"TOPI is the operator collection library for TVM, to provide sugars for "
"constructing compute declaration as well as optimized schedules."
msgstr ""

#: of tvm.topi:6
msgid ""
"Some of the schedule function may have been specially optimized for a "
"specific workload."
msgstr ""

#: of tvm.topi:1 tvm.topi.nn:1
msgid "**Classes:**"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`AssertStmt <tvm.topi.tvm.topi.AssertStmt>`\\ \\(condition\\, "
"message\\, body\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.AssertStmt:1 tvm.topi:1:<autosummary>:1
msgid "AssertStmt node."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`Cast <tvm.topi.tvm.topi.Cast>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1 tvm.topi:1:<autosummary>:1
msgid "Cast expression."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`Evaluate <tvm.topi.tvm.topi.Evaluate>`\\ \\(value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.Evaluate:1 tvm.topi:1:<autosummary>:1
msgid "Evaluate node."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`StringImm <tvm.topi.tvm.topi.StringImm>`\\ \\(value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.StringImm:1 tvm.topi:1:<autosummary>:1
msgid "String constant."
msgstr ""

#: of tvm.topi:1 tvm.topi.image:1 tvm.topi.nn:1 tvm.topi.sparse:1
msgid "**Functions:**"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`abs <tvm.topi.tvm.topi.abs>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.abs:1 tvm.topi:1:<autosummary>:1
msgid "Take absolute value of the input of x, element-wise."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`acos <tvm.topi.tvm.topi.acos>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.acos:1 tvm.topi:1:<autosummary>:1
msgid "Take arc cos of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`acosh <tvm.topi.tvm.topi.acosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.acosh:1 tvm.topi:1:<autosummary>:1
msgid "Take arc cosh of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`add <tvm.topi.tvm.topi.add>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.add:1 tvm.topi:1:<autosummary>:1
msgid "Addition with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`adv_index <tvm.topi.tvm.topi.adv_index>`\\ \\(data\\, indices\\)"
msgstr ""

#: of tvm.topi.transform.adv_index:1 tvm.topi:1:<autosummary>:1
msgid "Numpy style indexing with tensors."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`all <tvm.topi.tvm.topi.all>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.all:1 tvm.topi:1:<autosummary>:1
msgid "Logical AND of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`any <tvm.topi.tvm.topi.any>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.any:1 tvm.topi:1:<autosummary>:1
msgid "Logical OR of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`arange <tvm.topi.tvm.topi.arange>`\\ \\(start\\[\\, stop\\, step\\,"
" dtype\\]\\)"
msgstr ""

#: of tvm.topi.transform.arange:1 tvm.topi:1:<autosummary>:1
msgid "Creates a tensor with evenly spaced values within a given interval."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`argmax <tvm.topi.tvm.topi.argmax>`\\ \\(data\\[\\, axis\\, "
"keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.argmax:1 tvm.topi:1:<autosummary>:1
msgid "Returns the indices of the maximum values along an axis."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`argmin <tvm.topi.tvm.topi.argmin>`\\ \\(data\\[\\, axis\\, "
"keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.argmin:1 tvm.topi:1:<autosummary>:1
msgid "Returns the indices of the minimum values along an axis."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`argsort <tvm.topi.tvm.topi.argsort>`\\ \\(data\\[\\, "
"valid\\_count\\, axis\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.sort.argsort:1 tvm.topi:1:<autosummary>:1
msgid ""
"Performs sorting along the given axis and returns an array of indices "
"having the same shape as an input array that index data in sorted order."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`argwhere <tvm.topi.tvm.topi.argwhere>`\\ \\(output\\_shape\\, "
"condition\\)"
msgstr ""

#: of tvm.topi.argwhere.argwhere:1 tvm.topi:1:<autosummary>:1
msgid "Find the indices of elements of a tensor that are non-zero."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`asin <tvm.topi.tvm.topi.asin>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.asin:1 tvm.topi:1:<autosummary>:1
msgid "Take arc sin of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`asinh <tvm.topi.tvm.topi.asinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.asinh:1 tvm.topi:1:<autosummary>:1
msgid "Take arc sinh of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`atan <tvm.topi.tvm.topi.atan>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.atan:1 tvm.topi:1:<autosummary>:1
msgid "Take atan of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`atanh <tvm.topi.tvm.topi.atanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.atanh:1 tvm.topi:1:<autosummary>:1
msgid "Take atanh of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`bitwise_and <tvm.topi.tvm.topi.bitwise_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.bitwise_and:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise bitwise and of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`bitwise_not <tvm.topi.tvm.topi.bitwise_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.topi.broadcast.bitwise_not:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise bitwise not of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`bitwise_or <tvm.topi.tvm.topi.bitwise_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.bitwise_or:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise bitwise or of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`bitwise_xor <tvm.topi.tvm.topi.bitwise_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.bitwise_xor:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise bitwise xor of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`broadcast_to <tvm.topi.tvm.topi.broadcast_to>`\\ \\(data\\, shape\\)"
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:1 tvm.topi:1:<autosummary>:1
msgid "Broadcast the src to the target shape"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`cast <tvm.topi.tvm.topi.cast>`\\ \\(x\\, dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.topi.math.cast:1 tvm.topi:1:<autosummary>:1
msgid "Cast input to specified data type."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`ceil <tvm.topi.tvm.topi.ceil>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.ceil:1 tvm.topi:1:<autosummary>:1
msgid "Take ceil of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`clip <tvm.topi.tvm.topi.clip>`\\ \\(x\\, a\\_min\\, a\\_max\\)"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid "Clip (limit) the values in an array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`concatenate <tvm.topi.tvm.topi.concatenate>`\\ \\(a\\_tuple\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.transform.concatenate:1 tvm.topi:1:<autosummary>:1
msgid "Join a sequence of arrays along an existing axis."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`const_vector <tvm.topi.tvm.topi.const_vector>`\\ \\(vector\\[\\, "
"name\\]\\)"
msgstr ""

#: of tvm.topi.utils.const_vector:1 tvm.topi:1:<autosummary>:1
msgid "convert a const numpy 1-dimensional vector to tvm tensor"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`cos <tvm.topi.tvm.topi.cos>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.cos:1 tvm.topi:1:<autosummary>:1
msgid "Take cos of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`cosh <tvm.topi.tvm.topi.cosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.cosh:1 tvm.topi:1:<autosummary>:1
msgid "Take cosh of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`decl_buffer <tvm.topi.tvm.topi.decl_buffer>`\\ \\(shape\\[\\, "
"dtype\\, name\\, data\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:1 tvm.topi:1:<autosummary>:1
msgid "Declare a new symbolic buffer."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`divide <tvm.topi.tvm.topi.divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.divide:1 tvm.topi:1:<autosummary>:1
msgid "Division with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`elemwise_sum <tvm.topi.tvm.topi.elemwise_sum>`\\ \\(xs\\)"
msgstr ""

#: of tvm.topi.tensor.elemwise_sum:1 tvm.topi:1:<autosummary>:1
msgid "Perform element-wise sum on inputs"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`equal <tvm.topi.tvm.topi.equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.equal:1 tvm.topi:1:<autosummary>:1
msgid "Compute (lhs==rhs) with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`erf <tvm.topi.tvm.topi.erf>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.erf:1 tvm.topi:1:<autosummary>:1
msgid "Take gauss error function of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`exp <tvm.topi.tvm.topi.exp>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.exp:1 tvm.topi:1:<autosummary>:1
msgid "Take exponential of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`expand_dims <tvm.topi.tvm.topi.expand_dims>`\\ \\(a\\, axis\\[\\, "
"num\\_newaxis\\]\\)"
msgstr ""

#: of tvm.topi.transform.expand_dims:1 tvm.topi:1:<autosummary>:1
msgid "Expand the shape of an array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`expand_like <tvm.topi.tvm.topi.expand_like>`\\ \\(a\\, "
"shape\\_like\\, axis\\)"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid "Expand an input array with the shape of second array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`extern <tvm.topi.tvm.topi.extern>`\\ \\(shape\\, inputs\\, "
"fcompute\\[\\, name\\, ...\\]\\)"
msgstr ""

#: of tvm.te.operation.extern:1 tvm.topi:1:<autosummary>:1
msgid "Compute several tensors via an extern function."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`fast_erf <tvm.topi.tvm.topi.fast_erf>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.fast_erf:1 tvm.topi:1:<autosummary>:1
msgid "Take gauss error function of input x using fast_erf implementation."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`fast_exp <tvm.topi.tvm.topi.fast_exp>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.fast_exp:1 tvm.topi:1:<autosummary>:1
msgid "Take exponential of input x using fast_exp implementation"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`fast_tanh <tvm.topi.tvm.topi.fast_tanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.fast_tanh:1 tvm.topi:1:<autosummary>:1
msgid "Take tanhonential of input x using fast_tanh implementation"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`fixed_point_multiply <tvm.topi.tvm.topi.fixed_point_multiply>`\\ "
"\\(x\\, multiplier\\, shift\\)"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply:1 tvm.topi:1:<autosummary>:1
msgid ""
"Fixed point multiplication between data and a fixed point constant "
"expressed as multiplier * 2^(-shift), where multiplier is a Q-number with"
" 31 fractional bits"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`flip <tvm.topi.tvm.topi.flip>`\\ \\(a\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.transform.flip:1 tvm.topi:1:<autosummary>:1
msgid "Flip/reverse elements of an array in a particular axis."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`floor <tvm.topi.tvm.topi.floor>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.floor:1 tvm.topi:1:<autosummary>:1
msgid "Take floor of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`floor_divide <tvm.topi.tvm.topi.floor_divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.floor_divide:1 tvm.topi:1:<autosummary>:1
msgid "Floor division with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`floor_mod <tvm.topi.tvm.topi.floor_mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.floor_mod:1 tvm.topi:1:<autosummary>:1
msgid "Floor modulus with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`full <tvm.topi.tvm.topi.full>`\\ \\(shape\\, dtype\\, "
"fill\\_value\\)"
msgstr ""

#: of tvm.topi.tensor.full:1 tvm.topi:1:<autosummary>:1
msgid "Fill tensor with fill_value"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`full_like <tvm.topi.tvm.topi.full_like>`\\ \\(x\\, fill\\_value\\)"
msgstr ""

#: of tvm.topi.tensor.full_like:2 tvm.topi:1:<autosummary>:1
msgid "Construct a tensor with same shape as input tensor,"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`gather <tvm.topi.tvm.topi.gather>`\\ \\(data\\, axis\\, indices\\)"
msgstr ""

#: of tvm.topi.transform.gather:1 tvm.topi:1:<autosummary>:1
msgid "Gather values along given axis from given indices."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`gather_nd <tvm.topi.tvm.topi.gather_nd>`\\ \\(a\\, indices\\)"
msgstr ""

#: of tvm.topi.transform.gather_nd:1 tvm.topi:1:<autosummary>:1
msgid "Gather elements from a n-dimension array.."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`get_const_tuple <tvm.topi.tvm.topi.get_const_tuple>`\\ "
"\\(in\\_tuple\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.get_const_tuple:1 tvm.topi:1:<autosummary>:1
msgid "Verifies input tuple is IntImm or Var, returns tuple of int or Var."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`greater <tvm.topi.tvm.topi.greater>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.greater:1 tvm.topi:1:<autosummary>:1
msgid "Compute (lhs>rhs) with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`greater_equal <tvm.topi.tvm.topi.greater_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.greater_equal:1 tvm.topi:1:<autosummary>:1
msgid "Compute (lhs>=rhs) with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`hybrid_argwhere_1d <tvm.topi.tvm.topi.hybrid_argwhere_1d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_1d:1 tvm.topi:1:<autosummary>:1
msgid "Find the indices of elements of a 1-D tensor that are non-zero."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`hybrid_argwhere_2d <tvm.topi.tvm.topi.hybrid_argwhere_2d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_2d:1 tvm.topi:1:<autosummary>:1
msgid "Find the indices of elements of a 2-D tensor that are non-zero."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`hybrid_argwhere_3d <tvm.topi.tvm.topi.hybrid_argwhere_3d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_3d:1 tvm.topi:1:<autosummary>:1
msgid "Find the indices of elements of a 3-D tensor that are non-zero."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`hybrid_argwhere_4d <tvm.topi.tvm.topi.hybrid_argwhere_4d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_4d:1 tvm.topi:1:<autosummary>:1
msgid "Find the indices of elements of a 4-D tensor that are non-zero."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`hybrid_argwhere_5d <tvm.topi.tvm.topi.hybrid_argwhere_5d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_5d:1 tvm.topi:1:<autosummary>:1
msgid "Find the indices of elements of a 5-D tensor that are non-zero."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`identity <tvm.topi.tvm.topi.identity>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.identity:1 tvm.topi:1:<autosummary>:1
msgid "Take identity of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`isfinite <tvm.topi.tvm.topi.isfinite>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.isfinite:1 tvm.topi:1:<autosummary>:1
msgid "Check if value of x is finite, element-wise."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`isinf <tvm.topi.tvm.topi.isinf>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.isinf:1 tvm.topi:1:<autosummary>:1
msgid "Check if value of x is infinite, element-wise."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`isnan <tvm.topi.tvm.topi.isnan>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.isnan:1 tvm.topi:1:<autosummary>:1
msgid "Check if value of x is NaN, element-wise."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`layout_transform <tvm.topi.tvm.topi.layout_transform>`\\ "
"\\(array\\, src\\_layout\\, dst\\_layout\\)"
msgstr ""

#: of tvm.topi.transform.layout_transform:1 tvm.topi:1:<autosummary>:1
msgid "Transform the layout according to src_layout and dst_layout"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`left_shift <tvm.topi.tvm.topi.left_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.left_shift:1 tvm.topi:1:<autosummary>:1
msgid "Left shift with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`less <tvm.topi.tvm.topi.less>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.less:1 tvm.topi:1:<autosummary>:1
msgid "Compute (lhs<rhs) with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`less_equal <tvm.topi.tvm.topi.less_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.less_equal:1 tvm.topi:1:<autosummary>:1
msgid "Compute (lhs<=rhs) with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`log <tvm.topi.tvm.topi.log>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.log:1 tvm.topi:1:<autosummary>:1
msgid "Take logarithm of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`log10 <tvm.topi.tvm.topi.log10>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.log10:1 tvm.topi:1:<autosummary>:1
msgid "Take logarithm to the base 10 of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`log2 <tvm.topi.tvm.topi.log2>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.log2:1 tvm.topi:1:<autosummary>:1
msgid "Take logarithm to the base 2 of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`logical_and <tvm.topi.tvm.topi.logical_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.logical_and:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise logical and of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`logical_not <tvm.topi.tvm.topi.logical_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.topi.broadcast.logical_not:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise logical not of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`logical_or <tvm.topi.tvm.topi.logical_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.logical_or:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise logical or of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`logical_xor <tvm.topi.tvm.topi.logical_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.logical_xor:1 tvm.topi:1:<autosummary>:1
msgid "Compute element-wise logical xor of data."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`make_idx <tvm.topi.tvm.topi.make_idx>`\\ \\(b\\, e\\, s\\, z\\, i\\)"
msgstr ""

#: of tvm.topi.utils.make_idx:1 tvm.topi:1:<autosummary>:1
msgid ""
"Return the array position in the selection that corresponds to an array "
"position in the full array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`matmul <tvm.topi.tvm.topi.matmul>`\\ \\(a\\, b\\[\\, transp\\_a\\, "
"transp\\_b\\]\\)"
msgstr ""

#: of tvm.topi.transform.matmul:1 tvm.topi:1:<autosummary>:1
msgid ""
"Creates an operation that calculates a matrix multiplication (row-major "
"notation): A(i, k) * B(k, j) if trans_a == trans_b, the usual transposed "
"combinations, otherwise"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`matrix_set_diag <tvm.topi.tvm.topi.matrix_set_diag>`\\ \\(data\\, "
"diagonal\\[\\, k\\, align\\]\\)"
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:1 tvm.topi:1:<autosummary>:1
msgid ""
"Returns a tensor with the diagonals of input tensor replaced with the "
"provided diagonal values."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`max <tvm.topi.tvm.topi.max>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.max:1 tvm.topi:1:<autosummary>:1
msgid "Maximum of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`maximum <tvm.topi.tvm.topi.maximum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.maximum:1 tvm.topi.broadcast.minimum:1
#: tvm.topi:1:<autosummary>:1
msgid "Take element-wise maximum of two tensors with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`meshgrid <tvm.topi.tvm.topi.meshgrid>`\\ \\(a\\_tuple\\, indexing\\)"
msgstr ""

#: of tvm.topi.transform.meshgrid:1 tvm.topi:1:<autosummary>:1
msgid "Create coordinate matrices from coordinate vectors."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`min <tvm.topi.tvm.topi.min>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.min:1 tvm.topi:1:<autosummary>:1
msgid "Minimum of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`minimum <tvm.topi.tvm.topi.minimum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`mod <tvm.topi.tvm.topi.mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.mod:1 tvm.topi:1:<autosummary>:1
msgid "Modulus with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`multiply <tvm.topi.tvm.topi.multiply>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.multiply:1 tvm.topi:1:<autosummary>:1
msgid "Multiplication with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`ndarray_size <tvm.topi.tvm.topi.ndarray_size>`\\ \\(array\\[\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.topi.transform.ndarray_size:1 tvm.topi:1:<autosummary>:1
msgid "Get the number of elements of input array"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`negative <tvm.topi.tvm.topi.negative>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.negative:1 tvm.topi:1:<autosummary>:1
msgid "Take negation of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`not_equal <tvm.topi.tvm.topi.not_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.not_equal:1 tvm.topi:1:<autosummary>:1
msgid "Compute (lhs!=rhs) with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`one_hot <tvm.topi.tvm.topi.one_hot>`\\ \\(indices\\, on\\_value\\, "
"off\\_value\\, depth\\, ...\\)"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
"Returns a one-hot tensor where the locations repsented by indices take "
"value on_value, other locations take value off_value."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`power <tvm.topi.tvm.topi.power>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.power:1 tvm.topi:1:<autosummary>:1
msgid "Power with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`prod <tvm.topi.tvm.topi.prod>`\\ \\(data\\[\\, axis\\, "
"keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.prod:1 tvm.topi:1:<autosummary>:1
msgid "Product of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`reinterpret <tvm.topi.tvm.topi.reinterpret>`\\ \\(x\\, dtype\\)"
msgstr ""

#: of tvm.topi.math.reinterpret:1 tvm.topi:1:<autosummary>:1
msgid "Reinterpret input to specified data type."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`repeat <tvm.topi.tvm.topi.repeat>`\\ \\(a\\, repeats\\, axis\\)"
msgstr ""

#: of tvm.topi.transform.repeat:1 tvm.topi:1:<autosummary>:1
msgid "Repeats elements of an array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`reshape <tvm.topi.tvm.topi.reshape>`\\ \\(a\\, newshape\\)"
msgstr ""

#: of tvm.topi.transform.reshape:1 tvm.topi:1:<autosummary>:1
msgid "Reshape the array"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`reverse_sequence <tvm.topi.tvm.topi.reverse_sequence>`\\ \\(a\\, "
"seq\\_lengths\\[\\, seq\\_axis\\, ...\\]\\)"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid "Reverse the tensor for variable length slices."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`right_shift <tvm.topi.tvm.topi.right_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.right_shift:1 tvm.topi:1:<autosummary>:1
msgid "Right shift with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`round <tvm.topi.tvm.topi.round>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.round:1 tvm.topi:1:<autosummary>:1
msgid "Round elements of x to nearest integer."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`rsqrt <tvm.topi.tvm.topi.rsqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.rsqrt:1 tvm.topi:1:<autosummary>:1
msgid "Take inverse square root of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`scatter <tvm.topi.tvm.topi.scatter>`\\ \\(data\\, indices\\, "
"updates\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.scatter.scatter:1 tvm.topi:1:<autosummary>:1
msgid "Update data at positions defined by indices with values in updates"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`scatter_add <tvm.topi.tvm.topi.scatter_add>`\\ \\(data\\, "
"indices\\, updates\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.scatter_add.scatter_add:1 tvm.topi:1:<autosummary>:1
msgid "Update data by adding values in updates at positions defined by indices"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`scatter_nd <tvm.topi.tvm.topi.scatter_nd>`\\ \\(data\\, indices\\, "
"shape\\)"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:1 tvm.topi:1:<autosummary>:1
msgid "Scatter elements from a n-dimension array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`sequence_mask <tvm.topi.tvm.topi.sequence_mask>`\\ \\(data\\, "
"valid\\_length\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.transform.sequence_mask:1 tvm.topi:1:<autosummary>:1
msgid ""
"Sets all elements outside the expected length of the sequence to a "
"constant value."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`shape <tvm.topi.tvm.topi.shape>`\\ \\(array\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.topi.transform.shape:1 tvm.topi:1:<autosummary>:1
msgid "Get the shape of input array"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`sigmoid <tvm.topi.tvm.topi.sigmoid>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.sigmoid:1 tvm.topi:1:<autosummary>:1
msgid "Take sigmoid tanh of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`sign <tvm.topi.tvm.topi.sign>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.sign:1 tvm.topi:1:<autosummary>:1
msgid "Returns -1, 0, 1 based on sign of x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`sin <tvm.topi.tvm.topi.sin>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.sin:1 tvm.topi:1:<autosummary>:1
msgid "Take sin of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`sinh <tvm.topi.tvm.topi.sinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.sinh:1 tvm.topi:1:<autosummary>:1
msgid "Take sinh of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`sort <tvm.topi.tvm.topi.sort>`\\ \\(data\\[\\, axis\\, "
"is\\_ascend\\]\\)"
msgstr ""

#: of tvm.topi.sort.sort:1 tvm.topi:1:<autosummary>:1
msgid ""
"Performs sorting along the given axis and returns an array in sorted "
"order."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`sparse_to_dense <tvm.topi.tvm.topi.sparse_to_dense>`\\ "
"\\(sparse\\_indices\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:1 tvm.topi:1:<autosummary>:1
msgid "Converts a sparse representation into a dense tensor."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`split <tvm.topi.tvm.topi.split>`\\ \\(ary\\, "
"indices\\_or\\_sections\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.transform.split:1 tvm.topi:1:<autosummary>:1
msgid "Split an array into multiple sub-arrays."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`sqrt <tvm.topi.tvm.topi.sqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.sqrt:1 tvm.topi:1:<autosummary>:1
msgid "Take square root of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`squeeze <tvm.topi.tvm.topi.squeeze>`\\ \\(a\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.transform.squeeze:1 tvm.topi:1:<autosummary>:1
msgid "Remove single-dimensional entries from the shape of an array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`stack <tvm.topi.tvm.topi.stack>`\\ \\(a\\, axis\\)"
msgstr ""

#: of tvm.topi.transform.stack:1 tvm.topi.transform.tile:1
#: tvm.topi:1:<autosummary>:1
msgid "Repeats the whole array multiple times."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`strided_set <tvm.topi.tvm.topi.strided_set>`\\ \\(a\\, v\\, "
"begin\\, end\\[\\, strides\\]\\)"
msgstr ""

#: of tvm.topi.transform.strided_set:1 tvm.topi:1:<autosummary>:1
msgid "Set slice of an array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`strided_slice <tvm.topi.tvm.topi.strided_slice>`\\ \\(a\\, begin\\,"
" end\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.transform.strided_slice:1 tvm.topi:1:<autosummary>:1
msgid "Slice of an array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`subtract <tvm.topi.tvm.topi.subtract>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.topi.broadcast.subtract:1 tvm.topi:1:<autosummary>:1
msgid "Subtraction with auto-broadcasting"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`sum <tvm.topi.tvm.topi.sum>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.topi.reduction.sum:1 tvm.topi:1:<autosummary>:1
msgid "Sum of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`take <tvm.topi.tvm.topi.take>`\\ \\(a\\, indices\\[\\, axis\\, "
"mode\\]\\)"
msgstr ""

#: of tvm.topi.transform.take:1 tvm.topi:1:<autosummary>:1
msgid "Take elements from an array along an axis."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`take_legalize <tvm.topi.tvm.topi.take_legalize>`\\ \\(attrs\\, "
"inputs\\, types\\)"
msgstr ""

#: of tvm.topi.transform.take_legalize:1 tvm.topi:1:<autosummary>:1
msgid "Legalizes dyn.topk op."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`tan <tvm.topi.tvm.topi.tan>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.tan:1 tvm.topi:1:<autosummary>:1
msgid "Take tan of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`tanh <tvm.topi.tvm.topi.tanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.tanh:1 tvm.topi:1:<autosummary>:1
msgid "Take hyperbolic tanh of input x."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`tensordot <tvm.topi.tvm.topi.tensordot>`\\ \\(a\\, b\\, axes\\)"
msgstr ""

#: of tvm.topi.transform.tensordot:1 tvm.topi:1:<autosummary>:1
msgid "A generalization of matrix multiplication to tensor."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`tile <tvm.topi.tvm.topi.tile>`\\ \\(a\\, reps\\)"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`topk <tvm.topi.tvm.topi.topk>`\\ \\(data\\[\\, k\\, axis\\, "
"ret\\_type\\, is\\_ascend\\, dtype\\]\\)"
msgstr ""

#: of tvm.topi.sort.topk:1 tvm.topi:1:<autosummary>:1
msgid "Get the top k elements in an input tensor along the given axis."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`transpose <tvm.topi.tvm.topi.transpose>`\\ \\(a\\[\\, axes\\]\\)"
msgstr ""

#: of tvm.topi.transform.transpose:1 tvm.topi:1:<autosummary>:1
msgid "Permute the dimensions of an array."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`trunc <tvm.topi.tvm.topi.trunc>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.math.trunc:1 tvm.topi:1:<autosummary>:1
msgid "Take truncated value of the input of x, element-wise."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`unravel_index <tvm.topi.tvm.topi.unravel_index>`\\ \\(indices\\, "
"shape\\)"
msgstr ""

#: of tvm.topi.transform.unravel_index:1 tvm.topi:1:<autosummary>:1
msgid ""
"Convert a flat index or array of flat indices into a tuple of coordinate "
"arrays."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":obj:`where <tvm.topi.tvm.topi.where>`\\ \\(condition\\, x\\, y\\)"
msgstr ""

#: of tvm.topi.transform.where:1 tvm.topi:1:<autosummary>:1
msgid "Get the elements, either from x or y, depending on the condition."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ""
":obj:`within_index <tvm.topi.tvm.topi.within_index>`\\ \\(b\\, e\\, s\\, "
"i\\)"
msgstr ""

#: of tvm.topi.utils.within_index:1 tvm.topi:1:<autosummary>:1
msgid "Return a boolean value that indicates if i is within the given index."
msgstr ""

#: of tvm.topi:1
msgid "**Exceptions:**"
msgstr ""

#: of tvm.tir.stmt.AssertStmt:1:<autosummary>:1
msgid ":obj:`InvalidShapeError <tvm.topi.tvm.topi.InvalidShapeError>`\\"
msgstr ""

#: of tvm.tir.stmt.AssertStmt:1:<autosummary>:1
msgid "Invalid shape for a topi function."
msgstr ""

#: of tvm.te.operation.extern tvm.tir.buffer.decl_buffer tvm.tir.expr.Cast
#: tvm.tir.expr.StringImm tvm.tir.stmt.AssertStmt tvm.tir.stmt.Evaluate
#: tvm.topi.argwhere.argwhere tvm.topi.argwhere.hybrid_argwhere_1d
#: tvm.topi.argwhere.hybrid_argwhere_2d tvm.topi.argwhere.hybrid_argwhere_3d
#: tvm.topi.argwhere.hybrid_argwhere_4d tvm.topi.argwhere.hybrid_argwhere_5d
#: tvm.topi.broadcast.add tvm.topi.broadcast.bitwise_and
#: tvm.topi.broadcast.bitwise_not tvm.topi.broadcast.bitwise_or
#: tvm.topi.broadcast.bitwise_xor tvm.topi.broadcast.broadcast_to
#: tvm.topi.broadcast.divide tvm.topi.broadcast.equal
#: tvm.topi.broadcast.floor_divide tvm.topi.broadcast.floor_mod
#: tvm.topi.broadcast.greater tvm.topi.broadcast.greater_equal
#: tvm.topi.broadcast.left_shift tvm.topi.broadcast.less
#: tvm.topi.broadcast.less_equal tvm.topi.broadcast.logical_and
#: tvm.topi.broadcast.logical_not tvm.topi.broadcast.logical_or
#: tvm.topi.broadcast.logical_xor tvm.topi.broadcast.maximum
#: tvm.topi.broadcast.minimum tvm.topi.broadcast.mod
#: tvm.topi.broadcast.multiply tvm.topi.broadcast.not_equal
#: tvm.topi.broadcast.power tvm.topi.broadcast.right_shift
#: tvm.topi.broadcast.subtract tvm.topi.image.dilation2d.dilation2d_nchw
#: tvm.topi.image.dilation2d.dilation2d_nhwc
#: tvm.topi.image.grid_sample.affine_grid
#: tvm.topi.image.grid_sample.grid_sample tvm.topi.image.resize.crop_and_resize
#: tvm.topi.image.resize.resize tvm.topi.image.resize.resize3d
#: tvm.topi.image.resize.resize_bicubic tvm.topi.image.resize.resize_bilinear
#: tvm.topi.image.resize.resize_nearest_neighbor tvm.topi.math.abs
#: tvm.topi.math.acos tvm.topi.math.acosh tvm.topi.math.asin
#: tvm.topi.math.asinh tvm.topi.math.atan tvm.topi.math.atanh
#: tvm.topi.math.cast tvm.topi.math.ceil tvm.topi.math.clip tvm.topi.math.cos
#: tvm.topi.math.cosh tvm.topi.math.erf tvm.topi.math.exp
#: tvm.topi.math.fast_erf tvm.topi.math.fast_exp tvm.topi.math.fast_tanh
#: tvm.topi.math.fixed_point_multiply tvm.topi.math.floor
#: tvm.topi.math.identity tvm.topi.math.isfinite tvm.topi.math.isinf
#: tvm.topi.math.isnan tvm.topi.math.log tvm.topi.math.log10 tvm.topi.math.log2
#: tvm.topi.math.negative tvm.topi.math.reinterpret tvm.topi.math.round
#: tvm.topi.math.rsqrt tvm.topi.math.sigmoid tvm.topi.math.sign
#: tvm.topi.math.sin tvm.topi.math.sinh tvm.topi.math.sqrt tvm.topi.math.tan
#: tvm.topi.math.tanh tvm.topi.math.trunc tvm.topi.nn.batch_matmul.batch_matmul
#: tvm.topi.nn.batch_to_space_nd.batch_to_space_nd
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc
#: tvm.topi.nn.bitserial_dense.bitserial_dense
#: tvm.topi.nn.bitserial_util.bitpack tvm.topi.nn.bnn.binarize_pack
#: tvm.topi.nn.bnn.binary_dense tvm.topi.nn.conv1d.conv1d
#: tvm.topi.nn.conv1d.conv1d_ncw tvm.topi.nn.conv1d.conv1d_nwc
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw tvm.topi.nn.conv2d.conv2d
#: tvm.topi.nn.conv2d.conv2d_NCHWc tvm.topi.nn.conv2d.conv2d_NCHWc_int8
#: tvm.topi.nn.conv2d.conv2d_alter_layout
#: tvm.topi.nn.conv2d.conv2d_gemm_weight_transform
#: tvm.topi.nn.conv2d.conv2d_hwcn tvm.topi.nn.conv2d.conv2d_infer_layout
#: tvm.topi.nn.conv2d.conv2d_legalize tvm.topi.nn.conv2d.conv2d_nchw
#: tvm.topi.nn.conv2d.conv2d_nhwc tvm.topi.nn.conv2d.conv2d_winograd_nhwc
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform
#: tvm.topi.nn.conv2d.group_conv2d_nchw tvm.topi.nn.conv2d.group_conv2d_nhwc
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw
#: tvm.topi.nn.conv3d.conv3d_alter_layout tvm.topi.nn.conv3d.conv3d_ncdhw
#: tvm.topi.nn.conv3d.conv3d_ndhwc
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw
#: tvm.topi.nn.correlation.correlation_nchw
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc tvm.topi.nn.dense.dense
#: tvm.topi.nn.depth_to_space.depth_to_space
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc tvm.topi.nn.dilate.dilate
#: tvm.topi.nn.elemwise.leaky_relu tvm.topi.nn.elemwise.prelu
#: tvm.topi.nn.elemwise.relu tvm.topi.nn.fifo_buffer.fifo_buffer
#: tvm.topi.nn.flatten.flatten tvm.topi.nn.local_response_norm.lrn
#: tvm.topi.nn.mapping.scale_shift_nchw tvm.topi.nn.mapping.scale_shift_nhwc
#: tvm.topi.nn.pad.mirror_pad tvm.topi.nn.pad.pad
#: tvm.topi.nn.pooling.adaptive_pool tvm.topi.nn.pooling.global_pool
#: tvm.topi.nn.pooling.pool tvm.topi.nn.pooling.pool1d
#: tvm.topi.nn.pooling.pool3d tvm.topi.nn.pooling.pool_grad
#: tvm.topi.nn.softmax.fast_softmax tvm.topi.nn.softmax.log_softmax
#: tvm.topi.nn.softmax.softmax tvm.topi.nn.space_to_batch_nd.space_to_batch_nd
#: tvm.topi.nn.space_to_depth.space_to_depth tvm.topi.nn.sparse.sparse_dense
#: tvm.topi.nn.sparse.sparse_dense_alter_layout
#: tvm.topi.nn.sparse.sparse_dense_v1 tvm.topi.nn.sparse.sparse_dense_v2
#: tvm.topi.nn.sparse.sparse_transpose tvm.topi.nn.upsampling.upsampling
#: tvm.topi.nn.upsampling.upsampling3d tvm.topi.nn.utils.get_pad_tuple
#: tvm.topi.nn.utils.get_pad_tuple1d tvm.topi.nn.utils.get_pad_tuple3d
#: tvm.topi.reduction.all tvm.topi.reduction.any tvm.topi.reduction.argmax
#: tvm.topi.reduction.argmin tvm.topi.reduction.max tvm.topi.reduction.min
#: tvm.topi.reduction.prod tvm.topi.reduction.sum tvm.topi.scatter.scatter
#: tvm.topi.scatter.scatter_nd tvm.topi.scatter_add.scatter_add
#: tvm.topi.sort.argsort tvm.topi.sort.sort tvm.topi.sort.topk
#: tvm.topi.sparse.csrmm.csrmm tvm.topi.sparse.csrmv.csrmv
#: tvm.topi.sparse.dense.dense tvm.topi.tensor.elemwise_sum
#: tvm.topi.tensor.full tvm.topi.tensor.full_like tvm.topi.transform.adv_index
#: tvm.topi.transform.arange tvm.topi.transform.concatenate
#: tvm.topi.transform.expand_dims tvm.topi.transform.expand_like
#: tvm.topi.transform.flip tvm.topi.transform.gather
#: tvm.topi.transform.gather_nd tvm.topi.transform.layout_transform
#: tvm.topi.transform.matmul tvm.topi.transform.matrix_set_diag
#: tvm.topi.transform.meshgrid tvm.topi.transform.ndarray_size
#: tvm.topi.transform.one_hot tvm.topi.transform.repeat
#: tvm.topi.transform.reshape tvm.topi.transform.reverse_sequence
#: tvm.topi.transform.sequence_mask tvm.topi.transform.shape
#: tvm.topi.transform.sparse_to_dense tvm.topi.transform.split
#: tvm.topi.transform.squeeze tvm.topi.transform.stack
#: tvm.topi.transform.strided_set tvm.topi.transform.strided_slice
#: tvm.topi.transform.take tvm.topi.transform.take_legalize
#: tvm.topi.transform.tensordot tvm.topi.transform.tile
#: tvm.topi.transform.transpose tvm.topi.transform.unravel_index
#: tvm.topi.transform.where tvm.topi.utils.const_vector
#: tvm.topi.utils.equal_const_int tvm.topi.utils.get_const_int
#: tvm.topi.utils.get_const_tuple tvm.topi.utils.make_idx
#: tvm.topi.utils.simplify tvm.topi.utils.within_index
msgid "Parameters"
msgstr ""

#: of tvm.tir.stmt.AssertStmt:3
msgid "The assert condition."
msgstr ""

#: of tvm.tir.stmt.AssertStmt:5
msgid "The error message."
msgstr ""

#: of tvm.tir.stmt.AssertStmt:7
msgid "The body statement."
msgstr ""

#: of tvm.tir.expr.Cast:7 tvm.tir.expr.StringImm:5 tvm.tir.stmt.AssertStmt:9
#: tvm.tir.stmt.Evaluate:5
msgid "The location of this itervar in the source code."
msgstr ""

#: of tvm.tir.expr.Cast:3
msgid "The data type"
msgstr ""

#: of tvm.tir.expr.Cast:5 tvm.tir.expr.StringImm:3
msgid "The value of the function."
msgstr ""

#: of tvm.tir.stmt.Evaluate:3
msgid "The expression to be evalued."
msgstr ""

#: of tvm.topi.math.abs:3 tvm.topi.math.acos:3 tvm.topi.math.acosh:3
#: tvm.topi.math.asin:3 tvm.topi.math.asinh:3 tvm.topi.math.atan:3
#: tvm.topi.math.atanh:3 tvm.topi.math.cast:3 tvm.topi.math.ceil:3
#: tvm.topi.math.clip:4 tvm.topi.math.cos:3 tvm.topi.math.cosh:3
#: tvm.topi.math.erf:3 tvm.topi.math.exp:3 tvm.topi.math.fast_erf:3
#: tvm.topi.math.fast_exp:3 tvm.topi.math.fast_tanh:3
#: tvm.topi.math.fixed_point_multiply:5 tvm.topi.math.floor:3
#: tvm.topi.math.identity:3 tvm.topi.math.isfinite:3 tvm.topi.math.isinf:3
#: tvm.topi.math.isnan:3 tvm.topi.math.log:3 tvm.topi.math.log10:3
#: tvm.topi.math.log2:3 tvm.topi.math.negative:3 tvm.topi.math.reinterpret:3
#: tvm.topi.math.round:3 tvm.topi.math.rsqrt:3 tvm.topi.math.sigmoid:3
#: tvm.topi.math.sign:3 tvm.topi.math.sin:3 tvm.topi.math.sinh:3
#: tvm.topi.math.sqrt:3 tvm.topi.math.tan:3 tvm.topi.math.tanh:3
#: tvm.topi.math.trunc:3 tvm.topi.nn.elemwise.leaky_relu:3
#: tvm.topi.nn.elemwise.prelu:7 tvm.topi.nn.elemwise.relu:3
#: tvm.topi.tensor.full_like:4
msgid "Input argument."
msgstr ""

#: of tvm.te.operation.extern tvm.tir.buffer.decl_buffer
#: tvm.topi.argwhere.argwhere tvm.topi.argwhere.hybrid_argwhere_1d
#: tvm.topi.argwhere.hybrid_argwhere_2d tvm.topi.argwhere.hybrid_argwhere_3d
#: tvm.topi.argwhere.hybrid_argwhere_4d tvm.topi.argwhere.hybrid_argwhere_5d
#: tvm.topi.broadcast.add tvm.topi.broadcast.bitwise_and
#: tvm.topi.broadcast.bitwise_not tvm.topi.broadcast.bitwise_or
#: tvm.topi.broadcast.bitwise_xor tvm.topi.broadcast.broadcast_to
#: tvm.topi.broadcast.divide tvm.topi.broadcast.equal
#: tvm.topi.broadcast.floor_divide tvm.topi.broadcast.floor_mod
#: tvm.topi.broadcast.greater tvm.topi.broadcast.greater_equal
#: tvm.topi.broadcast.left_shift tvm.topi.broadcast.less
#: tvm.topi.broadcast.less_equal tvm.topi.broadcast.logical_and
#: tvm.topi.broadcast.logical_not tvm.topi.broadcast.logical_or
#: tvm.topi.broadcast.logical_xor tvm.topi.broadcast.maximum
#: tvm.topi.broadcast.minimum tvm.topi.broadcast.mod
#: tvm.topi.broadcast.multiply tvm.topi.broadcast.not_equal
#: tvm.topi.broadcast.power tvm.topi.broadcast.right_shift
#: tvm.topi.broadcast.subtract tvm.topi.image.dilation2d.dilation2d_nchw
#: tvm.topi.image.dilation2d.dilation2d_nhwc
#: tvm.topi.image.grid_sample.affine_grid
#: tvm.topi.image.grid_sample.grid_sample tvm.topi.image.resize.crop_and_resize
#: tvm.topi.image.resize.resize tvm.topi.image.resize.resize3d
#: tvm.topi.image.resize.resize_bicubic tvm.topi.image.resize.resize_bilinear
#: tvm.topi.image.resize.resize_nearest_neighbor tvm.topi.math.abs
#: tvm.topi.math.acos tvm.topi.math.acosh tvm.topi.math.asin
#: tvm.topi.math.asinh tvm.topi.math.atan tvm.topi.math.atanh
#: tvm.topi.math.cast tvm.topi.math.ceil tvm.topi.math.clip tvm.topi.math.cos
#: tvm.topi.math.cosh tvm.topi.math.erf tvm.topi.math.exp
#: tvm.topi.math.fast_erf tvm.topi.math.fast_exp tvm.topi.math.fast_tanh
#: tvm.topi.math.fixed_point_multiply tvm.topi.math.floor
#: tvm.topi.math.identity tvm.topi.math.isfinite tvm.topi.math.isinf
#: tvm.topi.math.isnan tvm.topi.math.log tvm.topi.math.log10 tvm.topi.math.log2
#: tvm.topi.math.negative tvm.topi.math.reinterpret tvm.topi.math.round
#: tvm.topi.math.rsqrt tvm.topi.math.sigmoid tvm.topi.math.sign
#: tvm.topi.math.sin tvm.topi.math.sinh tvm.topi.math.sqrt tvm.topi.math.tan
#: tvm.topi.math.tanh tvm.topi.math.trunc tvm.topi.nn.batch_matmul.batch_matmul
#: tvm.topi.nn.batch_to_space_nd.batch_to_space_nd
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc
#: tvm.topi.nn.bitserial_dense.bitserial_dense tvm.topi.nn.bnn.binarize_pack
#: tvm.topi.nn.bnn.binary_dense
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw tvm.topi.nn.conv2d.conv2d
#: tvm.topi.nn.conv2d.conv2d_NCHWc tvm.topi.nn.conv2d.conv2d_NCHWc_int8
#: tvm.topi.nn.conv2d.conv2d_gemm_weight_transform
#: tvm.topi.nn.conv2d.conv2d_hwcn tvm.topi.nn.conv2d.conv2d_infer_layout
#: tvm.topi.nn.conv2d.conv2d_legalize tvm.topi.nn.conv2d.conv2d_nchw
#: tvm.topi.nn.conv2d.conv2d_nhwc tvm.topi.nn.conv2d.conv2d_winograd_nhwc
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform
#: tvm.topi.nn.conv2d.group_conv2d_nchw tvm.topi.nn.conv2d.group_conv2d_nhwc
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw
#: tvm.topi.nn.conv3d.conv3d_ncdhw tvm.topi.nn.conv3d.conv3d_ndhwc
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw
#: tvm.topi.nn.correlation.correlation_nchw
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc tvm.topi.nn.dense.dense
#: tvm.topi.nn.depth_to_space.depth_to_space
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc tvm.topi.nn.dilate.dilate
#: tvm.topi.nn.elemwise.leaky_relu tvm.topi.nn.elemwise.prelu
#: tvm.topi.nn.elemwise.relu tvm.topi.nn.fifo_buffer.fifo_buffer
#: tvm.topi.nn.flatten.flatten tvm.topi.nn.local_response_norm.lrn
#: tvm.topi.nn.mapping.scale_shift_nchw tvm.topi.nn.mapping.scale_shift_nhwc
#: tvm.topi.nn.pad.mirror_pad tvm.topi.nn.pad.pad
#: tvm.topi.nn.pooling.adaptive_pool tvm.topi.nn.pooling.global_pool
#: tvm.topi.nn.pooling.pool tvm.topi.nn.pooling.pool1d
#: tvm.topi.nn.pooling.pool3d tvm.topi.nn.pooling.pool_grad
#: tvm.topi.nn.softmax.fast_softmax tvm.topi.nn.softmax.log_softmax
#: tvm.topi.nn.softmax.softmax tvm.topi.nn.space_to_batch_nd.space_to_batch_nd
#: tvm.topi.nn.space_to_depth.space_to_depth tvm.topi.nn.sparse.sparse_dense
#: tvm.topi.nn.sparse.sparse_dense_v1 tvm.topi.nn.sparse.sparse_dense_v2
#: tvm.topi.nn.sparse.sparse_transpose tvm.topi.nn.upsampling.upsampling
#: tvm.topi.nn.upsampling.upsampling3d tvm.topi.nn.utils.get_pad_tuple
#: tvm.topi.nn.utils.get_pad_tuple1d tvm.topi.nn.utils.get_pad_tuple3d
#: tvm.topi.reduction.all tvm.topi.reduction.any tvm.topi.reduction.argmax
#: tvm.topi.reduction.argmin tvm.topi.reduction.max tvm.topi.reduction.min
#: tvm.topi.reduction.prod tvm.topi.reduction.sum tvm.topi.scatter.scatter
#: tvm.topi.scatter.scatter_nd tvm.topi.scatter_add.scatter_add
#: tvm.topi.sort.argsort tvm.topi.sort.sort tvm.topi.sort.topk
#: tvm.topi.sparse.csrmm.csrmm tvm.topi.sparse.csrmv.csrmv
#: tvm.topi.sparse.dense.dense tvm.topi.tensor.elemwise_sum
#: tvm.topi.tensor.full tvm.topi.tensor.full_like tvm.topi.transform.adv_index
#: tvm.topi.transform.arange tvm.topi.transform.concatenate
#: tvm.topi.transform.expand_dims tvm.topi.transform.expand_like
#: tvm.topi.transform.flip tvm.topi.transform.gather
#: tvm.topi.transform.gather_nd tvm.topi.transform.matmul
#: tvm.topi.transform.matrix_set_diag tvm.topi.transform.meshgrid
#: tvm.topi.transform.ndarray_size tvm.topi.transform.one_hot
#: tvm.topi.transform.repeat tvm.topi.transform.reshape
#: tvm.topi.transform.reverse_sequence tvm.topi.transform.sequence_mask
#: tvm.topi.transform.shape tvm.topi.transform.sparse_to_dense
#: tvm.topi.transform.split tvm.topi.transform.squeeze tvm.topi.transform.stack
#: tvm.topi.transform.strided_set tvm.topi.transform.strided_slice
#: tvm.topi.transform.take tvm.topi.transform.take_legalize
#: tvm.topi.transform.tensordot tvm.topi.transform.tile
#: tvm.topi.transform.transpose tvm.topi.transform.unravel_index
#: tvm.topi.transform.where tvm.topi.utils.const_vector
#: tvm.topi.utils.equal_const_int tvm.topi.utils.get_const_int
#: tvm.topi.utils.get_const_tuple tvm.topi.utils.make_idx
#: tvm.topi.utils.simplify tvm.topi.utils.within_index
msgid "Returns"
msgstr ""

#: of tvm.topi.math.abs:6 tvm.topi.math.acos:6 tvm.topi.math.acosh:6
#: tvm.topi.math.asin:6 tvm.topi.math.asinh:6 tvm.topi.math.atan:6
#: tvm.topi.math.atanh:6 tvm.topi.math.cast:10 tvm.topi.math.ceil:6
#: tvm.topi.math.clip:11 tvm.topi.math.cos:6 tvm.topi.math.cosh:6
#: tvm.topi.math.erf:6 tvm.topi.math.exp:6 tvm.topi.math.fast_erf:6
#: tvm.topi.math.fast_exp:6 tvm.topi.math.fast_tanh:6
#: tvm.topi.math.fixed_point_multiply:12 tvm.topi.math.floor:6
#: tvm.topi.math.identity:6 tvm.topi.math.isfinite:6 tvm.topi.math.isinf:6
#: tvm.topi.math.isnan:6 tvm.topi.math.log:6 tvm.topi.math.log10:6
#: tvm.topi.math.log2:6 tvm.topi.math.negative:6 tvm.topi.math.reinterpret:8
#: tvm.topi.math.round:6 tvm.topi.math.rsqrt:6 tvm.topi.math.sigmoid:6
#: tvm.topi.math.sign:6 tvm.topi.math.sin:6 tvm.topi.math.sinh:6
#: tvm.topi.math.sqrt:6 tvm.topi.math.tan:6 tvm.topi.math.tanh:6
#: tvm.topi.math.trunc:6 tvm.topi.nn.elemwise.leaky_relu:8
#: tvm.topi.nn.elemwise.relu:6 tvm.topi.tensor.elemwise_sum:6
#: tvm.topi.tensor.full:10 tvm.topi.tensor.full_like:9
msgid "**y** -- The result."
msgstr ""

#: of tvm.te.operation.extern tvm.tir.buffer.decl_buffer
#: tvm.topi.argwhere.argwhere tvm.topi.argwhere.hybrid_argwhere_1d
#: tvm.topi.argwhere.hybrid_argwhere_2d tvm.topi.argwhere.hybrid_argwhere_3d
#: tvm.topi.argwhere.hybrid_argwhere_4d tvm.topi.argwhere.hybrid_argwhere_5d
#: tvm.topi.broadcast.add tvm.topi.broadcast.bitwise_and
#: tvm.topi.broadcast.bitwise_not tvm.topi.broadcast.bitwise_or
#: tvm.topi.broadcast.bitwise_xor tvm.topi.broadcast.broadcast_to
#: tvm.topi.broadcast.divide tvm.topi.broadcast.equal
#: tvm.topi.broadcast.floor_divide tvm.topi.broadcast.floor_mod
#: tvm.topi.broadcast.greater tvm.topi.broadcast.greater_equal
#: tvm.topi.broadcast.left_shift tvm.topi.broadcast.less
#: tvm.topi.broadcast.less_equal tvm.topi.broadcast.logical_and
#: tvm.topi.broadcast.logical_not tvm.topi.broadcast.logical_or
#: tvm.topi.broadcast.logical_xor tvm.topi.broadcast.maximum
#: tvm.topi.broadcast.minimum tvm.topi.broadcast.mod
#: tvm.topi.broadcast.multiply tvm.topi.broadcast.not_equal
#: tvm.topi.broadcast.power tvm.topi.broadcast.right_shift
#: tvm.topi.broadcast.subtract tvm.topi.image.dilation2d.dilation2d_nchw
#: tvm.topi.image.dilation2d.dilation2d_nhwc
#: tvm.topi.image.grid_sample.affine_grid
#: tvm.topi.image.grid_sample.grid_sample tvm.topi.image.resize.crop_and_resize
#: tvm.topi.image.resize.resize tvm.topi.image.resize.resize3d
#: tvm.topi.image.resize.resize_bicubic tvm.topi.image.resize.resize_bilinear
#: tvm.topi.image.resize.resize_nearest_neighbor tvm.topi.math.abs
#: tvm.topi.math.acos tvm.topi.math.acosh tvm.topi.math.asin
#: tvm.topi.math.asinh tvm.topi.math.atan tvm.topi.math.atanh
#: tvm.topi.math.cast tvm.topi.math.ceil tvm.topi.math.clip tvm.topi.math.cos
#: tvm.topi.math.cosh tvm.topi.math.erf tvm.topi.math.exp
#: tvm.topi.math.fast_erf tvm.topi.math.fast_exp tvm.topi.math.fast_tanh
#: tvm.topi.math.fixed_point_multiply tvm.topi.math.floor
#: tvm.topi.math.identity tvm.topi.math.isfinite tvm.topi.math.isinf
#: tvm.topi.math.isnan tvm.topi.math.log tvm.topi.math.log10 tvm.topi.math.log2
#: tvm.topi.math.negative tvm.topi.math.reinterpret tvm.topi.math.round
#: tvm.topi.math.rsqrt tvm.topi.math.sigmoid tvm.topi.math.sign
#: tvm.topi.math.sin tvm.topi.math.sinh tvm.topi.math.sqrt tvm.topi.math.tan
#: tvm.topi.math.tanh tvm.topi.math.trunc tvm.topi.nn.batch_matmul.batch_matmul
#: tvm.topi.nn.batch_to_space_nd.batch_to_space_nd
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc
#: tvm.topi.nn.bitserial_dense.bitserial_dense tvm.topi.nn.bnn.binarize_pack
#: tvm.topi.nn.bnn.binary_dense
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw tvm.topi.nn.conv2d.conv2d
#: tvm.topi.nn.conv2d.conv2d_NCHWc tvm.topi.nn.conv2d.conv2d_NCHWc_int8
#: tvm.topi.nn.conv2d.conv2d_gemm_weight_transform
#: tvm.topi.nn.conv2d.conv2d_hwcn tvm.topi.nn.conv2d.conv2d_infer_layout
#: tvm.topi.nn.conv2d.conv2d_legalize tvm.topi.nn.conv2d.conv2d_nchw
#: tvm.topi.nn.conv2d.conv2d_nhwc tvm.topi.nn.conv2d.conv2d_winograd_nhwc
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform
#: tvm.topi.nn.conv2d.group_conv2d_nchw tvm.topi.nn.conv2d.group_conv2d_nhwc
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw
#: tvm.topi.nn.conv3d.conv3d_ncdhw tvm.topi.nn.conv3d.conv3d_ndhwc
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw
#: tvm.topi.nn.correlation.correlation_nchw
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc tvm.topi.nn.dense.dense
#: tvm.topi.nn.depth_to_space.depth_to_space
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc tvm.topi.nn.dilate.dilate
#: tvm.topi.nn.elemwise.leaky_relu tvm.topi.nn.elemwise.relu
#: tvm.topi.nn.fifo_buffer.fifo_buffer tvm.topi.nn.flatten.flatten
#: tvm.topi.nn.local_response_norm.lrn tvm.topi.nn.mapping.scale_shift_nchw
#: tvm.topi.nn.mapping.scale_shift_nhwc tvm.topi.nn.pad.mirror_pad
#: tvm.topi.nn.pad.pad tvm.topi.nn.pooling.adaptive_pool
#: tvm.topi.nn.pooling.global_pool tvm.topi.nn.pooling.pool
#: tvm.topi.nn.pooling.pool1d tvm.topi.nn.pooling.pool3d
#: tvm.topi.nn.pooling.pool_grad tvm.topi.nn.softmax.fast_softmax
#: tvm.topi.nn.softmax.log_softmax tvm.topi.nn.softmax.softmax
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd
#: tvm.topi.nn.space_to_depth.space_to_depth tvm.topi.nn.sparse.sparse_dense
#: tvm.topi.nn.sparse.sparse_dense_v1 tvm.topi.nn.sparse.sparse_dense_v2
#: tvm.topi.nn.upsampling.upsampling tvm.topi.nn.upsampling.upsampling3d
#: tvm.topi.reduction.all tvm.topi.reduction.any tvm.topi.reduction.argmax
#: tvm.topi.reduction.argmin tvm.topi.reduction.max tvm.topi.reduction.min
#: tvm.topi.reduction.prod tvm.topi.reduction.sum tvm.topi.scatter.scatter
#: tvm.topi.scatter.scatter_nd tvm.topi.scatter_add.scatter_add
#: tvm.topi.sort.argsort tvm.topi.sort.sort tvm.topi.sort.topk
#: tvm.topi.sparse.csrmm.csrmm tvm.topi.sparse.csrmv.csrmv
#: tvm.topi.sparse.dense.dense tvm.topi.tensor.elemwise_sum
#: tvm.topi.tensor.full tvm.topi.tensor.full_like tvm.topi.transform.adv_index
#: tvm.topi.transform.arange tvm.topi.transform.concatenate
#: tvm.topi.transform.expand_dims tvm.topi.transform.expand_like
#: tvm.topi.transform.flip tvm.topi.transform.gather
#: tvm.topi.transform.gather_nd tvm.topi.transform.matmul
#: tvm.topi.transform.matrix_set_diag tvm.topi.transform.meshgrid
#: tvm.topi.transform.ndarray_size tvm.topi.transform.one_hot
#: tvm.topi.transform.repeat tvm.topi.transform.reshape
#: tvm.topi.transform.reverse_sequence tvm.topi.transform.sequence_mask
#: tvm.topi.transform.shape tvm.topi.transform.sparse_to_dense
#: tvm.topi.transform.split tvm.topi.transform.squeeze tvm.topi.transform.stack
#: tvm.topi.transform.strided_set tvm.topi.transform.strided_slice
#: tvm.topi.transform.take tvm.topi.transform.take_legalize
#: tvm.topi.transform.tensordot tvm.topi.transform.tile
#: tvm.topi.transform.transpose tvm.topi.transform.unravel_index
#: tvm.topi.transform.where tvm.topi.utils.const_vector
#: tvm.topi.utils.equal_const_int tvm.topi.utils.get_const_int
#: tvm.topi.utils.get_const_tuple tvm.topi.utils.make_idx
#: tvm.topi.utils.simplify tvm.topi.utils.within_index
msgid "Return type"
msgstr ""

#: of tvm.topi.broadcast.add:3 tvm.topi.broadcast.bitwise_and:3
#: tvm.topi.broadcast.bitwise_or:3 tvm.topi.broadcast.bitwise_xor:3
#: tvm.topi.broadcast.divide:3 tvm.topi.broadcast.equal:3
#: tvm.topi.broadcast.floor_divide:3 tvm.topi.broadcast.floor_mod:3
#: tvm.topi.broadcast.greater:3 tvm.topi.broadcast.greater_equal:3
#: tvm.topi.broadcast.left_shift:3 tvm.topi.broadcast.less:3
#: tvm.topi.broadcast.less_equal:3 tvm.topi.broadcast.logical_and:3
#: tvm.topi.broadcast.logical_or:3 tvm.topi.broadcast.logical_xor:3
#: tvm.topi.broadcast.maximum:3 tvm.topi.broadcast.minimum:3
#: tvm.topi.broadcast.mod:3 tvm.topi.broadcast.multiply:3
#: tvm.topi.broadcast.not_equal:3 tvm.topi.broadcast.power:3
#: tvm.topi.broadcast.right_shift:3 tvm.topi.broadcast.subtract:3
msgid "The left operand"
msgstr ""

#: of tvm.topi.broadcast.add:5 tvm.topi.broadcast.bitwise_and:5
#: tvm.topi.broadcast.bitwise_or:5 tvm.topi.broadcast.bitwise_xor:5
#: tvm.topi.broadcast.divide:5 tvm.topi.broadcast.equal:5
#: tvm.topi.broadcast.floor_divide:5 tvm.topi.broadcast.floor_mod:5
#: tvm.topi.broadcast.greater:5 tvm.topi.broadcast.greater_equal:5
#: tvm.topi.broadcast.left_shift:5 tvm.topi.broadcast.less:5
#: tvm.topi.broadcast.less_equal:5 tvm.topi.broadcast.logical_and:5
#: tvm.topi.broadcast.logical_or:5 tvm.topi.broadcast.logical_xor:5
#: tvm.topi.broadcast.maximum:5 tvm.topi.broadcast.minimum:5
#: tvm.topi.broadcast.mod:5 tvm.topi.broadcast.multiply:5
#: tvm.topi.broadcast.not_equal:5 tvm.topi.broadcast.power:5
#: tvm.topi.broadcast.right_shift:5 tvm.topi.broadcast.subtract:5
msgid "The right operand"
msgstr ""

#: of tvm.topi.broadcast.add:8 tvm.topi.broadcast.bitwise_and:8
#: tvm.topi.broadcast.bitwise_or:8 tvm.topi.broadcast.bitwise_xor:8
#: tvm.topi.broadcast.divide:8 tvm.topi.broadcast.equal:8
#: tvm.topi.broadcast.floor_divide:8 tvm.topi.broadcast.floor_mod:8
#: tvm.topi.broadcast.greater:8 tvm.topi.broadcast.greater_equal:8
#: tvm.topi.broadcast.left_shift:8 tvm.topi.broadcast.less:8
#: tvm.topi.broadcast.less_equal:8 tvm.topi.broadcast.logical_and:8
#: tvm.topi.broadcast.logical_or:8 tvm.topi.broadcast.logical_xor:8
#: tvm.topi.broadcast.maximum:8 tvm.topi.broadcast.minimum:8
#: tvm.topi.broadcast.mod:8 tvm.topi.broadcast.multiply:8
#: tvm.topi.broadcast.not_equal:8 tvm.topi.broadcast.power:8
#: tvm.topi.broadcast.right_shift:8 tvm.topi.broadcast.subtract:8
msgid ""
"**ret** -- Returns Expr if both operands are Expr. Otherwise returns "
"Tensor."
msgstr ""

#: of tvm.topi.transform.adv_index:3
msgid "Input data."
msgstr ""

#: of tvm.topi.transform.adv_index:5
msgid "Tensor index."
msgstr ""

#: of tvm.topi.transform.adv_index:8
msgid "**result** -- Output tensor"
msgstr ""

#: of tvm.topi.reduction.all:3 tvm.topi.reduction.any:3
msgid "The input tvm boolean tensor"
msgstr ""

#: of tvm.topi.reduction.all:5
msgid ""
"Axis or axes along which a logical AND is performed. The default, "
"axis=None, will perform logical AND over all elements of the input array."
" If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.topi.reduction.all:9 tvm.topi.reduction.any:9
#: tvm.topi.reduction.argmax:9 tvm.topi.reduction.argmin:9
#: tvm.topi.reduction.max:9 tvm.topi.reduction.min:9 tvm.topi.reduction.prod:9
#: tvm.topi.reduction.sum:9
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one. With this option, the result will broadcast"
" correctly against the input array."
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:11 tvm.topi.reduction.all:14
#: tvm.topi.reduction.any:14 tvm.topi.reduction.argmax:14
#: tvm.topi.reduction.argmin:14 tvm.topi.reduction.max:14
#: tvm.topi.reduction.min:14 tvm.topi.reduction.prod:14
#: tvm.topi.reduction.sum:14 tvm.topi.scatter.scatter_nd:25
#: tvm.topi.transform.concatenate:8 tvm.topi.transform.expand_dims:8
#: tvm.topi.transform.expand_like:29 tvm.topi.transform.flip:8
#: tvm.topi.transform.gather:21 tvm.topi.transform.gather_nd:8
#: tvm.topi.transform.repeat:10 tvm.topi.transform.reshape:8
#: tvm.topi.transform.split:10 tvm.topi.transform.stack:8
#: tvm.topi.transform.strided_set:16 tvm.topi.transform.strided_slice:20
#: tvm.topi.transform.take:16 tvm.topi.transform.tile:8
#: tvm.topi.transform.transpose:8
msgid "**ret**"
msgstr ""

#: of tvm.topi.reduction.any:5
msgid ""
"Axis or axes along which a logical OR is performed. The default, "
"axis=None, will perform logical OR over all elements of the input array. "
"If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.topi.transform.arange:3
msgid ""
"Start of interval. The interval includes this value. The default start "
"value is 0."
msgstr ""

#: of tvm.topi.transform.arange:6
msgid "Stop of interval. The interval does not include this value."
msgstr ""

#: of tvm.topi.transform.arange:8
msgid "Spacing between values. The default step size is 1."
msgstr ""

#: of tvm.topi.transform.arange:10 tvm.topi.transform.ndarray_size:5
#: tvm.topi.transform.shape:5
msgid "The target data type."
msgstr ""

#: of tvm.topi.transform.arange:13 tvm.topi.transform.ndarray_size:8
#: tvm.topi.transform.shape:8
msgid "**result** -- The resulting tensor."
msgstr ""

#: of tvm.topi.reduction.argmax:3 tvm.topi.reduction.argmin:3
#: tvm.topi.reduction.max:3 tvm.topi.reduction.min:3 tvm.topi.reduction.prod:3
#: tvm.topi.reduction.sum:3
msgid "The input tvm tensor"
msgstr ""

#: of tvm.topi.reduction.argmax:5
msgid ""
"Axis or axes along which a argmax operation is performed. The default, "
"axis=None, will find the indices of the maximum element of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.topi.reduction.argmin:5
msgid ""
"Axis or axes along which a argmin operation is performed. The default, "
"axis=None, will find the indices of minimum element all of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.topi.sort.argsort:5 tvm.topi.sort.sort:4 tvm.topi.sort.topk:3
msgid "The input tensor."
msgstr ""

#: of tvm.topi.sort.argsort:7
msgid "1-D tensor for valid number of boxes."
msgstr ""

#: of tvm.topi.sort.argsort:9 tvm.topi.sort.sort:6
msgid ""
"Axis along which to sort the input tensor. By default the flattened array"
" is used."
msgstr ""

#: of tvm.topi.sort.argsort:12 tvm.topi.sort.sort:9 tvm.topi.sort.topk:14
msgid "Whether to sort in ascending or descending order."
msgstr ""

#: of tvm.topi.sort.argsort:14 tvm.topi.sort.sort:11
msgid "DType of the output indices."
msgstr ""

#: of tvm.topi.sort.argsort:17 tvm.topi.sort.sort:14
msgid "**out** -- Sorted index tensor."
msgstr ""

#: of tvm.te.operation.extern:40 tvm.tir.buffer.decl_buffer:43
#: tvm.topi.sort.argsort:21
msgid "Example"
msgstr ""

#: of tvm.topi.argwhere.argwhere:3
msgid "Tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.argwhere:6 tvm.topi.argwhere.hybrid_argwhere_1d:6
#: tvm.topi.argwhere.hybrid_argwhere_2d:6
#: tvm.topi.argwhere.hybrid_argwhere_3d:6
#: tvm.topi.argwhere.hybrid_argwhere_4d:6
#: tvm.topi.argwhere.hybrid_argwhere_5d:6
msgid "**out** -- Indices of non-zero elements."
msgstr ""

#: of tvm.topi.broadcast.bitwise_not:6 tvm.topi.broadcast.logical_not:6
msgid "**ret** -- Returns Expr if the operand are Expr. Otherwise returns Tensor."
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:3
msgid ""
"We follows the numpy broadcasting rule. See also "
"https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:6 tvm.topi.nn.fifo_buffer.fifo_buffer:17
msgid "The input data"
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:8
msgid "The target shape to be broadcasted."
msgstr ""

#: of tvm.topi.math.cast:5 tvm.topi.math.reinterpret:5
msgid "Data type."
msgstr ""

#: of tvm.topi.math.cast:7
msgid "The location of the cast in the source."
msgstr ""

#: of tvm.topi.math.clip:1
msgid ""
"Clip (limit) the values in an array. Given an interval, values outside "
"the interval are clipped to the interval edges."
msgstr ""

#: of tvm.topi.math.clip:6
msgid "Minimum value."
msgstr ""

#: of tvm.topi.math.clip:8
msgid "Maximum value."
msgstr ""

#: of tvm.topi.transform.concatenate:3
msgid "The arrays to concatenate"
msgstr ""

#: of tvm.topi.transform.concatenate:5
msgid "The axis along which the arrays will be joined. Default is 0."
msgstr ""

#: of tvm.topi.utils.const_vector:3
msgid "Const input array"
msgstr ""

#: of tvm.topi.utils.const_vector:5
msgid "The name of output op"
msgstr ""

#: of tvm.topi.utils.const_vector:8
msgid "**tensor** -- The created tensor"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:3
msgid ""
"Normally buffer is created automatically during lower and build. This is "
"only needed if user want to specify their own buffer layout."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:6
msgid "See the note below for detailed discussion on usage of buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:8
msgid "The shape of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:10
msgid "The data type of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:12
msgid "The name of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:14
msgid "The data pointer in the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:16
msgid "The stride of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:18
msgid ""
"The beginning offset of the array to data. In terms of number of elements"
" of dtype."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:21
msgid ""
"The storage scope of the buffer, if not global. If scope equals empty "
"string, it means it is global memory."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:24
msgid ""
"The alignment of data pointer in bytes. If -1 is passed, the alignment "
"will be set to TVM's internal default."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:27
msgid ""
"The factor of elem_offset field, when set, elem_offset is required to be "
"multiple of offset_factor. If 0 is pssed, the alignment will be set to 1."
" if non-zero is passed, we will created a Var for elem_offset if "
"elem_offset is not None."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:32
msgid ""
"auto_broadcast buffer allows one to implement broadcast computation "
"without considering whether dimension size equals to one. TVM maps "
"buffer[i][j][k] -> buffer[i][0][k] if dimension j's shape equals 1."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:36
msgid "The location of the decl_buffer creation in the source."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:39
msgid "**buffer** -- The created buffer"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:44
msgid ""
"Here's an example of how broadcast buffer can be used to define a "
"symbolic broadcast operation,"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:67
msgid ""
"Buffer data structure reflects the DLTensor structure in dlpack. While "
"DLTensor data structure is very general, it is usually helpful to create "
"function that only handles specific case of data structure and make "
"compiled function benefit from it."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:72
msgid ""
"If user pass strides and elem_offset is passed as None when constructing "
"the function, then the function will be specialized for the DLTensor that"
" is compact and aligned. If user pass a fully generic symbolic array to "
"the strides, then the resulting function becomes fully generic."
msgstr ""

#: of tvm.topi.tensor.elemwise_sum:3
msgid "Input arguments."
msgstr ""

#: of tvm.topi.transform.expand_dims:3 tvm.topi.transform.expand_like:22
#: tvm.topi.transform.flip:3 tvm.topi.transform.transpose:3
msgid "The tensor to be expanded."
msgstr ""

#: of tvm.topi.transform.expand_dims:5
msgid "Number of newaxis to be inserted on axis"
msgstr ""

#: of tvm.topi.transform.expand_like:1
msgid ""
"Expand an input array with the shape of second array. This operation can "
"always be composed of unsqueezing and expanding dims on those unsqueezed "
"axes."
msgstr ""

#: of tvm.topi.transform.expand_like:6 tvm.topi.transform.matrix_set_diag:25
#: tvm.topi.transform.one_hot:22
msgid "Examples"
msgstr ""

#: of tvm.topi.transform.expand_like:24
msgid "The tensor to with target shape."
msgstr ""

#: of tvm.topi.transform.expand_like:26
msgid "axis to be expanded on"
msgstr ""

#: of tvm.te.operation.extern:3
msgid "The shape of the outputs."
msgstr ""

#: of tvm.te.operation.extern:5
msgid "The inputs"
msgstr ""

#: of tvm.te.operation.extern:7
msgid ""
"Specifies the IR statement to do the computation. See the following note "
"for function signature of fcompute  .. note::      **Parameters**       -"
" **ins** (list of :any:`Buffer`) - Placeholder for each inputs      - "
"**outs** (list of :any:`Buffer`) - Placeholder for each outputs       "
"**Returns**       - **stmt** (:any:`Stmt`) - The statement that carries "
"out array computation."
msgstr ""

#: of tvm.te.operation.extern:7
msgid ""
"Specifies the IR statement to do the computation. See the following note "
"for function signature of fcompute"
msgstr ""

#: of tvm.te.operation.extern:11
msgid "**Parameters**"
msgstr ""

#: of tvm.te.operation.extern:13
msgid "**ins** (list of :any:`Buffer`) - Placeholder for each inputs"
msgstr ""

#: of tvm.te.operation.extern:14
msgid "**outs** (list of :any:`Buffer`) - Placeholder for each outputs"
msgstr ""

#: of tvm.te.operation.extern:16
msgid "**Returns**"
msgstr ""

#: of tvm.te.operation.extern:18
msgid "**stmt** (:any:`Stmt`) - The statement that carries out array computation."
msgstr ""

#: of tvm.te.operation.extern:20
msgid "The name hint of the tensor"
msgstr ""

#: of tvm.te.operation.extern:22
msgid "The data types of outputs, by default dtype will be same as inputs."
msgstr ""

#: of tvm.te.operation.extern:25
msgid "Input buffers."
msgstr ""

#: of tvm.te.operation.extern:27
msgid "Output buffers."
msgstr ""

#: of tvm.te.operation.extern:31
msgid "tag: str, optional"
msgstr ""

#: of tvm.te.operation.extern:31
msgid "Additonal tag information about the compute."
msgstr ""

#: of tvm.te.operation.extern:34
msgid "attrs: dict, optional"
msgstr ""

#: of tvm.te.operation.extern:34
msgid "The additional auxiliary attributes about the compute."
msgstr ""

#: of tvm.te.operation.extern:36
msgid ""
"**tensor** -- The created tensor or tuple of tensors it it contains "
"multiple outputs."
msgstr ""

#: of tvm.te.operation.extern:41
msgid ""
"In the code below, C is generated by calling external PackedFunc "
"`tvm.contrib.cblas.matmul`"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply:7
msgid ""
"Multiplier of a fixed floating point number described as "
"multiplier*2^(-shift)."
msgstr ""

#: of tvm.topi.math.fixed_point_multiply:9
msgid "Shift of a fixed floating point number described as multiplier*2^(-shift)."
msgstr ""

#: of tvm.topi.transform.flip:5
msgid "The axis along which the tensors will be reveresed."
msgstr ""

#: of tvm.topi.tensor.full:3
msgid "Input tensor shape."
msgstr ""

#: of tvm.topi.tensor.full:5
msgid "Data type"
msgstr ""

#: of tvm.topi.tensor.full:7 tvm.topi.tensor.full_like:6
msgid "Value to be filled"
msgstr ""

#: of tvm.topi.tensor.full_like:2
msgid "then fill tensor with fill_value."
msgstr ""

#: of tvm.topi.transform.gather:3
msgid "E.g. for a 3D tensor, output is computed as:"
msgstr ""

#: of tvm.topi.transform.gather:11
msgid ""
"``indices`` must have same shape as ``data``, except at dimension "
"``axis`` which must just be not null. Output will have same shape as "
"``indices``."
msgstr ""

#: of tvm.topi.scatter.scatter:3 tvm.topi.scatter_add.scatter_add:3
#: tvm.topi.transform.gather:14
msgid "The input data to the operator."
msgstr ""

#: of tvm.topi.transform.gather:16
msgid "The axis along which to index."
msgstr ""

#: of tvm.topi.scatter.scatter_nd:20 tvm.topi.transform.gather:18
#: tvm.topi.transform.gather_nd:5 tvm.topi.transform.take:5
msgid "The indices of the values to extract."
msgstr ""

#: of tvm.topi.scatter.scatter_nd:18 tvm.topi.transform.gather_nd:3
#: tvm.topi.transform.layout_transform:3 tvm.topi.transform.take:3
msgid "The source array."
msgstr ""

#: of tvm.topi.utils.get_const_tuple:3 tvm.topi.utils.simplify:3
msgid "The input."
msgstr ""

#: of tvm.topi.utils.get_const_tuple:6
msgid "**out_tuple** -- The output."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_1d:3
msgid "1-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_2d:3
msgid "2-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_3d:3
msgid "3-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_4d:3
msgid "4-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_5d:3
msgid "5-D tensor with boolean values."
msgstr ""

#: of tvm.topi.transform.layout_transform:5
msgid "the source layout."
msgstr ""

#: of tvm.topi.transform.layout_transform:7
msgid "the destination layout."
msgstr ""

#: of tvm.topi.utils.make_idx:4
msgid ""
"The returned value is only meaningful if within_index() returns True for "
"the same set of parameters."
msgstr ""

#: of tvm.topi.utils.make_idx:7 tvm.topi.utils.within_index:3
msgid "beginning of the index"
msgstr ""

#: of tvm.topi.utils.make_idx:9 tvm.topi.utils.within_index:5
msgid "end of the index"
msgstr ""

#: of tvm.topi.utils.make_idx:11 tvm.topi.utils.within_index:7
msgid "strides of index"
msgstr ""

#: of tvm.topi.utils.make_idx:13
msgid "size of the indexed dimension"
msgstr ""

#: of tvm.topi.utils.make_idx:15 tvm.topi.utils.within_index:9
msgid "array position"
msgstr ""

#: of tvm.topi.utils.make_idx:18
msgid ""
"**postion** -- int expression that corresponds to an array position in "
"the selection."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:3
msgid "Input Tensor."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:5
msgid "Values to be filled in the diagonal."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:7
msgid ""
"Diagonal Offset(s). The diagonal or range of diagonals to set. (0 by "
"default) Positive value means superdiagonal, 0 refers to the main "
"diagonal, and negative value means subdiagonals. k can be a single "
"integer (for a single diagonal) or a pair of integers specifying the low "
"and high ends of a matrix band. k[0] must not be larger than k[1]."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:13
msgid ""
"Some diagonals are shorter than max_diag_len and need to be padded. align"
" is a string specifying how superdiagonals and subdiagonals should be "
"aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\""
" (default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". "
"\"RIGHT_LEFT\" aligns superdiagonals to the right (left-pads the row) and"
" subdiagonals to the left (right-pads the row). It is the packing format "
"LAPACK uses. cuSPARSE uses \"LEFT_RIGHT\", which is the opposite "
"alignment."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:21
msgid "**result** -- New tensor with given diagonal values."
msgstr ""

#: of tvm.topi.reduction.max:5
msgid ""
"Axis or axes along which the max operation is performed. The default, "
"axis=None, will find the max element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.topi.transform.meshgrid:3
msgid "The coordinate vectors or scalars."
msgstr ""

#: of tvm.topi.transform.meshgrid:5
msgid "Indexing mode, either \"ij\" or \"xy\"."
msgstr ""

#: of tvm.topi.transform.meshgrid:8
msgid "**result** -- The resulting grids for each axis."
msgstr ""

#: of tvm.topi.reduction.min:5
msgid ""
"Axis or axes along which a minimum operation is performed. The default, "
"axis=None, will find the minimum element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.topi.transform.ndarray_size:3 tvm.topi.transform.shape:3
msgid "The source tensor."
msgstr ""

#: of tvm.topi.transform.one_hot:1
msgid ""
"Returns a one-hot tensor where the locations repsented by indices take "
"value on_value, other locations take value off_value. Final dimension is "
"<indices outer dimensions> x depth x <indices inner dimensions>."
msgstr ""

#: of tvm.topi.transform.one_hot:5
msgid "Locations to set to on_value."
msgstr ""

#: of tvm.topi.transform.one_hot:7
msgid "Value to fill at indices."
msgstr ""

#: of tvm.topi.transform.one_hot:9
msgid "Value to fill at all other positions besides indices."
msgstr ""

#: of tvm.topi.transform.one_hot:11
msgid "Depth of the one-hot dimension."
msgstr ""

#: of tvm.topi.transform.one_hot:13
msgid "Axis to fill."
msgstr ""

#: of tvm.topi.transform.one_hot:15
msgid "Data type of the output tensor."
msgstr ""

#: of tvm.topi.transform.one_hot:18
msgid "**ret** -- The one-hot tensor."
msgstr ""

#: of tvm.topi.reduction.prod:5
msgid ""
"Axis or axes along which a prod operation is performed. The default, "
"axis=None, will get the prod element over all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.topi.transform.repeat:3
msgid "The tensor to be repeated."
msgstr ""

#: of tvm.topi.transform.repeat:5
msgid "Number of repetitions for each element"
msgstr ""

#: of tvm.topi.transform.repeat:7
msgid "The axis along which to repeat values"
msgstr ""

#: of tvm.topi.transform.reshape:3
msgid "The tensor to be reshaped"
msgstr ""

#: of tvm.topi.transform.reshape:5
msgid "The new shape"
msgstr ""

#: of tvm.topi.transform.reverse_sequence:1
msgid ""
"Reverse the tensor for variable length slices. Input is first sliced "
"along batch axis and then elements are reversed along seq axis."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:4
msgid "The tensor to be reversed."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:6
msgid ""
"A 1D Tensor with length a.dims[batch_axis] Must be one of the following "
"types: int32, int64 if seq_lengths[i] > a.dims[seq_axis], it is rounded "
"to a.dims[seq_axis] if seq_lengths[i] < 1, it is rounded to 1"
msgstr ""

#: of tvm.topi.transform.reverse_sequence:11
msgid "The axis along which the elements will be reversed. Default is 1."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:13
msgid "The axis along which the tensor will be sliced. Default is 0."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:16
msgid "**ret** -- The computed result of same shape and type as of input."
msgstr ""

#: of tvm.topi.scatter.scatter:5 tvm.topi.scatter_add.scatter_add:5
msgid "The index locations to update."
msgstr ""

#: of tvm.topi.scatter.scatter:7 tvm.topi.scatter_add.scatter_add:7
msgid "The values to update."
msgstr ""

#: of tvm.topi.scatter.scatter:9
msgid "The axis to scatter on"
msgstr ""

#: of tvm.topi.scatter.scatter:12 tvm.topi.scatter_add.scatter_add:12
msgid "**ret** -- The computed result."
msgstr ""

#: of tvm.topi.scatter_add.scatter_add:9
msgid "The axis to scatter_add on"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:3
msgid ""
"Given data with shape (Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1}), indices "
"with shape (M, Y_0, ..., Y_{K-1}), and output with shape (X_0, X_1, ..., "
"X_{N-1}), scatter_nd computes"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:16
msgid "all other entries in the output are 0. Repeated indices are summed."
msgstr ""

#: of tvm.topi.scatter.scatter_nd:22
msgid "The output shape. This must be specified because it cannot be inferred."
msgstr ""

#: of tvm.topi.transform.sequence_mask:3
msgid ""
"This function takes an n-dimensional input array of the form [MAX_LENGTH,"
" batch_size, ...] or [batch_size, MAX_LENGTH, ...] and returns an array "
"of the same shape."
msgstr ""

#: of tvm.topi.transform.sequence_mask:6
msgid ""
"`axis` means the axis of the length dimension and can only be 0 or 1. If "
"`axis` is 0, the data must have shape [MAX_LENGTH, batch_size, ...]. "
"Otherwise (axis=1), the data must have shape [batch_size, MAX_LENGTH, "
"...]."
msgstr ""

#: of tvm.topi.transform.sequence_mask:10
msgid ""
"`valid_length` gives the length of each sequence. `valid_length` should "
"be a 1D int array with positive ints and has dimension [batch_size,]."
msgstr ""

#: of tvm.topi.transform.sequence_mask:13
msgid ""
"N-D with shape [MAX_LENGTH, batch_size, ...] or [batch_size, MAX_LENGTH, "
"...] depending on the value of `axis`."
msgstr ""

#: of tvm.topi.transform.sequence_mask:16
msgid "1-D with shape [batch_size,]"
msgstr ""

#: of tvm.topi.transform.sequence_mask:18
msgid "The masking value, default 0"
msgstr ""

#: of tvm.topi.transform.sequence_mask:20
msgid "axis of the length dimension, must be 0 or 1, default 0"
msgstr ""

#: of tvm.topi.transform.sequence_mask:23
msgid ""
"**output** -- N-D with shape [MAX_LENGTH, batch_size, ...] or "
"[batch_size, MAX_LENGTH, ...] depending on the value of `axis`."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:3
msgid ""
"Example:: -   sparse_to_dense([[0, 0], [1, 1]], [2, 2], [3, 3], 0) = [[3,"
" 0], [0, 3]]"
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:6
msgid ""
"A 0-D, 1-D, or 2-D tensor of integers containing location of sparse "
"values."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:8
msgid "Shape of the dense output tensor."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:10
msgid "A 0-D or 1-D tensor containing the sparse values for the sparse indices."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:12
msgid ""
"A 0-D tensor containing the default value for the remaining locations. "
"Defaults to 0."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:16
msgid ""
"**result** -- Dense tensor of shape output_shape. Has the same type as "
"sparse_values."
msgstr ""

#: of tvm.topi.transform.squeeze:5
msgid ""
"Selects a subset of the single-dimensional entries in the shape. If an "
"axis is selected with shape entry greater than one, an error is raised."
msgstr ""

#: of tvm.topi.transform.squeeze:9
msgid "**squeezed**"
msgstr ""

#: of tvm.topi.transform.stack:3
msgid "The tensor to be stacked."
msgstr ""

#: of tvm.topi.transform.stack:5
msgid "The axis in the result array along which the input arrays are stacked."
msgstr ""

#: of tvm.topi.transform.strided_set:3 tvm.topi.transform.strided_slice:3
msgid "The tensor to be sliced."
msgstr ""

#: of tvm.topi.transform.strided_set:5
msgid "The values to set"
msgstr ""

#: of tvm.topi.transform.strided_set:7 tvm.topi.transform.strided_slice:5
msgid "The indices to begin with in the slicing."
msgstr ""

#: of tvm.topi.transform.strided_set:9 tvm.topi.transform.strided_slice:7
msgid "Indicies indicating end of the slice."
msgstr ""

#: of tvm.topi.transform.strided_set:11 tvm.topi.transform.strided_slice:9
msgid ""
"Specifies the stride values, it can be negative in that case, the input "
"tensor will be reversed in that particular axis."
msgstr ""

#: of tvm.topi.transform.strided_slice:13
msgid ""
"The slice mode [end, size]. end - The ending indices for the slice "
"[default]. size - The input strides will be ignored, input end in this "
"mode indicates the sizeof a slice starting at the location specified by "
"begin. If end[i] is -1, all remaining elements in that dimension are "
"included in the slice."
msgstr ""

#: of tvm.topi.reduction.sum:5
msgid ""
"Axis or axes along which a sum is performed. The default, axis=None, will"
" sum all of the elements of the input array. If axis is negative it "
"counts from the last to the first axis."
msgstr ""

#: of tvm.topi.transform.take:7
msgid ""
"The axis over which to select values. By default, the flattened input "
"array is used."
msgstr ""

#: of tvm.topi.transform.take:10
msgid ""
"Specifies how out-of-bound indices will behave. clip - clip to the range "
"(default) wrap - wrap around the indices fast - no clip or wrap around "
"(user must make sure indices are in-bound)"
msgstr ""

#: of tvm.topi.transform.take_legalize:3
msgid "Attributes of current op"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:5
#: tvm.topi.nn.conv2d.conv2d_legalize:5
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:5
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:5
#: tvm.topi.transform.take_legalize:5
msgid "The args of the Relay expr to be legalized"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:7
#: tvm.topi.nn.conv2d.conv2d_legalize:7
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:7
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:7
#: tvm.topi.transform.take_legalize:7
msgid "List of input and output types"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:10
#: tvm.topi.nn.conv2d.conv2d_legalize:10
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:10
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:10
#: tvm.topi.transform.take_legalize:10
msgid "**result** -- The legalized expr"
msgstr ""

#: of tvm.topi.transform.tile:3
msgid "The tensor to be tiled."
msgstr ""

#: of tvm.topi.transform.tile:5
msgid "The number of times for repeating the tensor"
msgstr ""

#: of tvm.topi.sort.topk:5
msgid "Number of top elements to select. Return all elements if k < 1."
msgstr ""

#: of tvm.topi.sort.topk:7
msgid "Axis long which to sort the input tensor."
msgstr ""

#: of tvm.topi.sort.topk:9
msgid ""
"The return type [both, values, indices]. \"both\": return both top k data"
" and indices. \"values\": return top k data only. \"indices\": return top"
" k indices only."
msgstr ""

#: of tvm.topi.sort.topk:16
msgid "The data type of the indices output."
msgstr ""

#: of tvm.topi.sort.topk:19
msgid "**out** -- The computed result."
msgstr ""

#: of tvm.topi.transform.transpose:5
msgid "By default, reverse the dimensions."
msgstr ""

#: of tvm.topi.transform.unravel_index:3
msgid "Example:: -   unravel_index([22, 41, 37], [7, 6]) = [[3, 6, 6], [4, 5, 1]]"
msgstr ""

#: of tvm.topi.transform.unravel_index:6
msgid "An integer array containing indices."
msgstr ""

#: of tvm.topi.transform.unravel_index:8
msgid "The shape of the array."
msgstr ""

#: of tvm.topi.transform.unravel_index:11
msgid "**result** -- The tuple of coordinate arrays."
msgstr ""

#: of tvm.topi.transform.where:3
msgid "The condition array."
msgstr ""

#: of tvm.topi.transform.where:5
msgid "First array to be selected."
msgstr ""

#: of tvm.topi.transform.where:7
msgid "Second array to be selected."
msgstr ""

#: of tvm.topi.transform.where:10
msgid "**result** -- A Tensor selected from x or y depending on condition."
msgstr ""

#: of tvm.topi.utils.within_index:12
msgid ""
"**selected** -- bool expression that is True is the array position would "
"be selected by the index and False otherwise"
msgstr ""

#: of tvm.topi.utils.InvalidShapeError:1
msgid ""
"Invalid shape for a topi function. i.e. call winograd template for non-"
"3x3 kernel)"
msgstr ""

#: ../../api/python/topi.rst:26
msgid "tvm.topi.nn"
msgstr ""

#: of tvm.topi.nn:1
msgid "Neural network operators"
msgstr ""

#: of tvm.topi.nn:1:<autosummary>:1
msgid ""
":obj:`Workload <tvm.topi.nn.tvm.topi.nn.Workload>`\\ \\(in\\_dtype\\, "
"out\\_dtype\\, height\\, width\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`adaptive_pool <tvm.topi.nn.tvm.topi.nn.adaptive_pool>`\\ \\(data\\,"
" output\\_size\\, pool\\_type\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.adaptive_pool:9 tvm.topi.nn.pooling.pool:7
msgid "Perform pooling on height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`adaptive_pool3d <tvm.topi.nn.tvm.topi.nn.adaptive_pool3d>`\\ "
"\\(data\\, output\\_size\\, pool\\_type\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Perform pooling on three dimensional data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`batch_matmul <tvm.topi.nn.tvm.topi.nn.batch_matmul>`\\ \\(x\\, "
"y\\[\\, oshape\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Computes batch matrix multiplication of `x` and `y` when `x` and `y` are "
"data in batch."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`batch_to_space_nd <tvm.topi.nn.tvm.topi.nn.batch_to_space_nd>`\\ "
"\\(data\\, block\\_shape\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Perform space to batch transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`binarize_pack <tvm.topi.nn.tvm.topi.nn.binarize_pack>`\\ "
"\\(data\\[\\, axis\\, name\\]\\)"
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Binarization and bit-packing along a certain axis."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`binary_dense <tvm.topi.nn.tvm.topi.nn.binary_dense>`\\ \\(data\\, "
"weight\\)"
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Binary matrix multiplication using xor and bit-count."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`bitpack <tvm.topi.nn.tvm.topi.nn.bitpack>`\\ \\(data\\, bits\\, "
"pack\\_axis\\, bit\\_axis\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Packs data into format necessary for bitserial computation"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`bitserial_conv2d_legalize "
"<tvm.topi.nn.tvm.topi.nn.bitserial_conv2d_legalize>`\\ \\(attrs\\, "
"inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Bitserial Conv2D op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`bitserial_conv2d_nchw "
"<tvm.topi.nn.tvm.topi.nn.bitserial_conv2d_nchw>`\\ \\(data\\, kernel\\, "
"stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Bitserial Conv2D operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`bitserial_conv2d_nhwc "
"<tvm.topi.nn.tvm.topi.nn.bitserial_conv2d_nhwc>`\\ \\(data\\, kernel\\, "
"stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`bitserial_dense <tvm.topi.nn.tvm.topi.nn.bitserial_dense>`\\ "
"\\(data\\, weight\\, data\\_bits\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "The default implementation of bitserial dense in topi."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`concatenate <tvm.topi.nn.tvm.topi.nn.concatenate>`\\ "
"\\(a\\_tuple\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv1d <tvm.topi.nn.tvm.topi.nn.conv1d>`\\ \\(data\\, kernel\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv1d_ncw <tvm.topi.nn.tvm.topi.nn.conv1d_ncw>`\\ \\(data\\, "
"kernel\\[\\, strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_ncw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution forward operator for NCW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv1d_nwc <tvm.topi.nn.tvm.topi.nn.conv1d_nwc>`\\ \\(data\\, "
"kernel\\[\\, strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_nwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution forward operator for NWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv1d_transpose_ncw "
"<tvm.topi.nn.tvm.topi.nn.conv1d_transpose_ncw>`\\ \\(data\\, kernel\\, "
"stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Transposed 1D convolution ncw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d <tvm.topi.nn.tvm.topi.nn.conv2d>`\\ \\(input\\, filter\\, "
"strides\\, padding\\, dilation\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_NCHWc <tvm.topi.nn.tvm.topi.nn.conv2d_NCHWc>`\\ \\(data\\, "
"kernel\\, stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:1 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D operator for nChw[x]c layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_NCHWc_int8 <tvm.topi.nn.tvm.topi.nn.conv2d_NCHWc_int8>`\\ "
"\\(data\\, kernel\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_alter_layout "
"<tvm.topi.nn.tvm.topi.nn.conv2d_alter_layout>`\\ \\(attrs\\, inputs\\, "
"tinfos\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Change Conv2D layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_gemm_weight_transform "
"<tvm.topi.nn.tvm.topi.nn.conv2d_gemm_weight_transform>`\\ \\(kernel\\, "
"...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:1
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:1
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Weight transformation for winograd"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_hwcn <tvm.topi.nn.tvm.topi.nn.conv2d_hwcn>`\\ \\(Input\\, "
"Filter\\, stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in HWCN layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_infer_layout "
"<tvm.topi.nn.tvm.topi.nn.conv2d_infer_layout>`\\ \\(workload\\, cfg\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:1
msgid "Infer input/output shapes and layouts from a workload and cfg."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_legalize <tvm.topi.nn.tvm.topi.nn.conv2d_legalize>`\\ "
"\\(attrs\\, inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Conv2D op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_nchw <tvm.topi.nn.tvm.topi.nn.conv2d_nchw>`\\ \\(Input\\, "
"Filter\\, stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_nhwc <tvm.topi.nn.tvm.topi.nn.conv2d_nhwc>`\\ \\(Input\\, "
"Filter\\, stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_transpose_legalize "
"<tvm.topi.nn.tvm.topi.nn.conv2d_transpose_legalize>`\\ \\(attrs\\, "
"inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Transposed 2D convolution op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_transpose_nchw "
"<tvm.topi.nn.tvm.topi.nn.conv2d_transpose_nchw>`\\ \\(Input\\, Filter\\, "
"...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Transposed 2D convolution nchw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_transpose_nchw_preprocess "
"<tvm.topi.nn.tvm.topi.nn.conv2d_transpose_nchw_preprocess>`\\ \\(data\\, "
"...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw_preprocess:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Preprocess data and kernel to make the compute pattern of "
"conv2d_transpose the same as conv2d"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_winograd_nhwc "
"<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_nhwc>`\\ \\(data\\, weight\\, "
"strides\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D Winograd in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_winograd_nhwc_without_weight_transform "
"<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_nhwc_without_weight_transform>`\\"
" \\(...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D Winograd without layout transform in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_winograd_nnpack_weight_transform "
"<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_nnpack_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv2d_winograd_weight_transform "
"<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_weight_transform>`\\ "
"\\(kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv3d_alter_layout "
"<tvm.topi.nn.tvm.topi.nn.conv3d_alter_layout>`\\ \\(attrs\\, inputs\\, "
"tinfos\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_alter_layout:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Change Conv3D layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv3d_ncdhw <tvm.topi.nn.tvm.topi.nn.conv3d_ncdhw>`\\ \\(Input\\, "
"Filter\\, stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv3D operator in NCDHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv3d_ndhwc <tvm.topi.nn.tvm.topi.nn.conv3d_ndhwc>`\\ \\(Input\\, "
"Filter\\, stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in NDHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv3d_transpose_legalize "
"<tvm.topi.nn.tvm.topi.nn.conv3d_transpose_legalize>`\\ \\(attrs\\, "
"inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Transposed 3D convolution op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv3d_transpose_ncdhw "
"<tvm.topi.nn.tvm.topi.nn.conv3d_transpose_ncdhw>`\\ \\(Input\\, Filter\\,"
" ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Transposed 3D convolution ncdhw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv3d_transpose_ncdhw_preprocess "
"<tvm.topi.nn.tvm.topi.nn.conv3d_transpose_ncdhw_preprocess>`\\ \\(data\\,"
" ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw_preprocess:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Preprocess data and kernel to make the compute pattern of "
"conv3d_transpose the same as conv3d"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`conv3d_winograd_weight_transform "
"<tvm.topi.nn.tvm.topi.nn.conv3d_winograd_weight_transform>`\\ "
"\\(kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Weight transformation for 3D winograd"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`correlation_nchw <tvm.topi.nn.tvm.topi.nn.correlation_nchw>`\\ "
"\\(data1\\, data2\\, kernel\\_size\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Correlation operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`declaration_conv2d_transpose_impl "
"<tvm.topi.nn.tvm.topi.nn.declaration_conv2d_transpose_impl>`\\ \\(data\\,"
" ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.declaration_conv2d_transpose_impl:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Implementation of conv2d transpose"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`declaration_conv3d_transpose_impl "
"<tvm.topi.nn.tvm.topi.nn.declaration_conv3d_transpose_impl>`\\ \\(data\\,"
" ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.declaration_conv3d_transpose_impl:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Implementation of conv3d transpose"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`deformable_conv2d_nchw "
"<tvm.topi.nn.tvm.topi.nn.deformable_conv2d_nchw>`\\ \\(data\\, offset\\, "
"kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Deformable conv2D operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`deformable_conv2d_nhwc "
"<tvm.topi.nn.tvm.topi.nn.deformable_conv2d_nhwc>`\\ \\(data\\, offset\\, "
"kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Deformable conv2D operator in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`dense <tvm.topi.nn.tvm.topi.nn.dense>`\\ \\(data\\, weight\\[\\, "
"bias\\, out\\_dtype\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.dense.dense:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "The default implementation of dense in topi."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`depth_to_space <tvm.topi.nn.tvm.topi.nn.depth_to_space>`\\ "
"\\(data\\, block\\_size\\[\\, layout\\, mode\\]\\)"
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Perform depth to space transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`depthwise_conv2d_NCHWc "
"<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_NCHWc>`\\ \\(Input\\, Filter\\,"
" ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:1
msgid "Depthwise convolution NCHW[x]c forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`depthwise_conv2d_backward_input_nhwc "
"<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_backward_input_nhwc>`\\ "
"\\(Filter\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:1
msgid "Depthwise convolution nhwc backward wrt input operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`depthwise_conv2d_backward_weight_nhwc "
"<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_backward_weight_nhwc>`\\ "
"\\(Input\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:1
msgid "Depthwise convolution nhwc backward wrt weight operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`depthwise_conv2d_infer_layout "
"<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_infer_layout>`\\ \\(workload\\,"
" cfg\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`depthwise_conv2d_nchw "
"<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_nchw>`\\ \\(Input\\, Filter\\, "
"stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:1
msgid "Depthwise convolution nchw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`depthwise_conv2d_nhwc "
"<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_nhwc>`\\ \\(Input\\, Filter\\, "
"stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:1
msgid "Depthwise convolution nhwc forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`dilate <tvm.topi.nn.tvm.topi.nn.dilate>`\\ \\(data\\, strides\\[\\,"
" dilation\\_value\\, name\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.dilate.dilate:1
msgid "Dilate data with given dilation value (0 by default)."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`equal_const_int <tvm.topi.nn.tvm.topi.nn.equal_const_int>`\\ "
"\\(expr\\, value\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.equal_const_int:1
msgid "Returns if expr equals value."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`fast_softmax <tvm.topi.nn.tvm.topi.nn.fast_softmax>`\\ \\(x\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.softmax.softmax:1
msgid "Perform softmax activation on the data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`fifo_buffer <tvm.topi.nn.tvm.topi.nn.fifo_buffer>`\\ \\(data\\, "
"buffer\\, axis\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.fifo_buffer.fifo_buffer:1
msgid "FIFO buffer to enable computation reuse in CNNs with sliding indow input"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":obj:`flatten <tvm.topi.nn.tvm.topi.nn.flatten>`\\ \\(data\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.flatten.flatten:1
msgid ""
"Flattens the input array into a 2-D array by collapsing the higher "
"dimensions."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":obj:`get_const_int <tvm.topi.nn.tvm.topi.nn.get_const_int>`\\ \\(expr\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.get_const_int:1
msgid "Verifies expr is integer and get the constant value."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`get_const_tuple <tvm.topi.nn.tvm.topi.nn.get_const_tuple>`\\ "
"\\(in\\_tuple\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`get_pad_tuple <tvm.topi.nn.tvm.topi.nn.get_pad_tuple>`\\ "
"\\(padding\\, kernel\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.utils.get_pad_tuple:1 tvm.topi.nn.utils.get_pad_tuple1d:1
#: tvm.topi.nn.utils.get_pad_tuple3d:1
msgid "Common code to get the pad option"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`get_pad_tuple1d <tvm.topi.nn.tvm.topi.nn.get_pad_tuple1d>`\\ "
"\\(padding\\, kernel\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`get_pad_tuple3d <tvm.topi.nn.tvm.topi.nn.get_pad_tuple3d>`\\ "
"\\(padding\\, kernel\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`global_pool <tvm.topi.nn.tvm.topi.nn.global_pool>`\\ \\(data\\, "
"pool\\_type\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.global_pool:7
msgid "Perform global pooling on height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`group_conv2d_nchw <tvm.topi.nn.tvm.topi.nn.group_conv2d_nchw>`\\ "
"\\(Input\\, Filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Group convolution operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`group_conv2d_nhwc <tvm.topi.nn.tvm.topi.nn.group_conv2d_nhwc>`\\ "
"\\(Input\\, Filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Group convolution operator in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":obj:`leaky_relu <tvm.topi.nn.tvm.topi.nn.leaky_relu>`\\ \\(x\\, alpha\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.elemwise.leaky_relu:1
msgid "Take leaky relu of input x."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":obj:`log_softmax <tvm.topi.nn.tvm.topi.nn.log_softmax>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.softmax.log_softmax:1
msgid "Perform log softmax activation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`lrn <tvm.topi.nn.tvm.topi.nn.lrn>`\\ \\(data\\, size\\[\\, axis\\, "
"alpha\\, beta\\, bias\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.local_response_norm.lrn:1
msgid ""
"Perform the across channels local response normalisation on the input "
"data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`mirror_pad <tvm.topi.nn.tvm.topi.nn.mirror_pad>`\\ \\(data\\, "
"pad\\_before\\[\\, pad\\_after\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pad.mirror_pad:1
msgid "Pad Input with mirroring either symmetric or reflected."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`namedtuple <tvm.topi.nn.tvm.topi.nn.namedtuple>`\\ \\(typename\\, "
"field\\_names\\, \\*\\[\\, ...\\]\\)"
msgstr ""

#: collections.namedtuple:1 of
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Returns a new subclass of tuple with named fields."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`pad <tvm.topi.nn.tvm.topi.nn.pad>`\\ \\(data\\, pad\\_before\\[\\, "
"pad\\_after\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pad.pad:1
msgid "Pad Input with zeros."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`pool <tvm.topi.nn.tvm.topi.nn.pool>`\\ \\(data\\, kernel\\, "
"stride\\, padding\\, pool\\_type\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`pool1d <tvm.topi.nn.tvm.topi.nn.pool1d>`\\ \\(data\\, kernel\\, "
"stride\\, padding\\, pool\\_type\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.pool1d:7
msgid "Perform pooling on width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`pool3d <tvm.topi.nn.tvm.topi.nn.pool3d>`\\ \\(data\\, kernel\\, "
"stride\\, padding\\, pool\\_type\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.pool3d:7
msgid "Perform pooling on depth, height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`pool_grad <tvm.topi.nn.tvm.topi.nn.pool_grad>`\\ \\(grads\\, "
"data\\, kernel\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.pool_grad:7
msgid "Gradient of pooling on height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`prelu <tvm.topi.nn.tvm.topi.nn.prelu>`\\ \\(x\\, slope\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "PReLU."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":obj:`relu <tvm.topi.nn.tvm.topi.nn.relu>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.elemwise.relu:1
msgid "Take relu of input x."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`scale_shift_nchw <tvm.topi.nn.tvm.topi.nn.scale_shift_nchw>`\\ "
"\\(Input\\, Scale\\, Shift\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.mapping.scale_shift_nchw:1
#: tvm.topi.nn.mapping.scale_shift_nhwc:1
msgid "Batch normalization operator in inference."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`scale_shift_nhwc <tvm.topi.nn.tvm.topi.nn.scale_shift_nhwc>`\\ "
"\\(Input\\, Scale\\, Shift\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":obj:`simplify <tvm.topi.nn.tvm.topi.nn.simplify>`\\ \\(expr\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.simplify:1
msgid "Simplify the expression if it is Expr, directly return if it is int."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":obj:`softmax <tvm.topi.nn.tvm.topi.nn.softmax>`\\ \\(x\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`softmax_common <tvm.topi.nn.tvm.topi.nn.softmax_common>`\\ \\(x\\, "
"axis\\, use\\_fast\\_exp\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.softmax.softmax_common:1
msgid "The common part of softmax and fast_softmax"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`space_to_batch_nd <tvm.topi.nn.tvm.topi.nn.space_to_batch_nd>`\\ "
"\\(data\\, block\\_shape\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:1
msgid "Perform batch to space transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`space_to_depth <tvm.topi.nn.tvm.topi.nn.space_to_depth>`\\ "
"\\(data\\, block\\_size\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.space_to_depth.space_to_depth:1
msgid "Perform space to depth transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`sparse_dense <tvm.topi.nn.tvm.topi.nn.sparse_dense>`\\ "
"\\(dense\\_data\\, sparse\\_data\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense:1
msgid ""
"Computes sparse-dense matrix multiplication of `data` and `(weight_data, "
"weight_indices, weight_indptr).T`, if sparse_lhs=False or Computes "
"sparse-dense matrix multiplication of `(data_data, data_indices, "
"data_indptr)` and `weight.T`, if sparse_lhs=True"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`sparse_dense_alter_layout "
"<tvm.topi.nn.tvm.topi.nn.sparse_dense_alter_layout>`\\ \\(\\_attrs\\, "
"\\_inputs\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:1
msgid "Change Sparse Dense layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`sparse_dense_v1 <tvm.topi.nn.tvm.topi.nn.sparse_dense_v1>`\\ "
"\\(data\\_data\\, data\\_indices\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense_v1:1
msgid ""
"Computes sparse-dense matrix multiplication of `(data_data, data_indices,"
" data_indptr)` and `weight.T`"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`sparse_dense_v2 <tvm.topi.nn.tvm.topi.nn.sparse_dense_v2>`\\ "
"\\(data\\, weight\\_data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense_v2:1
msgid ""
"Computes sparse-dense matrix multiplication of `data` and `(weight_data, "
"weight_indices, weight_indptr).T`"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`sparse_transpose <tvm.topi.nn.tvm.topi.nn.sparse_transpose>`\\ "
"\\(sparse\\_data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Transpose a square sparse matrix, `A` is an n-by-n sparse matrix in the "
"CSR format."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`strided_slice <tvm.topi.nn.tvm.topi.nn.strided_slice>`\\ \\(a\\, "
"begin\\, end\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`unpack_NCHWc_to_nchw "
"<tvm.topi.nn.tvm.topi.nn.unpack_NCHWc_to_nchw>`\\ \\(packed\\_out\\, "
"out\\_dtype\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Unpack conv2d_NCHWc output from layout NCHWc to NCHW"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`upsampling <tvm.topi.nn.tvm.topi.nn.upsampling>`\\ \\(data\\, "
"scale\\_h\\, scale\\_w\\[\\, layout\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.upsampling.upsampling:2 tvm.topi.nn.upsampling.upsampling3d:2
msgid "Perform upsampling on the data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`upsampling3d <tvm.topi.nn.tvm.topi.nn.upsampling3d>`\\ \\(data\\, "
"scale\\_d\\, scale\\_h\\, scale\\_w\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":obj:`winograd_transform_matrices "
"<tvm.topi.nn.tvm.topi.nn.winograd_transform_matrices>`\\ "
"\\(tile\\_size\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.winograd_util.winograd_transform_matrices:1
msgid ""
"Compute the A, B, and G transform matrices for `tile_size` as a "
"`tvm.Expr`."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1
msgid "**Attributes:**"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`dilation_h <tvm.topi.nn.tvm.topi.nn.Workload.dilation_h>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1
#: tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid "Alias for field number 12"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`dilation_w <tvm.topi.nn.tvm.topi.nn.Workload.dilation_w>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.dilation_w:1
msgid "Alias for field number 13"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`height <tvm.topi.nn.tvm.topi.nn.Workload.height>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.height:1
msgid "Alias for field number 2"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`in_dtype <tvm.topi.nn.tvm.topi.nn.Workload.in_dtype>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.in_dtype:1
msgid "Alias for field number 0"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`in_filter <tvm.topi.nn.tvm.topi.nn.Workload.in_filter>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.in_filter:1
msgid "Alias for field number 4"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`kernel_h <tvm.topi.nn.tvm.topi.nn.Workload.kernel_h>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.kernel_h:1
msgid "Alias for field number 6"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`kernel_w <tvm.topi.nn.tvm.topi.nn.Workload.kernel_w>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.kernel_w:1
msgid "Alias for field number 7"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`out_dtype <tvm.topi.nn.tvm.topi.nn.Workload.out_dtype>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.out_dtype:1
msgid "Alias for field number 1"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`out_filter <tvm.topi.nn.tvm.topi.nn.Workload.out_filter>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.out_filter:1
msgid "Alias for field number 5"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`padb <tvm.topi.nn.tvm.topi.nn.Workload.padb>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padb:1
msgid "Alias for field number 10"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`padl <tvm.topi.nn.tvm.topi.nn.Workload.padl>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padl:1
msgid "Alias for field number 9"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`padr <tvm.topi.nn.tvm.topi.nn.Workload.padr>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padr:1
msgid "Alias for field number 11"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`padt <tvm.topi.nn.tvm.topi.nn.Workload.padt>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padt:1
msgid "Alias for field number 8"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`stride_h <tvm.topi.nn.tvm.topi.nn.Workload.stride_h>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.stride_h:1
msgid "Alias for field number 14"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`stride_w <tvm.topi.nn.tvm.topi.nn.Workload.stride_w>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.stride_w:1
msgid "Alias for field number 15"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":obj:`width <tvm.topi.nn.tvm.topi.nn.Workload.width>`\\"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.width:1
msgid "Alias for field number 3"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:2
msgid ""
"The pooling kernel and stride sizes are automatically chosen for desired "
"output sizes. It decides the height and width dimension according to the "
"layout string, in which 'W' and 'H' means width and height respectively. "
"Width and height dimension cannot be split. For example, NCHW, NCHW16c, "
"etc. are valid for pool, while NCHW16w, NCHW16h are not. See parameter "
"`layout` for more information of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:11 tvm.topi.nn.pooling.global_pool:9
#: tvm.topi.nn.pooling.pool:9 tvm.topi.nn.pooling.pool1d:9
#: tvm.topi.nn.pooling.pool3d:9 tvm.topi.nn.pooling.pool_grad:9
#: tvm.topi.nn.pooling.pool_grad:11
msgid "n-D with shape of layout"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:13
msgid "output height and width."
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:15 tvm.topi.nn.pooling.global_pool:11
#: tvm.topi.nn.pooling.pool:17 tvm.topi.nn.pooling.pool1d:17
#: tvm.topi.nn.pooling.pool3d:17 tvm.topi.nn.pooling.pool_grad:19
msgid "Pool type, 'max' or 'avg'"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:17 tvm.topi.nn.pooling.global_pool:13
#: tvm.topi.nn.pooling.pool:21 tvm.topi.nn.pooling.pool_grad:23
msgid ""
"Layout of the input data. The layout is supposed to be composed of upper "
"cases, lower cases and numbers, where upper case indicates a dimension "
"and the corresponding lower case with factor size indicates the split "
"dimension. For example, NCHW16c can describe a 5-D tensor of [batch_size,"
" channel, height, width, channel_block], in which channel_block=16 is a "
"split of dimension channel."
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:26 tvm.topi.nn.pooling.pool:32
#: tvm.topi.nn.pooling.pool1d:32 tvm.topi.nn.pooling.pool3d:32
#: tvm.topi.nn.pooling.pool_grad:34
msgid "**output** -- n-D in the same layout"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool3d:1
msgid ""
"Perform pooling on three dimensional data. See the two dimensional "
"version above for details."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:1
msgid ""
"Computes batch matrix multiplication of `x` and `y` when `x` and `y` are "
"data in batch. Supports broadcasting for batch dimension."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:4
msgid "3-D with shape [batch, M, K]"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:6
msgid "3-D with shape [batch, N, K]"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:8
msgid ""
"Explicit intended output shape of the computation. Can be useful in cases"
" with dynamic input shapes."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:11
#: tvm.topi.nn.conv2d.conv2d_nhwc:17 tvm.topi.nn.conv2d.conv2d_winograd_nhwc:18
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:16
#: tvm.topi.nn.conv3d.conv3d_ndhwc:15 tvm.topi.nn.dense.dense:11
msgid "The layout after auto-scheduler's layout rewrite pass."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:14
msgid "**output** -- 3-D with shape [batch, M, N]"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:3
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:3
msgid ""
"N-D Tensor with shape [batch, spatial_shape, remaining_shapes], where "
"spatial_shape has M dimensions."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:6
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:6
msgid ""
"list of size [M] where M is number of spatial dims, specifies block size "
"for each spatial dimension."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:9
msgid ""
"list of shape [M] where M is number of spatial dims, specifies begin crop"
" size for each spatial dimension."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:12
msgid ""
"list of shape [M] where M is number of spatial dims, specifies end crop "
"size for each spatial dimension."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:16
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:18
msgid "**output**"
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:3 tvm.topi.nn.pad.mirror_pad:3
#: tvm.topi.nn.pad.pad:3
msgid "n-D input, can be any layout."
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:5
msgid ""
"The axis along which to do binarization and bit-packing, default is the "
"last axis."
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:8
msgid "The name prefix operators generate."
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:11
msgid "**output** -- n-D, the same layout as input, dtype is uint32."
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:3
msgid "2-D with shape [batch, in_dim], dtype is uint32."
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:5
msgid "2-D with shape [out_dim, in_dim], dtype is uint32."
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:8
msgid "**output** -- 2-D with shape [batch, out_dim], dtype is float32."
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:3
msgid "index of the axis to pack in data"
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:5
msgid "index of axis to place bit axis in resulting packed data"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:3
#: tvm.topi.nn.conv2d.conv2d_alter_layout:3
#: tvm.topi.nn.conv2d.conv2d_legalize:3
#: tvm.topi.nn.conv3d.conv3d_alter_layout:3
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:6
msgid "Attributes of current convolution"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:3
#: tvm.topi.image.grid_sample.grid_sample:20
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:3
#: tvm.topi.nn.conv2d.conv2d:3 tvm.topi.nn.conv2d.conv2d_nchw:3
#: tvm.topi.nn.conv2d.group_conv2d_nchw:3
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:3
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:5
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:3
msgid "4-D with shape [batch, in_channel, in_height, in_width]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:5
#: tvm.topi.nn.conv2d.conv2d:5 tvm.topi.nn.conv2d.conv2d_nchw:5
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:10
msgid "4-D with shape [num_filter, in_channel, filter_height, filter_width]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:7
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:7
#: tvm.topi.nn.conv2d.conv2d:7 tvm.topi.nn.conv2d.conv2d_NCHWc:9
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:9
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:8
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:8
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:12
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:13
msgid "stride size, or [stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:9
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:9
msgid ""
"padding size, [pad_height, pad_width], [pad_top, pad_left, pad_down, "
"pad_right]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:11
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:11
msgid "number of bits used for activations/input elements"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:13
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:13
msgid "number of bits used for weight elements"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:15
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:15
msgid "return type of convolution"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:17
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:17
msgid "bit packing type"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:19
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:19
msgid ""
"if binarization style is in unipolar 1/0 format, instead of bipolar -1/+1"
" format"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:22
#: tvm.topi.nn.conv2d.conv2d:18
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:23
msgid "**output** -- 4-D with shape [batch, out_channel, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:3
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:3
#: tvm.topi.nn.conv2d.conv2d_nhwc:3 tvm.topi.nn.conv2d.conv2d_winograd_nhwc:4
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:4
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:3
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:5
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:3
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:3
msgid "4-D with shape [batch, in_height, in_width, in_channel]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:5
#: tvm.topi.nn.conv2d.conv2d_hwcn:5 tvm.topi.nn.conv2d.conv2d_nhwc:5
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:6
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:6
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:11
msgid "4-D with shape [filter_height, filter_width, in_channel, num_filter]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:22
#: tvm.topi.nn.conv2d.conv2d_nhwc:20 tvm.topi.nn.conv2d.conv2d_winograd_nhwc:21
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:19
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:24
msgid "**output** -- 4-D with shape [batch, out_height, out_width, out_channel]"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:3 tvm.topi.nn.dense.dense:3
#: tvm.topi.sparse.dense.dense:4
msgid "2-D with shape [batch, in_dim]"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:5
msgid ""
"2-D with shape [out_dim, in_dim] or 3-D with shape [out_dim, weight_bits,"
" in_dim]"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:9 tvm.topi.nn.dense.dense:14
#: tvm.topi.sparse.dense.dense:11
msgid "**output** -- 2-D with shape [batch, out_dim]"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:3
msgid ""
"3-D input shape [batch, in_channel, in_width] for layout == 'NCW' and "
"[batch, in_width, in_channel] for layout == 'NWC'"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:6
msgid ""
"3-D kernel with shape [num_filter, in_channel, filter_size] for layout =="
" 'NCW' and [filter_size, in_channel, num_filter] for layout == 'NWC'"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:9 tvm.topi.nn.conv1d.conv1d_ncw:7
#: tvm.topi.nn.conv1d.conv1d_nwc:7
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:7
msgid "The spatial stride along width"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:11
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:9
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:9
#: tvm.topi.nn.conv3d.conv3d_ncdhw:9 tvm.topi.nn.conv3d.conv3d_ndhwc:9
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:11
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:9
#: tvm.topi.nn.utils.get_pad_tuple:3 tvm.topi.nn.utils.get_pad_tuple1d:3
#: tvm.topi.nn.utils.get_pad_tuple3d:3
msgid "Padding size, or ['VALID', 'SAME']"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:13 tvm.topi.nn.conv1d.conv1d_ncw:12
#: tvm.topi.nn.conv1d.conv1d_nwc:12
msgid "Dilation rate if convolution should be dilated."
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:15
msgid "How input data is laid out, must be one of ['NCW', 'NWC']"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:17 tvm.topi.nn.conv1d.conv1d_ncw:14
#: tvm.topi.nn.conv1d.conv1d_nwc:14
msgid "The output data type. If None then output is same type as input."
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_ncw:3
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:3
msgid "3-D with shape [batch, in_channel, in_width]"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_ncw:5
msgid "3-D with shape [num_filter, in_channel, filter_size]"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_ncw:9 tvm.topi.nn.conv1d.conv1d_nwc:9
msgid ""
"Padding size can be an integer for equal padding, a tuple of (left, "
"right) or a string in ['VALID', 'SAME']."
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_nwc:3
msgid "3-D with shape [batch, in_width, in_channel]"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_nwc:5
msgid "3-D with shape [filter_size, in_channel, num_filter]"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:5
msgid "3-D with shape [in_channel, num_filter, filter_width]"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:11
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:11
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:11
msgid "The output data type. This is used for mixed precision."
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:13
msgid ""
"Used to recover the actual output shape in case there are more than one "
"possible shape.  Must be smaller than stride."
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:17
msgid "**output** -- 3-D with shape [batch, out_channel, out_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:9 tvm.topi.nn.conv2d.conv2d_NCHWc:11
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:11 tvm.topi.nn.conv2d.conv2d_hwcn:9
#: tvm.topi.nn.conv2d.conv2d_nchw:9 tvm.topi.nn.conv2d.conv2d_nhwc:9
#: tvm.topi.nn.conv2d.group_conv2d_nchw:9
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:9
msgid ""
"padding size, or [pad_height, pad_width] for 2 ints, or [pad_top, "
"pad_left, pad_bottom, pad_right] for 4 ints"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:11
#: tvm.topi.image.dilation2d.dilation2d_nhwc:11 tvm.topi.nn.conv2d.conv2d:13
#: tvm.topi.nn.conv2d.conv2d_NCHWc:15 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:15
#: tvm.topi.nn.conv2d.conv2d_hwcn:13 tvm.topi.nn.conv2d.conv2d_nchw:13
#: tvm.topi.nn.conv2d.conv2d_nhwc:13 tvm.topi.nn.conv2d.conv2d_winograd_nhwc:12
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:12
#: tvm.topi.nn.conv2d.group_conv2d_nchw:13
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:13
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:16
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:17
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:13
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:11
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:11
msgid "dilation size, or [dilation_height, dilation_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:15
msgid "layout of data"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:3 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:3
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:3
msgid ""
"5-D with shape [batch, in_channel_chunk, in_height, in_width, "
"in_channel_block]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:5
msgid ""
"6-D with shape [num_filter_chunk, in_channel_chunk, filter_height, "
"filter_width, in_channel_block, num_filter_block]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:17
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:17
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:15
msgid "Input data layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:19
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:19
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:17
msgid "Output data layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:21
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:21
msgid "output data type"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:24
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:26
msgid ""
"**output** -- 5-D with shape [batch, out_channel_chunk, out_height, "
"out_width, out_channel_block]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc_int8:5
msgid ""
"7-D with shape [num_filter_chunk, in_channel_chunk, filter_height, "
"filter_width, in_channel_block/4, num_filter_block, 4]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc_int8:23
msgid "numer of int8 elements accumulated"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:5
#: tvm.topi.nn.conv3d.conv3d_alter_layout:5
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:8
msgid "Grouped input symbols"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:7
#: tvm.topi.nn.conv3d.conv3d_alter_layout:7
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:10
msgid "Input shape and dtype"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:9
#: tvm.topi.nn.conv3d.conv3d_alter_layout:9
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:12
msgid "The output type"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:12
#: tvm.topi.nn.conv3d.conv3d_alter_layout:12
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:15
msgid ""
"Unlike other TOPI functions, this function operates on both graph level "
"and operator level."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:3
msgid "The raw kernel tensor with layout \"NHWC\"."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:5
msgid "Tile rows of the weight transformation for ConvGemm."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:7
msgid "Tile columns of the weight transformation for ConvGemm."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:10
msgid "**output** -- 2-D with shape [CI*KH*KW,CO]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:3
msgid "4-D with shape [in_height, in_width, in_channel, batch]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:7
#: tvm.topi.image.dilation2d.dilation2d_nhwc:7 tvm.topi.nn.conv2d.conv2d_hwcn:7
#: tvm.topi.nn.conv2d.conv2d_nchw:7 tvm.topi.nn.conv2d.conv2d_nhwc:7
#: tvm.topi.nn.conv2d.group_conv2d_nchw:7
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:7
msgid "Stride size, or [stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:16
msgid "**output** -- 4-D with shape [out_height, out_width, out_channel, batch]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:3
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:3
msgid "conv2d workload"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:5
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:5
msgid "tvm.autotvm config"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:8
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:8
msgid "**Output** -- Input shapes and layouts, and output shapes and layouts"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nchw:16 tvm.topi.nn.conv2d.group_conv2d_nchw:20
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:16
#: tvm.topi.nn.correlation.correlation_nchw:22
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:16
msgid "**Output** -- 4-D with shape [batch, out_channel, out_height, out_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nhwc:15 tvm.topi.nn.conv3d.conv3d_ndhwc:13
msgid "The type of output tensor"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:3
msgid "Attributes of current Transposed 2D convolution"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:5
msgid "4-D with shape [in_channel, num_filter, filter_height, filter_width]"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:7
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:7
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:7
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:7
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:7
msgid "The spatial stride along height and width"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:13
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:13
msgid "Used to get the right output shape for gradients"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nhwc:1
msgid ""
"Conv2D Winograd in NHWC layout. This is a clean version to be used by the"
" auto-scheduler for both CPU and GPU."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nhwc:10
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:10
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:14
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:15
msgid "padding size, or [pad_height, pad_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:13
#: tvm.topi.image.dilation2d.dilation2d_nhwc:13
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:14
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:14
msgid "Specifies the output data type."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nhwc:16
msgid "Whether the kernel is precomputed"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:1
msgid ""
"Conv2D Winograd without layout transform in NHWC layout. This is a clean "
"version to be used by the auto-scheduler for both CPU and GPU."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:3
msgid ""
"The raw kernel tensor with layout \"NCHW\". Only 3x3 kernel is supported "
"for now."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:5
msgid "The convolution algorithm for Winograd NNPACK."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:8
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:8
msgid "**output** -- 4-D with shape [alpha, alpha, CO, CI]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:3
msgid "The raw kernel tensor with layout \"NCHW\"."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:5
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:5
msgid ""
"Tile size of winograd transform. e.g. 2 for F(2x2, 3x3) and 4 for F(4x4, "
"3x3)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:3
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:3
msgid "5-D with shape [batch, in_channel, in_depth, in_height, in_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:5
msgid ""
"5-D with shape [num_filter, in_channel, filter_depth, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:7
msgid "Stride size, or [strid_depth, stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:11 tvm.topi.nn.conv3d.conv3d_ndhwc:11
msgid "dilation size, or [dilation_depth, dilation_height, dilation_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:14
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:16
msgid ""
"**Output** -- 5-D with shape [batch, out_channel, out_depth, out_height, "
"out_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:3
msgid "5-D with shape [batch, in_depth, in_height, in_width, in_channel]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:5
msgid ""
"5-D with shape [filter_depth, filter_height, filter_width, in_channel, "
"num_filter]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:7
msgid "Stride size, or [stride_depth, stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:18
msgid ""
"**Output** -- 5-D with shape [batch, out_depth, out_height, out_width, "
"out_channel]"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:3
msgid "Attributes of current Transposed 3D convolution"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:5
msgid ""
"5-D with shape [in_channel, num_filter, filter_depth, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:7
msgid "The spatial stride along depth,height and width"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:3
msgid "The raw kernel tensor with layout \"NCDHW\"."
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:8
msgid "**output** -- 5-D with shape [alpha, alpha, alpha, CO, CI]"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:3
#: tvm.topi.nn.correlation.correlation_nchw:5
#: tvm.topi.nn.local_response_norm.lrn:8
msgid "4-D with shape [batch, channel, height, width]"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:7
msgid "Kernel size for correlation, must be an odd number"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:9
msgid "Max displacement of Correlation"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:11
msgid "Stride for data1"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:13
msgid "Stride for data2 within the neightborhood centered around data1"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:15
msgid ""
"Padding size, or [pad_height, pad_width] for 2 ints, or [pad_top, "
"pad_left, pad_bottom, pad_right] for 4 ints"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:19
msgid "operation type is either multiplication or substraction"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:3
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:3
msgid ""
"The deformable convolution operation is described in "
"https://arxiv.org/abs/1703.06211"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:7
msgid ""
"4-D with shape [batch, deformable_groups * filter_height * filter_width *"
" 2, out_height, out_width]."
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:18
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:19
msgid "number of deformable groups"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nchw:15
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:15
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:20
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:21
msgid "number of groups"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:7
msgid ""
"4-D with shape [batch, out_height, out_width,                 "
"deformable_groups * filter_height * filter_width * 2]."
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:8
msgid "4-D with shape [batch, out_height, out_width,"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:9
msgid "deformable_groups * filter_height * filter_width * 2]."
msgstr ""

#: of tvm.topi.nn.dense.dense:5 tvm.topi.sparse.dense.dense:6
msgid "2-D with shape [out_dim, in_dim]"
msgstr ""

#: of tvm.topi.nn.dense.dense:7 tvm.topi.sparse.dense.dense:8
msgid "1-D with shape [out_dim]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nchw:17
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:17 tvm.topi.nn.dense.dense:9
msgid "The output type. This is used for mixed precision."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:3
#: tvm.topi.nn.space_to_depth.space_to_depth:3
msgid "4-D tensor in either NCHW or NHWC layout."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:5
msgid "Size of blocks to compose from channel dimension."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:7
#: tvm.topi.nn.space_to_depth.space_to_depth:7
msgid "Either NCHW or NHWC, indicating data layout."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:9
msgid ""
"Either DCR or CDR, indicates how channels should be accessed. In DCR, "
"channels are interwoven in the Tensorflow style while in CDR channels are"
" accessed sequentially as in Pytorch."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:14
msgid ""
"**output** -- Output of shape [N, C / block_size**2, H * block_size, W * "
"block_size]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:5
msgid ""
"6-D with shape [out_channel_chunk, 1, filter_height, filter_width, 1, "
"out_channel_block] In NCHWc depthwise convolution, we group kernel's "
"in_channel and channel_multiplier together then do the tiling."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:19
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:13
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:13
msgid "Output data type"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:22
msgid ""
"**Output** -- 5-D with shape [batch, out_channel_chunk, out_height, "
"out_width, out_channel_block]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:3
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:5
msgid ""
"4-D with shape [filter_height, filter_width, in_channel, "
"channel_multiplier]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:5
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:5
msgid "4-D with shape [batch, out_height, out_width, out_channel]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:12
msgid "**Output** -- 4-D with shape [batch, in_height, in_width, in_channel]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:12
msgid ""
"**Output** -- 4-D with shape [filter_height, filter_width, in_channel, "
"channel_multiplier]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:5
msgid ""
"4-D with shape [in_channel, channel_multiplier, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nhwc:20
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:16
msgid "**Output** -- 4-D with shape [batch, out_height, out_width, out_channel]"
msgstr ""

#: of tvm.topi.nn.dilate.dilate:3
msgid "n-D, can be any layout."
msgstr ""

#: of tvm.topi.nn.dilate.dilate:5
msgid "Dilation stride on each dimension, 1 means no dilation."
msgstr ""

#: of tvm.topi.nn.dilate.dilate:7
msgid "Value used to dilate the input."
msgstr ""

#: of tvm.topi.nn.dilate.dilate:9 tvm.topi.nn.pad.mirror_pad:11
#: tvm.topi.nn.pad.pad:11
msgid "The name prefix operators generated"
msgstr ""

#: of tvm.topi.nn.dilate.dilate:12
msgid "**Output** -- n-D, the same layout as data."
msgstr ""

#: of tvm.topi.utils.equal_const_int:3 tvm.topi.utils.get_const_int:3
msgid "The input expression."
msgstr ""

#: of tvm.topi.utils.equal_const_int:6
msgid "**equal** -- Whether they equals."
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:1
msgid ""
"Perform softmax activation on the data. Use approximation to compute "
"exponent for faster speed."
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:4 tvm.topi.nn.softmax.softmax:3
msgid "can be any dimension"
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:6 tvm.topi.nn.softmax.softmax:5
msgid "channel axis"
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:9 tvm.topi.nn.softmax.softmax:8
msgid "**output** -- output shape is the same as input"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:3
msgid "Compute equivalent of"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:12
msgid "Useful for"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:14
msgid ""
"Encoding explicit re-use of computation in convolution ops operated on a "
"sliding window input"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:15
msgid ""
"Implementing a FIFO queue to cache intermediate results, e.g. as in Fast "
"WaveNet."
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:19
msgid "Previous value of the FIFO buffer"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:21
msgid "Specify which axis should be used for buffering"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:24
msgid "**result** -- Updated value for the buffer"
msgstr ""

#: of tvm.topi.nn.flatten.flatten:3
msgid "Input array."
msgstr ""

#: of tvm.topi.nn.flatten.flatten:6
msgid "**output** -- 2-D array with collapsed higher dimensions."
msgstr ""

#: of tvm.topi.utils.get_const_int:6
msgid "**out_value** -- The output."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:5 tvm.topi.nn.utils.get_pad_tuple1d:5
#: tvm.topi.nn.utils.get_pad_tuple3d:5
msgid "Conv kernel size"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:8
msgid ""
"* **pad_top** (*int*) -- Padding size on top * **pad_left** (*int*) -- "
"Padding size on left * **pad_down** (*int*) -- Padding size on down. * "
"**pad_right** (*int*) -- Padding size on right."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:8 tvm.topi.nn.utils.get_pad_tuple3d:9
msgid "**pad_top** (*int*) -- Padding size on top"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:9 tvm.topi.nn.utils.get_pad_tuple1d:8
#: tvm.topi.nn.utils.get_pad_tuple3d:10
msgid "**pad_left** (*int*) -- Padding size on left"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:10 tvm.topi.nn.utils.get_pad_tuple3d:12
msgid "**pad_down** (*int*) -- Padding size on down."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:11 tvm.topi.nn.utils.get_pad_tuple1d:9
#: tvm.topi.nn.utils.get_pad_tuple3d:13
msgid "**pad_right** (*int*) -- Padding size on right."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple1d:8
msgid ""
"* **pad_left** (*int*) -- Padding size on left * **pad_right** (*int*) --"
" Padding size on right."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple3d:8
msgid ""
"* **pad_front** (*int*) -- Padding size on front. * **pad_top** (*int*) "
"-- Padding size on top * **pad_left** (*int*) -- Padding size on left * "
"**pad_back** (*int*) -- Padding size on back. * **pad_down** (*int*) -- "
"Padding size on down. * **pad_right** (*int*) -- Padding size on right."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple3d:8
msgid "**pad_front** (*int*) -- Padding size on front."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple3d:11
msgid "**pad_back** (*int*) -- Padding size on back."
msgstr ""

#: of tvm.topi.nn.pooling.global_pool:2 tvm.topi.nn.pooling.pool:2
#: tvm.topi.nn.pooling.pool_grad:2
msgid ""
"It decides the height and width dimension according to the layout string,"
" in which 'W' and 'H' means width and height respectively. Width and "
"height dimension cannot be split. For example, NCHW, NCHW16c, etc. are "
"valid for pool, while NCHW16w, NCHW16h are not. See parameter `layout` "
"for more information of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.global_pool:22
msgid ""
"**output** -- n-D in same layout with height and width dimension size of "
"1. e.g., for NCHW, the output shape will be [batch, channel, 1, 1]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nchw:5
msgid ""
"4-D with shape [num_filter, in_channel // groups, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nhwc:5
msgid ""
"4-D with shape [filter_height, filter_width, in_channel // groups, "
"num_filter]"
msgstr ""

#: of tvm.topi.nn.elemwise.leaky_relu:5
msgid "The slope for the small gradient when x < 0"
msgstr ""

#: of tvm.topi.nn.softmax.log_softmax:3
msgid "2-D input data"
msgstr ""

#: of tvm.topi.nn.softmax.log_softmax:6
msgid "**output** -- 2-D output with same shape"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:4
msgid ""
"sum_sqr_up^i{x, y} = (bias+((alpha/size)*"
"                                 {sum_{j=max(0, "
"i-size/2)}^{min(N-1,i+size/2)}                                      "
"(data^j{x,y})^2}))^beta output^i{x, y} = data^i{x, y}/sum_sqr_up^i{x, y} "
"N is the number for input channels"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:10
msgid "normalisation window size"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:12
msgid "input data layout channel axis default value is 1 for NCHW format"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:15
msgid "offset to avoid dividing by 0"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:17
msgid "to be divided"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:19
msgid "exponent"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:22
msgid "**output** -- 4-D output with same shape"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:5 tvm.topi.nn.pad.pad:5
msgid "Pad width on each dimension to pad the before the axis begin."
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:7 tvm.topi.nn.pad.pad:7
msgid "Pad width each dimension to pad the after the axis end."
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:9
msgid "Type of mirror padding to apply. Must be SYMMETRIC or REFLECT"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:14 tvm.topi.nn.pad.pad:14
msgid "**Output** -- n-D, the same layout as Input."
msgstr ""

#: of tvm.topi.nn.pad.pad:9
msgid "The value to be padded."
msgstr ""

#: of tvm.topi.nn.pooling.pool:11 tvm.topi.nn.pooling.pool_grad:13
msgid "Kernel size, [kernel_height, kernel_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool:13 tvm.topi.nn.pooling.pool_grad:15
msgid "Stride size, [stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool:15 tvm.topi.nn.pooling.pool_grad:17
msgid "Pad size, [pad_top, pad_left, pad_bottom, pad_right]]"
msgstr ""

#: of tvm.topi.nn.pooling.pool:19 tvm.topi.nn.pooling.pool1d:19
#: tvm.topi.nn.pooling.pool3d:19 tvm.topi.nn.pooling.pool_grad:21
msgid "Whether to use ceil when calculating output size."
msgstr ""

#: of tvm.topi.nn.pooling.pool:29 tvm.topi.nn.pooling.pool1d:29
#: tvm.topi.nn.pooling.pool3d:29 tvm.topi.nn.pooling.pool_grad:31
msgid "Whether include padding in the calculation when pool_type is 'avg'"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:2
msgid ""
"Width axis is determined according to the layout string. in which 'w' "
"means width. Width dimension cannot be split. For example, NCW, NCW16c, "
"etc. are valid for pool, while NCW16w is not. See parameter `layout` for "
"more information of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:11
msgid "Kernel size, [kernel_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:13
msgid "Stride size, [stride_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:15
msgid "Pad size, [pad_left, pad_right]"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:21
msgid ""
"Layout of the input data. The layout is supposed to be composed of upper "
"cases, lower cases and numbers, where upper case indicates a dimension "
"and the corresponding lower case with factor size indicates the split "
"dimension. For example, NCW16c can describe a 4-D tensor of [batch_size, "
"channel, width, channel_block], in which channel_block=16 is a split of "
"dimension channel."
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:2
msgid ""
"It decides the depth, height and width dimension according to the layout "
"string, in which 'D', 'W' and 'H' means depth, width and height "
"respectively. Depth, width and height dimension cannot be split. For "
"example, NCDHW, NCDHW16c, etc. are valid for pool, while NCDHW16d, "
"NCDHW16w, NCDHW16h are not. See parameter `layout` for more information "
"of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:11
msgid "Kernel size, [kernel_depth, kernel_height, kernel_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:13
msgid "Stride size, [stride_depth, stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:15
msgid "Pad size, [pad_front, pad_top, pad_left, pad_back, pad_bottom, pad_right]"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:21
msgid ""
"Layout of the input data. The layout is supposed to be composed of upper "
"cases, lower cases and numbers, where upper case indicates a dimension "
"and the corresponding lower case with factor size indicates the split "
"dimension. For example, NCDHW16c can describe a 6-D tensor of "
"[batch_size, channel, depth, height, width, channel_block], in which "
"channel_block=16 is a split of dimension channel."
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:1
msgid ""
"PReLU. It accepts two arguments: an input ``x`` and a weight array ``W`` "
"and computes the output as :math:`PReLU(x) y = x > 0 ? x : W * x`, where "
":math:`*` is an elementwise multiplication for each sample in the batch."
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:9
msgid "Channelised slope tensor for prelu"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:11
msgid "The axis where the channel data needs to be applied"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:14
msgid ""
"* **y** (*tvm.te.Tensor*) -- The result. * *Links* * *-----* * **[http** "
"(*//arxiv.org/pdf/1502.01852v1.pdf]*)"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:14
msgid "**y** (*tvm.te.Tensor*) -- The result."
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:15
msgid "*Links*"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:16
msgid "*-----*"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:17
msgid "**[http** (*//arxiv.org/pdf/1502.01852v1.pdf]*)"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:3
msgid "Input tensor, layout is NCHW"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:5
#: tvm.topi.nn.mapping.scale_shift_nhwc:5
msgid "Scale tensor, 1-D of size channel number"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:7
#: tvm.topi.nn.mapping.scale_shift_nhwc:7
msgid "Shift tensor, 1-D of size channel number"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:10
msgid "**Output** -- Output tensor, layout is NCHW"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nhwc:3
msgid "Input tensor, layout is NHWC"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nhwc:10
msgid "**Output** -- Output tensor, layout is NHWC"
msgstr ""

#: of tvm.topi.utils.simplify:6
msgid "**out** -- The simplified output"
msgstr ""

#: of tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:9
msgid ""
"list of shape [M] where M is number of spatial dims, specifies zero-"
"padding size before each spatial dimension."
msgstr ""

#: of tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:12
msgid ""
"list of shape [M] where M is number of spatial dims, specifies zero-"
"padding size after each spatial dimension."
msgstr ""

#: of tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:15
msgid "The value used for padding."
msgstr ""

#: of tvm.topi.nn.space_to_depth.space_to_depth:5
msgid "Size of blocks to decompose into channel dimension."
msgstr ""

#: of tvm.topi.nn.space_to_depth.space_to_depth:10
msgid ""
"**output** -- Output of shape [N, C * block_size**2, H / block_size, W / "
"block_size]"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:7 tvm.topi.nn.sparse.sparse_dense_v2:4
msgid "2-D with shape [M, K], float32"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:9 tvm.topi.nn.sparse.sparse_dense_v1:4
#: tvm.topi.nn.sparse.sparse_dense_v2:6
msgid ""
"1-D with shape [nnz] (CSR) or 3-D with shape [num_blocks, bs_r, bs_c] "
"(BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:12 tvm.topi.nn.sparse.sparse_dense_v1:6
#: tvm.topi.nn.sparse.sparse_dense_v2:9
msgid "1-D with shape [nnz] (CSR) or 1-D with shape [num_blocks] (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:15 tvm.topi.nn.sparse.sparse_dense_v2:12
msgid "1-D with shape [N + 1] (CSR) or 1-D with shape [(N + 1) // bs_r] (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:18
msgid "Indicates whether lhs or rhs matrix is sparse. Default value is False."
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:21 tvm.topi.nn.sparse.sparse_dense_v1:12
#: tvm.topi.nn.sparse.sparse_dense_v2:16
msgid "**output** -- 2-D with shape [M, N]"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_alter_layout:3
msgid ""
"This is used for modifying the inputs weights so they are more amenable "
"for the target."
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_v1:8
msgid "1-D with shape [M + 1] (CSR) or 1-D with shape [(M + 1) // bs_r] (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_v1:10
msgid "2-D with shape [N, K], float32"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:1
msgid ""
"Transpose a square sparse matrix, `A` is an n-by-n sparse matrix in the "
"CSR format. ** Currently only support Square Matrices **"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:5
msgid "1-D with shape [nonzeros], dtype of 'float32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:7
msgid "1-D with shape [nonzeros], dtype of 'int32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:9
msgid "1-D with shape [n+1], dtype of 'int32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:12
msgid ""
"* **out_data** (*tvm.te.Tensor*) -- 1-D with shape [nonzeros], dtype of "
"'float32' * **out_indices** (*tvm.te.Tensor*) -- 1-D with shape "
"[nonzeros], dtype of 'int32' * **out_indptr** (*tvm.te.Tensor*) -- 1-D "
"with shape [n+1], dtype of 'int32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:12
msgid ""
"**out_data** (*tvm.te.Tensor*) -- 1-D with shape [nonzeros], dtype of "
"'float32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:13
msgid ""
"**out_indices** (*tvm.te.Tensor*) -- 1-D with shape [nonzeros], dtype of "
"'int32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:14
msgid "**out_indptr** (*tvm.te.Tensor*) -- 1-D with shape [n+1], dtype of 'int32'"
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:3
msgid "The output tensor of conv2d_NCHWc."
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:5
msgid "The output dtype."
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:8
msgid "**unpacked_out** -- The unpacked output tensor in NCHW layout."
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:2 tvm.topi.nn.upsampling.upsampling3d:2
msgid "Nearest neighbor and bilinear upsampling are supported."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:3 tvm.topi.image.resize.resize:3
#: tvm.topi.image.resize.resize_bicubic:7
#: tvm.topi.image.resize.resize_bilinear:7
#: tvm.topi.image.resize.resize_nearest_neighbor:7
#: tvm.topi.nn.upsampling.upsampling:4
msgid ""
"inputs is a 4-D tensor with shape [batch, channel, in_height, in_width] "
"or  [batch, in_height, in_width, channel]"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:8
#: tvm.topi.nn.upsampling.upsampling3d:10
msgid "Scaling factor for height"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:10
#: tvm.topi.nn.upsampling.upsampling3d:12
msgid "Scaling factor for width"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:12
msgid "either \"NCHW\" or \"NHWC\""
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:14
#: tvm.topi.nn.upsampling.upsampling3d:16
msgid "Method to be used for upsampling."
msgstr ""

#: of tvm.topi.image.resize.resize:20 tvm.topi.nn.upsampling.upsampling:16
#: tvm.topi.nn.upsampling.upsampling3d:23
msgid ""
"Shape to return. If left None will be inferred (If shape is determined "
"dynamically, pass out_dtype.shape as output_shape)"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:20
msgid ""
"**output** -- 4-D with shape [batch, channel, in_height*scale_h, "
"in_width*scale_w] or [batch, in_height*scale, in_width*scale, channel]"
msgstr ""

#: of tvm.topi.image.resize.resize3d:3 tvm.topi.nn.upsampling.upsampling3d:4
msgid ""
"inputs is a 5-D tensor with shape [batch, channel, in_depth, in_height, "
"in_width] or  [batch, in_depth, in_height, in_width, channel]"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:8
msgid "Scaling factor for depth"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:14
msgid "either \"NCDHW\" or \"NDHWC\""
msgstr ""

#: of tvm.topi.image.resize.resize:11 tvm.topi.image.resize.resize_bicubic:29
#: tvm.topi.image.resize.resize_bilinear:29
#: tvm.topi.image.resize.resize_nearest_neighbor:29
#: tvm.topi.nn.upsampling.upsampling3d:18
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Refer to the ONNX Resize operator "
"specification for details. Available options are \"half_pixel\", "
"\"align_corners\" and \"asymmetric\"."
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:27
msgid ""
"**output** -- 5-D with shape [batch, channel, in_depth*scale, "
"in_height*scale, in_width*scale] or [batch, in_depth*scale, "
"in_height*scale, in_width*scale, channel]"
msgstr ""

#: ../../api/python/topi.rst:34
msgid "tvm.topi.image"
msgstr ""

#: of tvm.topi.image:1
msgid "IMAGE network operators"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`affine_grid <tvm.topi.image.tvm.topi.image.affine_grid>`\\ "
"\\(data\\, target\\_shape\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1
#: tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "affine_grid operator that generates 2D sampling grid."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`crop_and_resize <tvm.topi.image.tvm.topi.image.crop_and_resize>`\\ "
"\\(data\\, boxes\\, box\\_indices\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.crop_and_resize:1
msgid "Perform crop and resize operation on the data."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`dilation2d_nchw <tvm.topi.image.tvm.topi.image.dilation2d_nchw>`\\ "
"\\(input\\, filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:1
#: tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "Morphological dilation operator in NCHW layout."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`dilation2d_nhwc <tvm.topi.image.tvm.topi.image.dilation2d_nhwc>`\\ "
"\\(input\\, filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:1
#: tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "Morphological 2d dilation NHWC layout."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`get_2d_indices <tvm.topi.image.tvm.topi.image.get_2d_indices>`\\ "
"\\(indices\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_2d_indices:1
msgid "Get 2d indices"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`get_2d_pixel <tvm.topi.image.tvm.topi.image.get_2d_pixel>`\\ "
"\\(data\\, layout\\, boxes\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_2d_pixel:1
msgid "Get 2d pixel"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`get_pad_tuple <tvm.topi.image.tvm.topi.image.get_pad_tuple>`\\ "
"\\(padding\\, kernel\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`grid_sample <tvm.topi.image.tvm.topi.image.grid_sample>`\\ "
"\\(data\\, grid\\[\\, method\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.grid_sample.grid_sample:1
msgid "Applies bilinear sampling to input feature map."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`nchw_pack_layout "
"<tvm.topi.image.tvm.topi.image.nchw_pack_layout>`\\ \\(layout\\_info\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.utils.nchw_pack_layout:1
msgid "Check whether the layout type is NCHWinic"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`nchw_xc_layout <tvm.topi.image.tvm.topi.image.nchw_xc_layout>`\\ "
"\\(layout\\_info\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.utils.nchw_xc_layout:1
msgid "Check whether the layout type is NCHWxc"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`pad <tvm.topi.image.tvm.topi.image.pad>`\\ \\(data\\, "
"pad\\_before\\[\\, pad\\_after\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`resize <tvm.topi.image.tvm.topi.image.resize>`\\ \\(data\\, "
"size\\[\\, layout\\, method\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.resize:1 tvm.topi.image.resize.resize3d:1
msgid "Perform resize operation on the data."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`resize3d <tvm.topi.image.tvm.topi.image.resize3d>`\\ \\(data\\, "
"size\\[\\, layout\\, method\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`resize_bicubic <tvm.topi.image.tvm.topi.image.resize_bicubic>`\\ "
"\\(indices\\, data\\, image\\_height\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "Perform resize operation with bicubic method on the data."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`resize_bilinear <tvm.topi.image.tvm.topi.image.resize_bilinear>`\\ "
"\\(indices\\, data\\, image\\_height\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "Perform resize operation with bilinear method on the data."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":obj:`resize_nearest_neighbor "
"<tvm.topi.image.tvm.topi.image.resize_nearest_neighbor>`\\ \\(indices\\, "
"data\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "Perform resize operation with nearest neighbor method on the data."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ":obj:`simplify <tvm.topi.image.tvm.topi.image.simplify>`\\ \\(expr\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:3
msgid ""
"This operation is described in https://arxiv.org/pdf/1506.02025.pdf. It "
"generates a uniform sampling grid within the target shape and normalizes "
"it to [-1, 1]. The provided affine transformation is then applied on the "
"sampling grid."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:7
msgid "3-D with shape [batch, 2, 3]. The affine matrix."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:9
msgid "Specifies the output shape (H, W)."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:12
msgid "**Output** -- 4-D with shape [batch, 2, target_height, target_width]"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:7
#: tvm.topi.image.resize.resize_bicubic:19
#: tvm.topi.image.resize.resize_bilinear:19
#: tvm.topi.image.resize.resize_nearest_neighbor:19
msgid ""
"A 2-D tensor of shape [num_boxes, 4]. Each row of the tensor specifies "
"the coordinates of a box."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:10
#: tvm.topi.image.resize.resize_bicubic:22
#: tvm.topi.image.resize.resize_bilinear:22
#: tvm.topi.image.resize.resize_nearest_neighbor:22
msgid ""
"A 1-D tensor of shape [num_boxes], box_indices[i] specifies the data that"
" the i-th box refers to."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:13
msgid "The target size of each box."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:15
msgid "\"NCHW\", \"NHWC\""
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:17 tvm.topi.image.resize.resize:16
#: tvm.topi.image.resize.resize3d:17
msgid "Method to be used for resizing."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:19
#: tvm.topi.image.resize.resize_bicubic:25
#: tvm.topi.image.resize.resize_bilinear:25
#: tvm.topi.image.resize.resize_nearest_neighbor:25
msgid "Value used for extrapolation, when applicable."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:21 tvm.topi.image.resize.resize:18
#: tvm.topi.image.resize.resize3d:19 tvm.topi.image.resize.resize_bicubic:34
#: tvm.topi.image.resize.resize_bilinear:34
#: tvm.topi.image.resize.resize_nearest_neighbor:34
msgid "Type to return. If left None will be same as input type."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:24
msgid ""
"**output** -- 4-D with shape [num_boxes, channel, crop_height, "
"crop_width] or [num_boxes, crop_height, crop_width, channel]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:5
msgid "3-D with shape [ in_channel, filter_height, filter_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:9
#: tvm.topi.image.dilation2d.dilation2d_nhwc:9
msgid "Padding size"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:16
msgid "**Output** -- 4-D with shape [batch, in_channel, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:5
msgid "3-D with shape [filter_height, filter_width, in_channel]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:16
msgid "**Output** -- 4-D with shape [batch, out_height, out_width, in_channel]"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:3
msgid ""
"Given :math:`data` and :math:`grid`, assuming NCHW layout, then the "
"output is computed by"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:5
msgid ""
"x_{src} = grid[batch, 0, y_{dst}, x_{dst}] \\\n"
"y_{src} = grid[batch, 1, y_{dst}, x_{dst}] \\\n"
"output[batch, channel, y_{dst}, x_{dst}] = G(data[batch, channel, "
"y_{src}, x_{src})"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:11
msgid ""
":math:`x_{dst}`, :math:`y_{dst}` enumerate all spatial locations in "
":math:`output`, and :math:`G()` denotes the interpolation method. The "
"out-boundary points will be padded with zeros. The shape of the output "
"will be (data.shape[0], data.shape[1], grid.shape[2], grid.shape[3])."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:16
msgid "The operator assumes that :math:`grid` has been normalized to [-1, 1]."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:18
msgid ""
"grid_sample often cooperates with affine_grid which generates sampling "
"grids for grid_sample."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:22
msgid "4-D with shape [batch, 2, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:24
msgid "The interpolation method. Only 'bilinear' is supported."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:26
msgid "The layout of input data and the output."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:29
msgid "**Output** -- 4-D with shape [batch, 2, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.resize.resize:7 tvm.topi.image.resize.resize3d:7
msgid "Output resolution scale to"
msgstr ""

#: of tvm.topi.image.resize.resize:9 tvm.topi.image.resize.resize_bicubic:27
#: tvm.topi.image.resize.resize_bilinear:27
#: tvm.topi.image.resize.resize_nearest_neighbor:27
msgid "\"NCHW\", \"NHWC\", or \"NCHWc\"."
msgstr ""

#: of tvm.topi.image.resize.resize:24
msgid ""
"**output** -- 4-D with shape [batch, channel, in_height*scale, "
"in_width*scale] or [batch, in_height*scale, in_width*scale, channel] or "
"5-D with shape [batch, channel-major, in_height*scale, in_width*scale, "
"channel-minor]"
msgstr ""

#: of tvm.topi.image.resize.resize3d:9
msgid "\"NCDHW\", \"NDHWC\", or \"NCDHWc\"."
msgstr ""

#: of tvm.topi.image.resize.resize3d:11
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Refer to the ONNX Resize operator "
"specification for details.  Available options are \"half_pixel\", "
"\"align_corners\" and \"asymmetric\"."
msgstr ""

#: of tvm.topi.image.resize.resize3d:11
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Refer to the ONNX Resize operator "
"specification for details."
msgstr ""

#: of tvm.topi.image.resize.resize3d:15
msgid ""
"Available options are \"half_pixel\", \"align_corners\" and "
"\"asymmetric\"."
msgstr ""

#: of tvm.topi.image.resize.resize3d:22
msgid ""
"**output** -- 5-D with shape [batch, channel, in_depth*scale, "
"in_height*scale, in_width*scale] or [batch, in_depth*scale, "
"in_height*scale, in_width*scale, channel] or 5-D with shape [batch, "
"channel-major, in_depth*scale, in_height*scale, in_width*scale, channel-"
"minor]"
msgstr ""

#: of tvm.topi.image.resize.resize_bicubic:1
msgid ""
"Perform resize operation with bicubic method on the data. More details "
"about Bicubic interpolation please refer to "
"https://en.wikipedia.org/wiki/Bicubic_interpolation."
msgstr ""

#: of tvm.topi.image.resize.resize_bicubic:5
#: tvm.topi.image.resize.resize_bilinear:5
#: tvm.topi.image.resize.resize_nearest_neighbor:5
msgid "The indices of input data"
msgstr ""

#: of tvm.topi.image.resize.resize_bicubic:11
#: tvm.topi.image.resize.resize_bilinear:11
#: tvm.topi.image.resize.resize_nearest_neighbor:11
msgid "Input image height"
msgstr ""

#: of tvm.topi.image.resize.resize_bicubic:13
#: tvm.topi.image.resize.resize_bilinear:13
#: tvm.topi.image.resize.resize_nearest_neighbor:13
msgid "Input image width"
msgstr ""

#: of tvm.topi.image.resize.resize_bicubic:15
#: tvm.topi.image.resize.resize_bilinear:15
#: tvm.topi.image.resize.resize_nearest_neighbor:15
msgid "The target resized image height"
msgstr ""

#: of tvm.topi.image.resize.resize_bicubic:17
#: tvm.topi.image.resize.resize_bilinear:17
#: tvm.topi.image.resize.resize_nearest_neighbor:17
msgid "The target resized image width"
msgstr ""

#: of tvm.topi.image.resize.resize_bicubic:37
#: tvm.topi.image.resize.resize_bilinear:37
#: tvm.topi.image.resize.resize_nearest_neighbor:37
msgid "**output** -- The computed result with type out_dtype"
msgstr ""

#: of tvm.topi.image.resize.resize_bilinear:1
msgid ""
"Perform resize operation with bilinear method on the data. For details "
"about Bilinear interpolation please refer to "
"https://en.wikipedia.org/wiki/Bilinear_interpolation."
msgstr ""

#: of tvm.topi.image.resize.resize_nearest_neighbor:1
msgid ""
"Perform resize operation with nearest neighbor method on the data. For "
"details about Nearest-neighbor interpolation please refer to "
"https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation."
msgstr ""

#: ../../api/python/topi.rst:42
msgid "tvm.topi.sparse"
msgstr ""

#: of tvm.topi.sparse:1
msgid "Sparse operators"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:1:<autosummary>:1
msgid ":obj:`csrmv <tvm.topi.sparse.csrmv>`\\ \\(a\\, x\\[\\, y\\]\\)"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:1
#: tvm.topi.sparse.csrmv.csrmv:1:<autosummary>:1
msgid ""
"The `csrmv` routine performs a matrix-vector operation defined as "
":math:`y := A*x + y`, where `x` and `y` are vectors, `A` is an m-by-k "
"sparse matrix in the CSR format."
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:1:<autosummary>:1
msgid ":obj:`csrmm <tvm.topi.sparse.csrmm>`\\ \\(a\\, b\\[\\, c\\]\\)"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:1
#: tvm.topi.sparse.csrmv.csrmv:1:<autosummary>:1
msgid ""
"The `csrmm` routine performs a matrix-matrix operation defined as "
":math:`C := A*B + C`, where `B` and `C` are dense matrices, `A` is an "
"m-by-k sparse matrix in the CSR format."
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:1:<autosummary>:1
msgid ":obj:`dense <tvm.topi.sparse.dense>`\\ \\(data\\, weight\\[\\, bias\\]\\)"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:1:<autosummary>:1
msgid "Applies a linear transformation: :math:`Y = XW^T + b`."
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:4 tvm.topi.sparse.csrmv.csrmv:4
msgid "2-D sparse matrix with shape [m, k]"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:6
msgid "2-D dense matrix with shape [k, 1]"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:8
msgid "1-D dense vector with shape [1]"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:11
msgid "**output** -- 2-D dense matrix with shape [m, 1]"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:6
msgid "2-D dense matrix with shape [k, n]"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:8
msgid "1-D dense vector with shape [n]"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:11
msgid "**output** -- 2-D with shape [m, n]"
msgstr ""

#: of tvm.topi.sparse.dense.dense:1
msgid ""
"Applies a linear transformation: :math:`Y = XW^T + b`. Either data or "
"weight should be tvm.contrib.sparse.CSRNDArray."
msgstr ""


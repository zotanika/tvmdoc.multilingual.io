# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-04 20:34+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_vta_tutorials_optimize_matrix_multiply_opt.py>` to "
"download the full example code"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:24
msgid "Matrix Multiply Blocking"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:25
msgid "**Author**: `Thierry Moreau <https://homes.cs.washington.edu/~moreau/>`_"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:27
msgid ""
"This tutorial provides an overview on how to use TVM to map matrix "
"multiplication efficiently on the VTA design. We recommend covering the "
":ref:`basic-mat-mult` tutorial first."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:31
msgid ""
"In this tutorial, we will demonstrate TVM schedule optimizations to break"
" large neural network operators down onto smaller blocks to achieve "
"computation within limited hardware accelerator resources."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:38
msgid "RPC Setup"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:39
msgid "We start by programming the Pynq's FPGA and building its RPC runtime."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:109
msgid "Computation Declaration"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:110
msgid ""
"As a first step, we need to describe our matrix multiplication "
"computation. We define the matrix multiplication as the computation one "
"would find in a fully connected layer, defined by its batch size, input "
"channels, and output channels. These have to be integer multiples of the "
"VTA tensor shape: :code:`BATCH`, :code:`BLOCK_IN`, and :code:`BLOCK_OUT` "
"respectively."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:117
msgid ""
"We've added extra operators to the matrix multiplication that apply "
"shifting and clipping to the output in order to mimic a fixed-point "
"matrix multiplication followed by a rectified linear activation. We "
"describe the TVM dataflow graph of the fully connected layer below:"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:125
msgid ""
"This computation is intentionally too large to fit onto VTA's on-chip "
"buffers all at once. Therefore in the scheduling phase we'll rely on "
"computation blocking strategies to break the computation down into "
"manageable chunks."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:192
msgid "Scheduling the Computation"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:193
msgid ""
"We'll look at a set of schedule transformations necessary to map the "
"matrix multiplications onto VTA in an efficient fashion. Those include:"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:197
msgid "Computation blocking"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:198
msgid "Lowering to VTA hardware intrinsics"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:215
msgid "Blocking the Computation"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:216
msgid ""
"The matrix multiplication is by default too large for activations or "
"weights to fit on VTA's on-chip buffers all at once. We block the (1, "
"1024) by (1024, 1024) matrix multiplication into smaller (1, 256) by "
"(256, 256) matrix multiplications so the intermediate tensors can fit on "
"the accelerator's on-chip SRAM. This approach is similar to blocking "
"techniques applied to CPUs and GPUs in order to increase cache hit rate."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:224
msgid ""
"We perform blocking along each axes (the batch axis being untouched since"
" we are performing singe-batch inference). We also leave the inner-most "
"tensorization axes as-is in order to allow TVM to pattern-match "
"tensorization. We show the outcome of blocking on the computation "
"schedule in the diagram below:"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:237
msgid ""
"The code after loop splitting and reordering is equivalent to the "
"following pseudo-code. We ignore the batch axis since we are only "
"performing single-batch inference in this example:"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:308
msgid "Lowering Copies to DMA Transfers"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:309
msgid ""
"Next we set the buffer scopes to the corresponding on-chip VTA SRAM "
"buffers. We move the load loops into the matrix multiply computation loop"
" to stage memory loads such that they fit in the on-chip SRAM buffers. "
"Finally we annotate the load/store loop outer axes with the DMA copy "
"pragma to perform bulk memory transfers on VTA."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:345
msgid "Lowering Computation to VTA Compute Intrinsics"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:346
msgid ""
"The last phase is to lower the computation loops down to VTA hardware "
"intrinsics by mapping the matrix multiplication to tensor intrinsics, and"
" mapping the shift, and clipping computation to the vector ALU."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:372
msgid "TVM Compilation and Verification"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:373
msgid ""
"After specifying the schedule, we can compile it into a TVM function. We "
"save the module so we can send it over RPC. We run the function and "
"verify it against a numpy implementation to ensure correctness."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:440
msgid "Summary"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:441
msgid ""
"This tutorial demonstrates how TVM scheduling primitives can achieve "
"computation blocking for a matrix multiplication example. This allows us "
"to map arbitrarily large computation onto limited hardware accelerator "
"resources."
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:460
msgid ""
":download:`Download Python source code: matrix_multiply_opt.py "
"<matrix_multiply_opt.py>`"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:466
msgid ""
":download:`Download Jupyter notebook: matrix_multiply_opt.ipynb "
"<matrix_multiply_opt.ipynb>`"
msgstr ""

#: ../../vta/tutorials/optimize/matrix_multiply_opt.rst:473
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


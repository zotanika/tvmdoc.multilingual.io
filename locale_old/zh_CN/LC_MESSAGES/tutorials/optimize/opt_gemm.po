# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-04 20:34+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../tutorials/optimize/opt_gemm.rst:13
msgid ""
"Click :ref:`here <sphx_glr_download_tutorials_optimize_opt_gemm.py>` to "
"download the full example code"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:24
msgid "How to optimize GEMM on CPU"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:25
msgid ""
"**Author**: `Jian Weng <https://github.com/were>`_,             `Ruofei "
"Yu <https://github.com/yuruofeifei>`_"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:27
msgid ""
"(TL;DR) TVM provides abstract interfaces which allows users to depict an "
"algorithm and the algorithm's implementing organization (the so-called "
"schedule) separately. Typically, writing algorithm in high-performance "
"schedule breaks the algorithm's readability and modularity. Also, trying "
"various seemingly promising schedules is time-consuming. With the help of"
" TVM, we can try these schedules efficiently to enhance the performance."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:33
msgid ""
"In this tutorial, we will demonstrate how to use TVM to optimize square "
"matrix multiplication and achieve 200 times faster than baseline by "
"simply adding 18 extra lines of code."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:43
msgid ""
"There are two important optimizations on intense computation applications"
" executed on CPU:"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:37
msgid ""
"Increase the cache hit rate of memory access. Both complex numerical "
"computation and hot-spot memory access can be accelerated from high cache"
" hit rate. This requires us to transform the origin memory access pattern"
" to the pattern fits the cache policy."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:40
msgid ""
"SIMD (Single instruction multi-data), or we call it vector processing "
"unit. Every time, a small batch of data, rather than a single grid, will "
"be processed. This requires us to transform the data access pattern in "
"the loop body in uniform pattern so that the LLVM backend can lower it to"
" SIMD."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:45
msgid ""
"Actually, all the methodologies used in this tutorial is a subset of "
"tricks mentioned in this `repo <https://github.com/flame/how-to-optimize-"
"gemm>`_. Some of them have been applied by TVM abstraction automatically,"
" but some of them cannot be simply applied due to TVM constraints."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:49
msgid ""
"All the experiment results mentioned below, are executed on 2015's 15' "
"MacBook equipped with Intel i7-4770HQ CPU. The cache line size should be "
"64 bytes for all the x86 CPUs."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:55
msgid "Preparation and Baseline"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:56
msgid ""
"In this tutorial, we will demo how to use TVM to optimize matrix "
"multiplication. Before actually demonstrating, we first define these "
"variables. Then we write a baseline implementation, the simplest way to "
"write a matrix multiplication in TVM."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:131
#: ../../tutorials/optimize/opt_gemm.rst:159
#: ../../tutorials/optimize/opt_gemm.rst:227
#: ../../tutorials/optimize/opt_gemm.rst:253
#: ../../tutorials/optimize/opt_gemm.rst:329
#: ../../tutorials/optimize/opt_gemm.rst:355
#: ../../tutorials/optimize/opt_gemm.rst:426
#: ../../tutorials/optimize/opt_gemm.rst:452
#: ../../tutorials/optimize/opt_gemm.rst:546
#: ../../tutorials/optimize/opt_gemm.rst:572
#: ../../tutorials/optimize/opt_gemm.rst:664
#: ../../tutorials/optimize/opt_gemm.rst:690
#: ../../tutorials/optimize/opt_gemm.rst:788
#: ../../tutorials/optimize/opt_gemm.rst:814
msgid "Out:"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:143
msgid ""
"In TVM, we can always inspect lower level IR to debug or optimize our "
"schedule. Here is the generated IR using our baseline schedule."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:187
msgid "Blocking"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:188
msgid ""
"A important trick to enhance the cache hit rate is blocking --- data "
"chunk will be computed block by block. The memory access inside the block"
" is a small neighbourhood which is with high memory locality. In this "
"tutorial, I picked up 32 as the blocking factor. So the block will fill "
"32 * 32 * sizeof(float) which is 4KB in the cache whose total size is "
"32KB (L1 data cache)"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:238
#: ../../tutorials/optimize/opt_gemm.rst:675
msgid "Here is the generated IR after blocking."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:291
msgid "Vectorization"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:292
msgid ""
"Another important trick is vectorization. When the memory access pattern "
"is uniform, the compiler can detect this pattern and pass the continuous "
"memory to vector processor. In TVM, we can use `vectorize` interface to "
"hint the compiler this pattern, so that we can accelerate it vastly."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:296
msgid ""
"In this tutorial, we chose to vectorize the inner loop row data since it "
"is cache friendly."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:340
msgid "Here is the generated IR after vectorization."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:389
msgid "Loop Permutation"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:390
msgid ""
"If we look at the above IR, we can see the inner loop row data is "
"vectorized and B is transformed into PackedB. The traversal of PackedB is"
" sequential now. So we will look at the access pattern of A. In current "
"schedule, A is accessed column by column which is not cache friendly. If "
"we change the nested loop order of ki and inner axes xi, the access "
"pattern for A matrix is more cache friendly."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:437
msgid "Here is the generated IR after loop permutation."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:486
msgid "Array Packing"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:487
msgid ""
"Another important trick is array packing. This trick is to reorder the "
"storage dimension of the array to convert the continuous access pattern "
"on certain dimension to a sequential pattern after flattening."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:497
msgid ""
"Just as it is shown in the figure above, after blocking the computations,"
" we can observe the array access pattern of B (after flattening), which "
"is regular but discontinuous. We expect that after some transformation we"
" can get continuous access pattern. We can reorder a [16][16] array to a "
"[16/4][16][4] array, so that the access pattern of B will be sequential "
"when grabing the corresponding value from the packed array."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:557
msgid "Here is the generated IR after array packing."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:614
msgid "Write cache for blocks"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:615
msgid ""
"After blocking, the program will write result to C block by block, the "
"access pattern is not sequential. So we can use a sequential cache array "
"to hold the block results and write to C when all the block results are "
"ready."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:740
msgid "Parallel"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:741
msgid ""
"Futhermore, we can also utilize multi-core processors to do the thread-"
"level parallelization."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:799
msgid "Here is the generated IR after parallelization."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:864
msgid "Summary"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:865
#, python-format
msgid ""
"After applying the above simple optimizations with only 18 lines of code,"
" our generated code can achieve 60% of the `numpy` performance with MKL. "
"Note that the outputs on the web page reflect the running times on a non-"
"exclusive Docker container, thereby they are *unreliable*. It is highly "
"encouraged to run the tutorial by yourself to observe the performance "
"gain acheived by TVM."
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:884
msgid ":download:`Download Python source code: opt_gemm.py <opt_gemm.py>`"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:890
msgid ":download:`Download Jupyter notebook: opt_gemm.ipynb <opt_gemm.ipynb>`"
msgstr ""

#: ../../tutorials/optimize/opt_gemm.rst:897
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-04 20:34+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_tutorials_optimize_opt_matmul_auto_tensorcore.py>` to "
"download the full example code"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:24
msgid "How to optimize matmul with Auto TensorCore CodeGen"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:25
msgid ""
"**Author**: `Minmin Sun <https://github.com/minminsun>`_,             "
"`Lanbo Li <https://github.com/Orion34C>`_,             `Chenfan Jia "
"<https://github.com/jcf94>`_,             `Jun Yang "
"<https://github.com/yangjunpro>`_"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:27
msgid ""
"In this tutorial, we will demonstrate how to write a high performance "
"matmul schedule on Volta/Turing GPUs with TVM Auto TensorCore CodeGen. "
"This is a transparent solution to generate tensorcore kernel with most "
"transformations done in ir passes. Users can also write schedule with "
"tensorization to generate TensorCore code. Both solutions use the same "
"tensorcore intrinsics. Please refer to :ref:`opt-conv-tensorcore` "
"tutorial for more details."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:38
msgid "Preparation and Algorithm"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:39
msgid ""
"2 kinds of input data types are supported: float16 and int8. For float16,"
" the accumulator is float32. For int8, the accumulator is int32. For data"
" layouts, 'N' means None-transpose while 'T' means Transpose."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:96
msgid "Scheduling the Computation"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:97
msgid ""
"This schedule is no different than a non-tensorcore matmul schedule on "
"GPU. Please refer to :ref:`opt-gemm` tutorial for basics of optimizing "
"matmul schedule. When the \"tensor_core\" pragma is set, the \"rewrite "
"for tensorcore\" ir pass will automatically transform the schedule for "
"tensorcore codegen, otherwise normal CUDA code, with lower performance "
"but equal functionality, will be generated."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:105
msgid "*Requirements of TesnsorCore*"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:107
msgid ""
"Note that in the following 2 cases, even though the \"tensor_core\" "
"pragma is set, TVM will still fall back to normal CUDA codegen: (1) The "
"m, n or k of input matrices is not multiple of 16; (2) The warp tile size"
" is not 16x16x16 on CUDA9, or not one of {16x16x16, 32x8x16, 8x32x16} on "
"CUDA version >= 10.0."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:111
msgid ""
"In this schedule, storage_align is used to reduce bank conflicts of "
"shared memory. Please refer to this `doc "
"<https://tvm.apache.org/docs/api/python/te.html#tvm.te.Stage.storage_align>`_"
" for the usage of storage_align primitive. In short, we need to add an "
"offset to some shared memory buffer to reduce bank conflicts. According "
"to the `wmma doc <https://docs.nvidia.com/cuda/cuda-c-programming-"
"guide/index.html#wmma-description>`_, the stride of load_matrix_sync must"
" be a multiple of 16 bytes, so we choose 8 as offset for float16 and 16 "
"as offset for int8."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:119
msgid "We use AutoTVM to search for best configurations in this schedule."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:272
msgid "AutoTune and Test"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:273
msgid ""
"Finally we use a tuner to tune the schedule, generate code with best "
"config and run the kernel to compare with numpy to check whether the "
"results are correct."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:451
msgid "Sample Output"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:588
msgid "Summary"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:589
msgid ""
"This tutorial demonstrates how to use the AutoTensorCoreCodeGen of TVM to"
" generate tensorcore kernels."
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:605
msgid ""
":download:`Download Python source code: opt_matmul_auto_tensorcore.py "
"<opt_matmul_auto_tensorcore.py>`"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:611
msgid ""
":download:`Download Jupyter notebook: opt_matmul_auto_tensorcore.ipynb "
"<opt_matmul_auto_tensorcore.ipynb>`"
msgstr ""

#: ../../tutorials/optimize/opt_matmul_auto_tensorcore.rst:618
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""


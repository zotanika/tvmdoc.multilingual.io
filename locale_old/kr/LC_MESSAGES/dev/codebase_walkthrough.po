# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-04 20:34+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../dev/codebase_walkthrough.rst:20
msgid "TVM Codebase Walkthrough by Example"
msgstr "예제로 TVM 코드베이스 익히기"

#: ../../dev/codebase_walkthrough.rst:22
msgid ""
"Getting to know a new codebase can be a challenge. This is especially "
"true for a codebase like that of TVM, where different components interact"
" in non-obvious ways. In this guide, we try to illustrate the key "
"elements that comprise a compilation pipeline with a simple example. For "
"each important step, we show where in the codebase it is implemented. The"
" purpose is to let new developers and interested users dive into the "
"codebase more quickly."
msgstr ""
"새로운 코드베이스에 익숙해지기란 언제나 도전입니다. 특히나 TVM 처럼 서로 다른 요소들이 "
"직관적이지 않은 방식으로 상호 작용하는 코드베이스들은 더욱 그렇죠. "
"이 가이드에서 우리는 간단한 예제를 통해 컴파일레이션 파이프라인을 구성하는 핵심 요소들을 "
"묘사해 보려고 합니다. 중요한 단계 단계마다 코드베이스의 어느 위치에 구현되어 있는지도 "
"소개하겠습니다. 목표는 새로운 개발자들과 TVM에 흥미를 가진 사용자들이 코드베이스에 더 빨리 "
"빠져들게 돕는 것입니다."

#: ../../dev/codebase_walkthrough.rst:26
msgid "Codebase Structure Overview"
msgstr "코드베이스 구조 개요"

#: ../../dev/codebase_walkthrough.rst:28
msgid ""
"At the root of the TVM repository, we have following subdirectories that "
"together comprise a bulk of the codebase."
msgstr ""
"TVM 저장소 맨 위에서부터, 아래와 같은 서브디렉터리들이 코드베이스 덩어리를 함께 구성합니다. "

#: ../../dev/codebase_walkthrough.rst:30
msgid "``src`` - C++ code for operator compilation and deployment runtimes."
msgstr "``src`` - 연산자 컴파일레이션과 탑재 런타임을 위한 C++ 코드."

#: ../../dev/codebase_walkthrough.rst:31
msgid ""
"``src/relay`` - Implementation of Relay, a new functional IR for deep "
"learning framework."
msgstr ""
"``src/relay`` - 딥러닝을 위한 새로운 함수형 IR인 Relay의 구현체."

#: ../../dev/codebase_walkthrough.rst:32
msgid ""
"``python`` - Python frontend that wraps C++ functions and objects "
"implemented in ``src``."
msgstr ""
"``python`` - ``src`` 에 구현된 C++ 함수와 객체를 포장한 파이썬 프론트엔드."

#: ../../dev/codebase_walkthrough.rst:33
msgid ""
"``src/topi`` - Compute definitions and backend schedules for standard "
"neural network operators."
msgstr ""
"``src/topi`` - 표준 신경망 연산자를 위한 컴퓨트 정의와 백엔드 스케줄."

#: ../../dev/codebase_walkthrough.rst:35
msgid ""
"Using standard Deep Learning terminology, ``src/relay`` is the component "
"that manages a computational graph, and nodes in a graph are compiled and"
" executed using infrastructure implemented in the rest of ``src``. "
"``python`` provides python bindings for the C++ API and driver code that "
"users can use to execute compilation. Operators corresponding to each "
"node are registered in ``src/relay/op``. Implementations of operators are"
" in ``topi``, and they are coded in either C++ or Python."
msgstr ""
"표준 딥러닝 용어를 빌면, ``src/relay`` 는 계산 그래프를 관리하는 컴포넌트이며, "
"그래프 내의 노드들의 컴파일과 실행에는 ``src`` 의 나머지 부분에 구현되어 있는 "
"인프라스트럭쳐가 활용됩니다. "
"``python`` 은 사용자들이 컴파일할 때 사용할 수 있는 C++ API와 드라이버 코드들의 "
"파이썬 바인딩을 제공합니다. 각 노드에 대응하는 연산자들은 ``src/relay/op`` 안에 등록되어 "
"있으며, C++이나 파이썬 코드로 구현된 연산자들은 ``topi`` 에 위치합니다. "

#: ../../dev/codebase_walkthrough.rst:37
msgid ""
"When a user invokes graph compilation by ``relay.build(...)``, the "
"following sequence of actions happens for each node in the graph:"
msgstr ""
"사용자가 ``relay.build(...)`` 로 그래프 컴파일을 개시하면, 그래프 안의 각 노드에 "
"대해 다음과 같은 일련의 동작들이 이어집니다: "

#: ../../dev/codebase_walkthrough.rst:39
msgid "Look up an operator implementation by querying the operator registry"
msgstr "연산자 레지스트리에 질의하여 연산자 구현을 검색"

#: ../../dev/codebase_walkthrough.rst:40
msgid "Generate a compute expression and a schedule for the operator"
msgstr "연사자에 대한 컴퓨트 표현과 스케줄을 생성"

#: ../../dev/codebase_walkthrough.rst:41
msgid "Compile the operator into object code"
msgstr "연산자를 객체코드로 컴파일"

#: ../../dev/codebase_walkthrough.rst:43
msgid ""
"One of the interesting aspects of the TVM codebase is that "
"interoperability between C++ and Python is not unidirectional. Typically,"
" all code that performs heavy lifting is implemented in C++, and Python "
"bindings are provided for the user interface. This is also true in TVM, "
"but in the TVM codebase, C++ code can also call into functions defined in"
" a Python module. For example, the convolution operator is implemented in"
" Python, and its implementation is invoked from C++ code in Relay."
msgstr ""
"TVM 코드베이스의 재밌는 점 하나는 C++과 파이썬의 상호운용성이 일방적이지 않다는 것입니다. "
"일반적으로 무거운 작업을 수행하는 코드는 C++로 구현되고 파이썬 바인딩은 사용자 인터페이스를 "
"위해 제공되지요. TVM에서도 마찬가지입니다만, TVM 코드베이스에서는 C++ 코드 역시 파이썬 모듈에 "
"정의되어 있는 함수를 호출할 수 있습니다. 예를 들어, 합성곱(convolution) 연산은 파이썬 모듈에 "
"정의되어 있는데, Relay에서는 C++ 코드가 이 구현을 불러올 수 있습니다. "

#: ../../dev/codebase_walkthrough.rst:47
msgid "Vector Add Example"
msgstr "벡터합 예제"

#: ../../dev/codebase_walkthrough.rst:49
msgid ""
"We use a simple example that uses the low level TVM API directly. The "
"example is vector addition, which is covered in detail in :ref:`tutorial-"
"tensor-expr-get-started`"
msgstr ""
"저수준 TVM API를 직접 사용하는 간단한 예로 벡터합을 들어 보겠습니다. "
":ref:`tutorial-tensor-expr-get-started` 에서 자세하게 다뤄집니다 "

#: ../../dev/codebase_walkthrough.rst:58
msgid ""
"Here, types of ``A``, ``B``, ``C`` are ``tvm.tensor.Tensor``, defined in "
"``python/tvm/te/tensor.py``. The Python ``Tensor`` is backed by C++ "
"``Tensor``, implemented in ``include/tvm/te/tensor.h`` and "
"``src/te/tensor.cc``. All Python types in TVM can be thought of as a "
"handle to the underlying C++ type with the same name. If you look at the "
"definition of Python ``Tensor`` type below, you can see it is a subclass "
"of ``Object``."
msgstr ""
"여기서 ``A``, ``B``, ``C`` 의 타입은 ``python/tvm/te/tensor.py`` 에 정의된 "
"``tvm.tensor.Tensor`` 입니다. 파이썬 ``Tensor`` 는 ``include/tvm/te/tensor.h`` 와 "
"``src/te/tensor.cc`` 에 구현되어 있는 C++ ``Tensor`` 로 뒷받침됩니다. TVM 내의 모든 "
"파이썬 타입들은 같은 명칭으로 내장되어 있는 C++ 타입에 대응하는 핸들로 봐도 됩니다. "
"일단 파이썬 ``Tensor`` 의 정의를 살펴보면, ``Object`` 의 서브클래스임을 알 수 있습니다."

#: ../../dev/codebase_walkthrough.rst:69
msgid ""
"The object protocol is the basis of exposing C++ types to frontend "
"languages, including Python. The way TVM implements Python wrapping is "
"not straightforward. It is briefly covered in :ref:`tvm-runtime-system`, "
"and details are in ``python/tvm/_ffi/`` if you are interested."
msgstr ""
"객체 프로토콜은 C++ 타입들을 파이썬과 같은 프론트엔드 언어에 노출시키기 위한 기반이 됩니다. "
"TVM이 파이썬 래핑을 구현한 방법은 직관적이지 않습니다. :ref:`tvm-runtime-system` 에 개략적으로, "
"``python/tvm/_ffi/`` 에서 자세히 다루고 있으니 흥미가 있다면 참조하세요."

#: ../../dev/codebase_walkthrough.rst:71
msgid ""
"We use the ``TVM_REGISTER_*`` macro to expose C++ functions to frontend "
"languages, in the form of a :ref:`tvm-runtime-system-packed-func`. A "
"``PackedFunc`` is another mechanism by which TVM implements "
"interoperability between C++ and Python. In particular, this is what "
"makes calling Python functions from the C++ codebase very easy. You can "
"also checkout `FFI Navigator <https://github.com/tqchen/ffi-navigator>`_ "
"which allows you to navigate between python and c++ FFI calls."
msgstr ""
"C++ 함수들을 :ref:`tvm-runtime-system-packed-func` 의 형태로 프론트엔드 언어에 "
"노출시키기 위해 ``TVM_REGISTER_*`` 매크로가 쓰입니다. "
"``PackedFunc`` 은 TVM이 C++와 파이썬의 상호운용성을 실현하는 데에 필요한 또다른 메커니즘입니다. "
"특히 이것은 C++ 코드베이스에서 파이썬 함수를 쉽게 부를 수 있게 만듭니다. "
"파이썬과 C++ FFI 호출 사이를 오갈 수 있게 하는 `FFI 네이게이터 <https://github.com/tqchen/ffi-navigator>` "
"도 확인해 보세요."

#: ../../dev/codebase_walkthrough.rst:74
msgid ""
"A ``Tensor`` object has an ``Operation`` object associated with it, "
"defined in ``python/tvm/te/tensor.py``, ``include/tvm/te/operation.h``, "
"and ``src/tvm/te/operation`` subdirectory. A ``Tensor`` is an output of "
"its ``Operation`` object. Each ``Operation`` object has in turn "
"``input_tensors()`` method, which returns a list of input ``Tensor`` to "
"it. This way we can keep track of dependencies between ``Operation``."
msgstr ""
"``Tensor`` 객체는 자신과 결부된 ``Operation`` 객체를 가지며, ``python/tvm/te/tensor.py``, "
"``include/tvm/te/operation.h``, ``src/tvm/te/operation`` 서브디렉터리에 정의됩니다. "
"``Tensor`` 는 자신의 ``Operation`` 객체의 출력입니다. 반면 각 ``Operation`` 객체는 "
"자신에게 입력되는 ``Tensor`` 리스트를 반환하는 ``input_tensors()`` 메서드를 가집니다. "
"이로써 ``Operation`` 사이의 의존성을 추적할 수 있습니다. "

#: ../../dev/codebase_walkthrough.rst:76
msgid ""
"We pass the operation corresponding to the output tensor ``C`` to "
"``tvm.te.create_schedule()`` function in ``python/tvm/te/schedule.py``."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:82
msgid "This function is mapped to the C++ function in ``include/tvm/schedule.h``."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:90
msgid ""
"``Schedule`` consists of collections of ``Stage`` and output "
"``Operation``."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:92
msgid ""
"``Stage`` corresponds to one ``Operation``. In the vector add example "
"above, there are two placeholder ops and one compute op, so the schedule "
"``s`` contains three stages. Each ``Stage`` holds information about a "
"loop nest structure, types of each loop (``Parallel``, ``Vectorized``, "
"``Unrolled``), and where to execute its computation in the loop nest of "
"the next ``Stage``, if any."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:94
msgid ""
"``Schedule`` and ``Stage`` are defined in ``tvm/python/te/schedule.py``, "
"``include/tvm/te/schedule.h``, and ``src/te/schedule/schedule_ops.cc``."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:96
msgid ""
"To keep it simple, we call ``tvm.build(...)`` on the default schedule "
"created by ``create_schedule()`` function above."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:103
msgid ""
"``tvm.build()``, defined in ``python/tvm/driver/build_module.py``, takes "
"a schedule, input and output ``Tensor``, and a target, and returns a "
":py:class:`tvm.runtime.Module` object. A :py:class:`tvm.runtime.Module` "
"object contains a compiled function which can be invoked with function "
"call syntax."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:105
msgid "The process of ``tvm.build()`` can be divided into two steps:"
msgstr ""

#: ../../dev/codebase_walkthrough.rst:107
msgid ""
"Lowering, where a high level, initial loop nest structures are "
"transformed into a final, low level IR"
msgstr ""

#: ../../dev/codebase_walkthrough.rst:108
msgid ""
"Code generation, where target machine code is generated from the low "
"level IR"
msgstr ""

#: ../../dev/codebase_walkthrough.rst:110
msgid ""
"Lowering is done by ``tvm.lower()`` function, defined in "
"``python/tvm/build_module.py``. First, bound inference is performed, and "
"an initial loop nest structure is created."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:124
msgid ""
"Bound inference is the process where all loop bounds and sizes of "
"intermediate buffers are inferred. If you target the CUDA backend and you"
" use shared memory, its required minimum size is automatically determined"
" here. Bound inference is implemented in ``src/te/schedule/bound.cc``, "
"``src/te/schedule/graph.cc`` and ``src/te/schedule/message_passing.cc``. "
"For more information on how bound inference works, see :ref:`dev-"
"InferBound-Pass`."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:127
msgid ""
"``stmt``, which is the output of ``ScheduleOps()``, represents an initial"
" loop nest structure. If you have applied ``reorder`` or ``split`` "
"primitives to your schedule, then the initial loop nest already reflects "
"those changes. ``ScheduleOps()`` is defined in "
"``src/te/schedule/schedule_ops.cc``."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:129
msgid ""
"Next, we apply a number of lowering passes to ``stmt``. These passes are "
"implemented in ``src/tir/pass`` subdirectory. For example, if you have "
"applied ``vectorize`` or ``unroll`` primitives to your schedule, they are"
" applied in loop vectorization and unrolling passes below."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:144
msgid ""
"After lowering is done, ``build()`` function generates target machine "
"code from the lowered function. This code can contain SSE or AVX "
"instructions if you target x86, or PTX instructions for CUDA target. In "
"addition to target specific machine code, TVM also generates host side "
"code that is responsible for memory management, kernel launch etc."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:146
msgid ""
"Code generation is done by ``build_module()`` function, defined in "
"``python/tvm/target/codegen.py``. On the C++ side, code generation is "
"implemented in ``src/target/codegen`` subdirectory. ``build_module()`` "
"Python function will reach ``Build()`` function below in "
"``src/target/codegen/codegen.cc``:"
msgstr ""

#: ../../dev/codebase_walkthrough.rst:150
msgid ""
"The ``Build()`` function looks up the code generator for the given target"
" in the ``PackedFunc`` registry, and invokes the function found. For "
"example, ``codegen.build_cuda`` function is registered in "
"``src/codegen/build_cuda_on.cc``, like this:"
msgstr ""

#: ../../dev/codebase_walkthrough.rst:159
msgid ""
"The ``BuildCUDA()`` above generates CUDA kernel source from the lowered "
"IR using ``CodeGenCUDA`` class defined in "
"``src/codegen/codegen_cuda.cc``, and compile the kernel using NVRTC. If "
"you target a backend that uses LLVM, which includes x86, ARM, NVPTX and "
"AMDGPU, code generation is done primarily by ``CodeGenLLVM`` class "
"defined in ``src/codegen/llvm/codegen_llvm.cc``. ``CodeGenLLVM`` "
"translates TVM IR into LLVM IR, runs a number of LLVM optimization "
"passes, and generates target machine code."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:161
msgid ""
"The ``Build()`` function in ``src/codegen/codegen.cc`` returns a "
"``runtime::Module`` object, defined in ``include/tvm/runtime/module.h`` "
"and ``src/runtime/module.cc``. A ``Module`` object is a container for the"
" underlying target specific ``ModuleNode`` object. Each backend "
"implements a subclass of ``ModuleNode`` to add target specific runtime "
"API calls. For example, the CUDA backend implements ``CUDAModuleNode`` "
"class in ``src/runtime/cuda/cuda_module.cc``, which manages the CUDA "
"driver API. The ``BuildCUDA()`` function above wraps ``CUDAModuleNode`` "
"with ``runtime::Module`` and return it to the Python side. The LLVM "
"backend implements ``LLVMModuleNode`` in "
"``src/codegen/llvm/llvm_module.cc``, which handles JIT execution of "
"compiled code. Other subclasses of ``ModuleNode`` can be found under "
"subdirectories of ``src/runtime`` corresponding to each backend."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:163
msgid ""
"The returned module, which can be thought of as a combination of a "
"compiled function and a device API, can be invoked on TVM's NDArray "
"objects."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:174
msgid ""
"Under the hood, TVM allocates device memory and manages memory transfers "
"automatically. To do that, each backend needs to subclass ``DeviceAPI`` "
"class, defined in ``include/tvm/runtime/device_api.h``, and override "
"memory management methods to use device specific API. For example, the "
"CUDA backend implements ``CUDADeviceAPI`` in "
"``src/runtime/cuda/cuda_device_api.cc`` to use ``cudaMalloc``, "
"``cudaMemcpy`` etc."
msgstr ""

#: ../../dev/codebase_walkthrough.rst:176
msgid ""
"The first time you invoke the compiled module with ``fadd(a, b, c)``, "
"``GetFunction()`` method of ``ModuleNode`` is called to get a "
"``PackedFunc`` that can be used for a kernel call. For example, in "
"``src/runtime/cuda/cuda_module.cc`` the CUDA backend implements "
"``CUDAModuleNode::GetFunction()`` like this:"
msgstr ""

#: ../../dev/codebase_walkthrough.rst:190
msgid ""
"The ``PackedFunc``'s overloaded ``operator()`` will be called, which in "
"turn calls ``operator()`` of ``CUDAWrappedFunc`` in "
"``src/runtime/cuda/cuda_module.cc``, where finally we see the "
"``cuLaunchKernel`` driver call:"
msgstr ""

#: ../../dev/codebase_walkthrough.rst:220
msgid ""
"This concludes an overview of how TVM compiles and executes a function. "
"Although we did not detail TOPI or Relay, in the end, all neural network "
"operators go through the same compilation process as above. You are "
"encouraged to dive into the details of the rest of the codebase."
msgstr ""


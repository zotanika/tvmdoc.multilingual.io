# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-04 20:34+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../deploy/tensorrt.rst:19
msgid "Relay TensorRT Integration"
msgstr "Relay TensorRT 구현"

#: ../../deploy/tensorrt.rst:20
msgid "**Author**: `Trevor Morris <https://github.com/trevor-m>`_"
msgstr "**저자**: `Trevor Morris <https://github.com/trevor-m>`_"

#: ../../deploy/tensorrt.rst:23
msgid "Introduction"
msgstr ""

#: ../../deploy/tensorrt.rst:25
msgid ""
"NVIDIA TensorRT is a library for optimized deep learning inference. This "
"integration will offload as many operators as possible from Relay to "
"TensorRT, providing a performance boost on NVIDIA GPUs without the need "
"to tune schedules."
msgstr ""

#: ../../deploy/tensorrt.rst:29
msgid ""
"This guide will demonstrate how to install TensorRT and build TVM with "
"TensorRT BYOC and runtime enabled. It will also provide example code to "
"compile and run a ResNet-18 model using TensorRT and how to configure the"
" compilation and runtime settings. Finally, we document the supported "
"operators and how to extend the integration to support other operators."
msgstr ""

#: ../../deploy/tensorrt.rst:35
msgid "Installing TensorRT"
msgstr ""

#: ../../deploy/tensorrt.rst:37
msgid ""
"In order to download TensorRT, you will need to create an NVIDIA "
"Developer program account. Please see NVIDIA's documentation for more "
"info: https://docs.nvidia.com/deeplearning/tensorrt/install-"
"guide/index.html. If you have a Jetson device such as a TX1, TX2, Xavier,"
" or Nano, TensorRT will already be installed on the device via the "
"JetPack SDK."
msgstr ""

#: ../../deploy/tensorrt.rst:43
msgid "There are two methods to install TensorRT:"
msgstr ""

#: ../../deploy/tensorrt.rst:45
msgid "System install via deb or rpm package."
msgstr ""

#: ../../deploy/tensorrt.rst:46
msgid "Tar file installation."
msgstr ""

#: ../../deploy/tensorrt.rst:48
msgid ""
"With the tar file installation method, you must provide the path of the "
"extracted tar archive to USE_TENSORRT_RUNTIME=/path/to/TensorRT. With the"
" system install method, USE_TENSORRT_RUNTIME=ON will automatically locate"
" your installation."
msgstr ""

#: ../../deploy/tensorrt.rst:53
msgid "Building TVM with TensorRT support"
msgstr ""

#: ../../deploy/tensorrt.rst:55
msgid ""
"There are two separate build flags for TensorRT integration in TVM. These"
" flags also enable cross-compilation: USE_TENSORRT_CODEGEN=ON will also "
"you to build a module with TensorRT support on a host machine, while "
"USE_TENSORRT_RUNTIME=ON will enable the TVM runtime on an edge device to "
"execute the TensorRT module. You should enable both if you want to "
"compile and also execute models with the same TVM build."
msgstr ""

#: ../../deploy/tensorrt.rst:61
msgid ""
"USE_TENSORRT_CODEGEN=ON/OFF - This flag will enable compiling a TensorRT "
"module, which does not require any TensorRT library."
msgstr ""

#: ../../deploy/tensorrt.rst:63
msgid ""
"USE_TENSORRT_RUNTIME=ON/OFF/path-to-TensorRT - This flag will enable the "
"TensorRT runtime module. This will build TVM against the installed "
"TensorRT library."
msgstr ""

#: ../../deploy/tensorrt.rst:66
msgid "Example setting in config.cmake file:"
msgstr ""

#: ../../deploy/tensorrt.rst:75
msgid "Build and Deploy ResNet-18 with TensorRT"
msgstr ""

#: ../../deploy/tensorrt.rst:77
msgid "Create a Relay graph from a MXNet ResNet-18 model."
msgstr ""

#: ../../deploy/tensorrt.rst:92
msgid ""
"Annotate and partition the graph for TensorRT. All ops which are "
"supported by the TensorRT integration will be marked and offloaded to "
"TensorRT. The rest of the ops will go through the regular TVM CUDA "
"compilation and code generation."
msgstr ""

#: ../../deploy/tensorrt.rst:102
msgid ""
"Build the Relay graph, using the new module and config returned by "
"partition_for_tensorrt. The target must always be a cuda target. "
"``partition_for_tensorrt`` will automatically fill out the required "
"values in the config, so there is no need to modify it - just pass it "
"along to the PassContext so the values can be read during compilation."
msgstr ""

#: ../../deploy/tensorrt.rst:114
msgid "Export the module."
msgstr ""

#: ../../deploy/tensorrt.rst:121
msgid ""
"Load module and run inference on the target machine, which must be built "
"with ``USE_TENSORRT_RUNTIME`` enabled. The first run will take longer "
"because the TensorRT engine will have to be built."
msgstr ""

#: ../../deploy/tensorrt.rst:135
msgid "Partitioning and Compilation Settings"
msgstr ""

#: ../../deploy/tensorrt.rst:137
msgid ""
"There are some options which can be configured in "
"``partition_for_tensorrt``."
msgstr ""

#: ../../deploy/tensorrt.rst:139
msgid ""
"``version`` - TensorRT version to target as tuple of (major, minor, "
"patch). If TVM is compiled with USE_TENSORRT_RUNTIME=ON, the linked "
"TensorRT version will be used instead. The version will affect which ops "
"can be partitioned to TensorRT."
msgstr ""

#: ../../deploy/tensorrt.rst:142
msgid ""
"``use_implicit_batch`` - Use TensorRT implicit batch mode (default true)."
" Setting to false will enable explicit batch mode which will widen "
"supported operators to include those which modify the batch dimension, "
"but may reduce performance for some models."
msgstr ""

#: ../../deploy/tensorrt.rst:145
msgid ""
"``remove_no_mac_subgraphs`` - A heuristic to improve performance. Removes"
" subgraphs which have been partitioned for TensorRT if they do not have "
"any multiply-accumulate operations. The removed subgraphs will go through"
" TVM's standard compilation instead."
msgstr ""

#: ../../deploy/tensorrt.rst:148
msgid ""
"``max_workspace_size`` - How many bytes of workspace size to allow each "
"subgraph to use for TensorRT engine creation. See TensorRT documentation "
"for more info. Can be overriden at runtime."
msgstr ""

#: ../../deploy/tensorrt.rst:153
msgid "Runtime Settings"
msgstr ""

#: ../../deploy/tensorrt.rst:155
msgid ""
"There are some additional options which can be configured at runtime "
"using environment variables."
msgstr ""

#: ../../deploy/tensorrt.rst:157
msgid ""
"Automatic FP16 Conversion - Environment variable "
"``TVM_TENSORRT_USE_FP16=1`` can be set to automatically convert the "
"TensorRT components of your model to 16-bit floating point precision. "
"This can greatly increase performance, but may cause some slight loss in "
"the model accuracy."
msgstr ""

#: ../../deploy/tensorrt.rst:160
msgid ""
"Caching TensorRT Engines - During the first inference, the runtime will "
"invoke the TensorRT API to build an engine. This can be time consuming, "
"so you can set ``TVM_TENSORRT_CACHE_DIR`` to point to a directory to save"
" these built engines to on the disk. The next time you load the model and"
" give it the same directory, the runtime will load the already built "
"engines to avoid the long warmup time. A unique directory is required for"
" each model."
msgstr ""

#: ../../deploy/tensorrt.rst:165
msgid ""
"TensorRT has a paramter to configure the maximum amount of scratch space "
"that each layer in the model can use. It is generally best to use the "
"highest value which does not cause you to run out of memory. You can use "
"``TVM_TENSORRT_MAX_WORKSPACE_SIZE`` to override this by specifying the "
"workspace size in bytes you would like to use."
msgstr ""

#: ../../deploy/tensorrt.rst:172
msgid "Operator support"
msgstr ""

#: ../../deploy/tensorrt.rst:174
msgid "Relay Node"
msgstr ""

#: ../../deploy/tensorrt.rst:174
msgid "Remarks"
msgstr ""

#: ../../deploy/tensorrt.rst:176
msgid "nn.relu"
msgstr ""

#: ../../deploy/tensorrt.rst:178
msgid "sigmoid"
msgstr ""

#: ../../deploy/tensorrt.rst:180
msgid "tanh"
msgstr ""

#: ../../deploy/tensorrt.rst:182
msgid "nn.batch_norm"
msgstr ""

#: ../../deploy/tensorrt.rst:184
msgid "nn.softmax"
msgstr ""

#: ../../deploy/tensorrt.rst:186
msgid "nn.conv2d"
msgstr ""

#: ../../deploy/tensorrt.rst:188
msgid "nn.dense"
msgstr ""

#: ../../deploy/tensorrt.rst:190
msgid "nn.bias_add"
msgstr ""

#: ../../deploy/tensorrt.rst:192
msgid "add"
msgstr ""

#: ../../deploy/tensorrt.rst:194
msgid "subtract"
msgstr ""

#: ../../deploy/tensorrt.rst:196
msgid "multiply"
msgstr ""

#: ../../deploy/tensorrt.rst:198
msgid "divide"
msgstr ""

#: ../../deploy/tensorrt.rst:200
msgid "power"
msgstr ""

#: ../../deploy/tensorrt.rst:202
msgid "maximum"
msgstr ""

#: ../../deploy/tensorrt.rst:204
msgid "minimum"
msgstr ""

#: ../../deploy/tensorrt.rst:206
msgid "nn.max_pool2d"
msgstr ""

#: ../../deploy/tensorrt.rst:208
msgid "nn.avg_pool2d"
msgstr ""

#: ../../deploy/tensorrt.rst:210
msgid "nn.global_max_pool2d"
msgstr ""

#: ../../deploy/tensorrt.rst:212
msgid "nn.global_avg_pool2d"
msgstr ""

#: ../../deploy/tensorrt.rst:214
msgid "exp"
msgstr ""

#: ../../deploy/tensorrt.rst:216
msgid "log"
msgstr ""

#: ../../deploy/tensorrt.rst:218
msgid "sqrt"
msgstr ""

#: ../../deploy/tensorrt.rst:220
msgid "abs"
msgstr ""

#: ../../deploy/tensorrt.rst:222
msgid "negative"
msgstr ""

#: ../../deploy/tensorrt.rst:224
msgid "nn.batch_flatten"
msgstr ""

#: ../../deploy/tensorrt.rst:226
msgid "expand_dims"
msgstr ""

#: ../../deploy/tensorrt.rst:228
msgid "squeeze"
msgstr ""

#: ../../deploy/tensorrt.rst:230
msgid "concatenate"
msgstr ""

#: ../../deploy/tensorrt.rst:232
msgid "nn.conv2d_transpose"
msgstr ""

#: ../../deploy/tensorrt.rst:234
msgid "transpose"
msgstr ""

#: ../../deploy/tensorrt.rst:236
msgid "layout_transform"
msgstr ""

#: ../../deploy/tensorrt.rst:238
msgid "reshape"
msgstr ""

#: ../../deploy/tensorrt.rst:240
msgid "nn.pad"
msgstr ""

#: ../../deploy/tensorrt.rst:242
msgid "sum"
msgstr ""

#: ../../deploy/tensorrt.rst:244
msgid "prod"
msgstr ""

#: ../../deploy/tensorrt.rst:246
msgid "max"
msgstr ""

#: ../../deploy/tensorrt.rst:248
msgid "min"
msgstr ""

#: ../../deploy/tensorrt.rst:250
msgid "mean"
msgstr ""

#: ../../deploy/tensorrt.rst:252
msgid "nn.adaptive_max_pool2d"
msgstr ""

#: ../../deploy/tensorrt.rst:254
msgid "nn.adaptive_avg_pool2d"
msgstr ""

#: ../../deploy/tensorrt.rst:256
msgid "clip"
msgstr ""

#: ../../deploy/tensorrt.rst:256 ../../deploy/tensorrt.rst:258
#: ../../deploy/tensorrt.rst:260 ../../deploy/tensorrt.rst:262
#: ../../deploy/tensorrt.rst:264 ../../deploy/tensorrt.rst:266
#: ../../deploy/tensorrt.rst:268 ../../deploy/tensorrt.rst:270
msgid "Requires TensorRT 5.1.5 or greater"
msgstr ""

#: ../../deploy/tensorrt.rst:258
msgid "nn.leaky_relu"
msgstr ""

#: ../../deploy/tensorrt.rst:260
msgid "sin"
msgstr ""

#: ../../deploy/tensorrt.rst:262
msgid "cos"
msgstr ""

#: ../../deploy/tensorrt.rst:264
msgid "atan"
msgstr ""

#: ../../deploy/tensorrt.rst:266
msgid "ceil"
msgstr ""

#: ../../deploy/tensorrt.rst:268
msgid "floor"
msgstr ""

#: ../../deploy/tensorrt.rst:270
msgid "strided_slice"
msgstr ""

#: ../../deploy/tensorrt.rst:272
msgid "nn.conv3d"
msgstr ""

#: ../../deploy/tensorrt.rst:272 ../../deploy/tensorrt.rst:274
#: ../../deploy/tensorrt.rst:276 ../../deploy/tensorrt.rst:278
msgid "Requires TensorRT 6.0.1 or greater"
msgstr ""

#: ../../deploy/tensorrt.rst:274
msgid "nn.max_pool3d"
msgstr ""

#: ../../deploy/tensorrt.rst:276
msgid "nn.avg_pool3d"
msgstr ""

#: ../../deploy/tensorrt.rst:278
msgid "nn.conv3d_transpose"
msgstr ""

#: ../../deploy/tensorrt.rst:283
msgid "Adding a new operator"
msgstr ""

#: ../../deploy/tensorrt.rst:284
msgid ""
"To add support for a new operator, there are a series of files we need to"
" make changes to:"
msgstr ""

#: ../../deploy/tensorrt.rst:286
msgid ""
"`src/runtime/contrib/tensorrt/tensorrt_ops.cc` Create a new op converter "
"class which implements the ``TensorRTOpConverter`` interface. You must "
"implement the constructor to specify how many inputs there are and "
"whether they are tensors or weights. You must also implement the "
"``Convert`` method to perform the conversion. This is done by using the "
"inputs, attributes, and network from params to add the new TensorRT "
"layers and push the layer outputs. You can use the existing converters as"
" an example. Finally, register your new op conventer in the "
"``GetOpConverters()`` map."
msgstr ""

#: ../../deploy/tensorrt.rst:293
msgid ""
"`python/relay/op/contrib/tensorrt.py` This file contains the annotation "
"rules for TensorRT. These determine which operators and their attributes "
"that are supported. You must register an annotation function for the "
"relay operator and specify which attributes are supported by your "
"converter, by checking the attributes are returning true or false."
msgstr ""

#: ../../deploy/tensorrt.rst:297
msgid ""
"`tests/python/contrib/test_tensorrt.py` Add unit tests for the given "
"operator."
msgstr ""


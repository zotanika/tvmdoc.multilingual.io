# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-01-04 20:34+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_tutorials_frontend_deploy_prequantized_tflite.py>` to "
"download the full example code"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:22
msgid "Deploy a Framework-prequantized Model with TVM - Part 3 (TFLite)"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:23
msgid "**Author**: `Siju Samuel <https://github.com/siju-samuel>`_"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:25
msgid ""
"Welcome to part 3 of the Deploy Framework-Prequantized Model with TVM "
"tutorial. In this part, we will start with a Quantized TFLite graph and "
"then compile and execute it via TVM."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:29
msgid ""
"For more details on quantizing the model using TFLite, readers are "
"encouraged to go through `Converting Quantized Models "
"<https://www.tensorflow.org/lite/convert/quantization>`_."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:33
msgid ""
"The TFLite models can be downloaded from this `link "
"<https://www.tensorflow.org/lite/guide/hosted_models>`_."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:36
msgid ""
"To get started, Tensorflow and TFLite package needs to be installed as "
"prerequisite."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:44
msgid ""
"Now please check if TFLite package is installed successfully, ``python -c"
" \"import tflite\"``"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:49
msgid "Necessary imports"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:74
msgid "Download pretrained Quantized TFLite model"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:102
#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:177
#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:406
#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:438
msgid "Out:"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:114
msgid "Utils for downloading and extracting zip files"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:145
msgid "Load a test image"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:150
msgid "Get a real image for e2e testing"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:189
msgid "Load a tflite model"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:193
msgid "Now we can open mobilenet_v2_1.0_224.tflite"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:221
msgid ""
"Lets run TFLite pre-quantized model inference and get the TFLite "
"prediction."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:267
msgid ""
"Lets run TVM compiled pre-quantized model inference and get the TVM "
"prediction."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:294
msgid "TFLite inference"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:298
msgid "Run TFLite inference on the quantized model."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:317
msgid "TVM compilation and inference"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:321
msgid ""
"We use the TFLite-Relay parser to convert the TFLite pre-quantized graph "
"into Relay IR. Note that frontend parser call for a pre-quantized model "
"is exactly same as frontend parser call for a FP32 model. We encourage "
"you to remove the comment from print(mod) and inspect the Relay module. "
"You will see many QNN operators, like, Requantize, Quantize and QNN "
"Conv2D."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:345
msgid ""
"Lets now the compile the Relay module. We use the \"llvm\" target here. "
"Please replace it with the target platform that you are interested in."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:365
msgid "Finally, lets call inference on the TVM compiled module."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:383
msgid "Accuracy comparison"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:387
msgid ""
"Print the top-5 labels for MXNet and TVM inference. Checking the labels "
"because the requantize implementation is different between TFLite and "
"Relay. This cause final output numbers to mismatch. So, testing accuracy "
"via labels."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:419
msgid "Measure performance"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:420
msgid ""
"Here we give an example of how to measure performance of TVM compiled "
"models."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:451
msgid ""
"Unless the hardware has special support for fast 8 bit instructions, "
"quantized models are not expected to be any faster than FP32 models. "
"Without fast 8 bit instructions, TVM does quantized convolution in 16 "
"bit, even if the model itself is 8 bit."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:455
msgid ""
"For x86, the best performance can be achieved on CPUs with AVX512 "
"instructions set. In this case, TVM utilizes the fastest available 8 bit "
"instructions for the given target. This includes support for the VNNI 8 "
"bit dot product instruction (CascadeLake or newer). For EC2 C5.12x large "
"instance, TVM latency for this tutorial is ~2 ms."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:460
msgid ""
"Intel conv2d NCHWc schedule on ARM gives better end-to-end latency "
"compared to ARM NCHW conv2d spatial pack schedule for many TFLite "
"networks. ARM winograd performance is higher but it has a high memory "
"footprint."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:464
msgid "Moreover, the following general tips for CPU performance equally applies:"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:466
msgid ""
"Set the environment variable TVM_NUM_THREADS to the number of physical "
"cores"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:467
msgid ""
"Choose the best target for your hardware, such as \"llvm -mcpu=skylake-"
"avx512\" or \"llvm -mcpu=cascadelake\" (more CPUs with AVX512 would come "
"in the future)"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:469
msgid ""
"Perform autotuning - `Auto-tuning a convolution network for x86 CPU "
"<https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_x86.html>`_."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:471
msgid ""
"To get best inference performance on ARM CPU, change target argument "
"according to your device and follow `Auto-tuning a convolution network "
"for ARM CPU "
"<https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_arm.html>`_."
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:488
msgid ""
":download:`Download Python source code: deploy_prequantized_tflite.py "
"<deploy_prequantized_tflite.py>`"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:494
msgid ""
":download:`Download Jupyter notebook: deploy_prequantized_tflite.ipynb "
"<deploy_prequantized_tflite.ipynb>`"
msgstr ""

#: ../../tutorials/frontend/deploy_prequantized_tflite.rst:501
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

